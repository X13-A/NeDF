{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZh3aklt3ZET"
   },
   "source": [
    "## Tiny NeDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ptTYjWao3VsM"
   },
   "outputs": [],
   "source": [
    "# Import all the good stuff\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilitary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meshgrid_xy(tensor1: torch.Tensor, tensor2: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"Mimick np.meshgrid(..., indexing=\"xy\") in pytorch. torch.meshgrid only allows \"ij\" indexing.\n",
    "    (If you're unsure what this means, safely skip trying to understand this, and run a tiny example!)\n",
    "\n",
    "    Args:\n",
    "      tensor1 (torch.Tensor): Tensor whose elements define the first dimension of the returned meshgrid.\n",
    "      tensor2 (torch.Tensor): Tensor whose elements define the second dimension of the returned meshgrid.\n",
    "    \"\"\"\n",
    "    # TESTED\n",
    "    ii, jj = torch.meshgrid(tensor1, tensor2)\n",
    "    return ii.transpose(-1, -2), jj.transpose(-1, -2)\n",
    "\n",
    "\n",
    "def cumprod_exclusive(tensor: torch.Tensor) -> torch.Tensor:\n",
    "  r\"\"\"Mimick functionality of tf.math.cumprod(..., exclusive=True), as it isn't available in PyTorch.\n",
    "\n",
    "  Args:\n",
    "    tensor (torch.Tensor): Tensor whose cumprod (cumulative product, see `torch.cumprod`) along dim=-1\n",
    "      is to be computed.\n",
    "\n",
    "  Returns:\n",
    "    cumprod (torch.Tensor): cumprod of Tensor along dim=-1, mimiciking the functionality of\n",
    "      tf.math.cumprod(..., exclusive=True) (see `tf.math.cumprod` for details).\n",
    "  \"\"\"\n",
    "  # TESTED\n",
    "  # Only works for the last dimension (dim=-1)\n",
    "  dim = -1\n",
    "  # Compute regular cumprod first (this is equivalent to `tf.math.cumprod(..., exclusive=False)`).\n",
    "  cumprod = torch.cumprod(tensor, dim)\n",
    "  # \"Roll\" the elements along dimension 'dim' by 1 element.\n",
    "  cumprod = torch.roll(cumprod, 1, dim)\n",
    "  # Replace the first element by \"1\" as this is what tf.cumprod(..., exclusive=True) does.\n",
    "  cumprod[..., 0] = 1.\n",
    "\n",
    "  return cumprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EHNwlsOT7NTp"
   },
   "outputs": [],
   "source": [
    "def get_ray_bundle(height: int, width: int, focal_length: float, tform_cam2world: torch.Tensor):\n",
    "  r\"\"\"Compute the bundle of rays passing through all pixels of an image (one ray per pixel).\n",
    "\n",
    "  Args:\n",
    "    height (int): Height of an image (number of pixels).\n",
    "    width (int): Width of an image (number of pixels).\n",
    "    focal_length (float or torch.Tensor): Focal length (number of pixels, i.e., calibrated intrinsics).\n",
    "    tform_cam2world (torch.Tensor): A 6-DoF rigid-body transform (shape: :math:`(4, 4)`) that\n",
    "      transforms a 3D point from the camera frame to the \"world\" frame for the current example.\n",
    "\n",
    "  Returns:\n",
    "    ray_origins (torch.Tensor): A tensor of shape :math:`(width, height, 3)` denoting the centers of\n",
    "      each ray. `ray_origins[i][j]` denotes the origin of the ray passing through pixel at\n",
    "      row index `j` and column index `i`.\n",
    "      (TODO: double check if explanation of row and col indices convention is right).\n",
    "    ray_directions (torch.Tensor): A tensor of shape :math:`(width, height, 3)` denoting the\n",
    "      direction of each ray (a unit vector). `ray_directions[i][j]` denotes the direction of the ray\n",
    "      passing through the pixel at row index `j` and column index `i`.\n",
    "      (TODO: double check if explanation of row and col indices convention is right).\n",
    "  \"\"\"\n",
    "  # TESTED\n",
    "  ii, jj = meshgrid_xy(\n",
    "      torch.arange(width).to(tform_cam2world),\n",
    "      torch.arange(height).to(tform_cam2world)\n",
    "  )\n",
    "  directions = torch.stack([(ii - width * .5) / focal_length,\n",
    "                            -(jj - height * .5) / focal_length,\n",
    "                            -torch.ones_like(ii)\n",
    "                           ], dim=-1)\n",
    "  ray_directions = torch.sum(directions[..., None, :] * tform_cam2world[:3, :3], dim=-1)\n",
    "  ray_origins = tform_cam2world[:3, -1].expand(ray_directions.shape)\n",
    "  return ray_origins, ray_directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgoNR03iIs7R"
   },
   "source": [
    "### Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "UjFN6FNzIqxl"
   },
   "outputs": [],
   "source": [
    "class VeryTinyNeDFModel(torch.nn.Module):\n",
    "    r\"\"\"Define a \"very tiny\" NeDF model comprising three fully connected layers.\"\"\"\n",
    "    def __init__(self, filter_size=128, far_thresh=1000.0, L=10):\n",
    "        self.L = L\n",
    "        # self.input_dim = 3 + 2 * L * 3 # Due to positional encoding\n",
    "        self.input_dim = 3 # Since we don't use it, just x,y,z\n",
    "        super(VeryTinyNeDFModel, self).__init__()\n",
    "        # Input layer (default: 3 + 2 * L * 3 -> 128 if positional encoding is enabled, 3 -> 128 otherwise)\n",
    "        self.layer1 = torch.nn.Linear(self.input_dim, filter_size)\n",
    "        # Layer 2 (default: 128 -> 128)\n",
    "        self.layer2 = torch.nn.Linear(filter_size, filter_size)\n",
    "        # Layer 3 (default: 128 -> 1) for predicting distance\n",
    "        self.layer3 = torch.nn.Linear(filter_size, 1)\n",
    "        # Short hand for torch.nn.functional.relu\n",
    "        self.relu = torch.nn.functional.relu\n",
    "        self.far_thresh = far_thresh\n",
    "\n",
    "    # Not convincing currently\n",
    "    def positional_encoding(self, x):\n",
    "        encoded = [x]\n",
    "        for i in range(self.L):\n",
    "            encoded.append(torch.sin(2**i * x))\n",
    "            encoded.append(torch.cos(2**i * x))\n",
    "        return torch.cat(encoded, dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.positional_encoding(x)\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x) # Output is raw distance\n",
    "        return torch.clamp(x, min=0.0, max=self.far_thresh) # Clamp output to prevent crazy / negative distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ierxw2dsL3pU"
   },
   "source": [
    "### GPU vs CPU ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKNiPtnML8i9"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zB3NGalaLlN1"
   },
   "source": [
    "### Load input images, poses, intrinsics, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "1w2QkjCkLc9Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100])\n"
     ]
    }
   ],
   "source": [
    "# Load input images, poses, and intrinsics\n",
    "data = np.load(\"depth_map_test.npz\")\n",
    "\n",
    "# Camera extrinsics (poses)\n",
    "testpose = data[\"pose\"]\n",
    "testpose = torch.from_numpy(testpose).to(device)\n",
    "\n",
    "# Focal length (intrinsics)\n",
    "focal_length = data[\"focal\"]\n",
    "focal_length = torch.from_numpy(focal_length).to(device)\n",
    "\n",
    "testimg = data[\"depth_map\"]\n",
    "# testimg = np.resize(testimg, (6, 6))\n",
    "testimg = torch.from_numpy(testimg).to(device)\n",
    "\n",
    "print(testimg.shape)\n",
    "# Height and width depth map\n",
    "height, width = testimg.shape[:2]\n",
    "\n",
    "# Near and far clipping thresholds for depth values.\n",
    "near_thresh = 0.01\n",
    "far_thresh = 500.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVsKUODnM9KK"
   },
   "source": [
    "### Display the image used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "_04FamFHM7l8",
    "outputId": "f4999258-65d6-475d-a17c-a0836b8062c0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(testimg.detach().cpu().numpy(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFUaajNpNNgJ"
   },
   "source": [
    "### Train !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method renders a depth map using the model's predictions to dynamically adjust step size during ray marching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_depth_sphere_tracing(\n",
    "    distance_field_model: torch.nn.Module,\n",
    "    ray_origins: torch.Tensor,\n",
    "    ray_directions: torch.Tensor,\n",
    "    depth_map: torch.Tensor,\n",
    "    near_thresh: float,\n",
    "    max_iterations: int = 50\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    # Create a tensor to track active rays\n",
    "    active_mask = torch.ones_like(depth_map, dtype=torch.bool)\n",
    "\n",
    "    # Predicted depth map\n",
    "    dstTravelled = torch.full_like(depth_map, 0, requires_grad=True)\n",
    "    steps = torch.zeros_like(active_mask, dtype=torch.float32)\n",
    "    \n",
    "    all_query_points = []\n",
    "    all_predicted_distances = []\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # Stop if no active rays remain\n",
    "        if not torch.any(active_mask):\n",
    "            break\n",
    "        \n",
    "        # Compute the query points\n",
    "        query_points = ray_origins + ray_directions * dstTravelled[..., None]\n",
    "        query_points.requires_grad_()\n",
    "        \n",
    "        # Predict distances using the model\n",
    "        predicted_distances = distance_field_model(query_points).squeeze(-1)\n",
    "\n",
    "        # Increment the steps for active rays\n",
    "        steps = torch.where(active_mask, steps + 1, steps)\n",
    "\n",
    "        # Mask for rays that are within the surface threshold or exceeding far_thresh\n",
    "        hit_mask = ((predicted_distances < near_thresh) | (dstTravelled >= depth_map)) & active_mask\n",
    "\n",
    "        # Store active query points and predictions\n",
    "        all_query_points.append(query_points[~hit_mask])\n",
    "        all_predicted_distances.append(predicted_distances[~hit_mask])\n",
    "\n",
    "        # Update active mask to deactivate rays that hit\n",
    "        active_mask = active_mask & ~hit_mask\n",
    "        \n",
    "        # Update the depth map only for rays that have hit\n",
    "        dstTravelled = torch.where(active_mask, dstTravelled + predicted_distances, dstTravelled)\n",
    "\n",
    "    # Concatenate query points and distances\n",
    "    all_query_points = torch.cat(all_query_points, dim=0)\n",
    "    all_predicted_distances = torch.cat(all_predicted_distances, dim=0)\n",
    "\n",
    "    return dstTravelled, steps, all_query_points, all_predicted_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(model, query_points, eps=1e-4, sample_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Computes numerical gradients of SDF using finite differences.\n",
    "\n",
    "    Args:\n",
    "    - model: The implicit function.\n",
    "    - query_points: Tensor of shape [N, 3], query points.\n",
    "    - eps: Small step for finite differences.\n",
    "    - sample_ratio: Fraction of points to sample for gradient computation.\n",
    "\n",
    "    Returns:\n",
    "    - gradients: Tensor of shape [sampled_N, 3], computed gradients.\n",
    "    \"\"\"\n",
    "    N = query_points.shape[0]\n",
    "    device = query_points.device\n",
    "\n",
    "    # Randomly sample query points\n",
    "    nb_samples = min(int(N * sample_ratio), 5000)\n",
    "    sampled_indices = torch.randperm(N, device=device)[:nb_samples]\n",
    "    sampled_points = query_points[sampled_indices]\n",
    "\n",
    "    # Allocate memory for gradients\n",
    "    gradients = torch.zeros_like(sampled_points, device=device)\n",
    "\n",
    "    for i in range(3):  # Compute gradient w.r.t x, y, z\n",
    "        offset = torch.zeros_like(sampled_points, device=device)\n",
    "        offset[:, i] = eps\n",
    "\n",
    "        forward_points = sampled_points + offset\n",
    "        backward_points = sampled_points - offset\n",
    "\n",
    "        forward_sdf = model(forward_points).squeeze(-1)\n",
    "        backward_sdf = model(backward_points).squeeze(-1)\n",
    "\n",
    "        # Central finite difference\n",
    "        gradients[:, i] = (forward_sdf - backward_sdf) / (2 * eps)\n",
    "\n",
    "    return gradients\n",
    "\n",
    "def compute_eikonal_loss(gradients):\n",
    "    \"\"\"\n",
    "    Computes the Eikonal loss using the computed gradients.\n",
    "\n",
    "    Args:\n",
    "    - gradients (torch.Tensor): Tensor of shape [N, 3] containing the gradients of SDF predictions.\n",
    "\n",
    "    Returns:\n",
    "    - loss (torch.Tensor): Computed Eikonal loss.\n",
    "    \"\"\"\n",
    "    # ||grad|| should be close to 1, compute the deviation from 1\n",
    "    loss = torch.mean(torch.abs(torch.norm(gradients, dim=-1) - 1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JovhcSy1NIhr",
    "outputId": "50f322e3-552b-4802-e03e-1e5237a64ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:\n",
      "Loss: 6.885768413543701\n",
      "Average steps: 45.96039962768555\n",
      "Eikonal loss: 0.9384565949440002\n",
      "Step 5:\n",
      "Loss: 5.6848344802856445\n",
      "Average steps: 38.29280090332031\n",
      "Eikonal loss: 0.9151368141174316\n",
      "Step 10:\n",
      "Loss: 4.950045108795166\n",
      "Average steps: 31.802499771118164\n",
      "Eikonal loss: 0.8978699445724487\n",
      "Step 15:\n",
      "Loss: 4.63460636138916\n",
      "Average steps: 28.05099868774414\n",
      "Eikonal loss: 0.887160062789917\n",
      "Step 20:\n",
      "Loss: 4.453784465789795\n",
      "Average steps: 25.941999435424805\n",
      "Eikonal loss: 0.8805781006813049\n",
      "Step 25:\n",
      "Loss: 4.378615379333496\n",
      "Average steps: 23.988300323486328\n",
      "Eikonal loss: 0.8731752634048462\n",
      "Step 30:\n",
      "Loss: 4.329902172088623\n",
      "Average steps: 22.45009994506836\n",
      "Eikonal loss: 0.8630385398864746\n",
      "Step 35:\n",
      "Loss: 4.272353649139404\n",
      "Average steps: 21.62299919128418\n",
      "Eikonal loss: 0.8497633337974548\n",
      "Step 40:\n",
      "Loss: 4.224574565887451\n",
      "Average steps: 21.331499099731445\n",
      "Eikonal loss: 0.8375365138053894\n",
      "Step 45:\n",
      "Loss: 4.179099082946777\n",
      "Average steps: 20.965700149536133\n",
      "Eikonal loss: 0.825829267501831\n",
      "Step 50:\n",
      "Loss: 4.133422374725342\n",
      "Average steps: 20.44369888305664\n",
      "Eikonal loss: 0.814041256904602\n",
      "Step 55:\n",
      "Loss: 4.088363170623779\n",
      "Average steps: 19.8887996673584\n",
      "Eikonal loss: 0.8030604720115662\n",
      "Step 60:\n",
      "Loss: 4.036245822906494\n",
      "Average steps: 19.247299194335938\n",
      "Eikonal loss: 0.7922254800796509\n",
      "Step 65:\n",
      "Loss: 3.9710447788238525\n",
      "Average steps: 18.4385986328125\n",
      "Eikonal loss: 0.7801875472068787\n",
      "Step 70:\n",
      "Loss: 3.903202772140503\n",
      "Average steps: 17.65369987487793\n",
      "Eikonal loss: 0.7669326066970825\n",
      "Step 75:\n",
      "Loss: 3.832953929901123\n",
      "Average steps: 16.9114990234375\n",
      "Eikonal loss: 0.752781331539154\n",
      "Step 80:\n",
      "Loss: 3.767753839492798\n",
      "Average steps: 16.253700256347656\n",
      "Eikonal loss: 0.7391871213912964\n",
      "Step 85:\n",
      "Loss: 3.698402166366577\n",
      "Average steps: 15.68429946899414\n",
      "Eikonal loss: 0.7243943214416504\n",
      "Step 90:\n",
      "Loss: 3.62372088432312\n",
      "Average steps: 15.07029914855957\n",
      "Eikonal loss: 0.7085968852043152\n",
      "Step 95:\n",
      "Loss: 3.5512185096740723\n",
      "Average steps: 14.5201997756958\n",
      "Eikonal loss: 0.6930912137031555\n",
      "Step 100:\n",
      "Loss: 3.472907781600952\n",
      "Average steps: 13.960899353027344\n",
      "Eikonal loss: 0.6760429739952087\n",
      "Step 105:\n",
      "Loss: 3.389517068862915\n",
      "Average steps: 13.387299537658691\n",
      "Eikonal loss: 0.6579456925392151\n",
      "Step 110:\n",
      "Loss: 3.3019134998321533\n",
      "Average steps: 12.769399642944336\n",
      "Eikonal loss: 0.6395564675331116\n",
      "Step 115:\n",
      "Loss: 3.2015163898468018\n",
      "Average steps: 12.163799285888672\n",
      "Eikonal loss: 0.6183213591575623\n",
      "Step 120:\n",
      "Loss: 3.103729248046875\n",
      "Average steps: 11.595699310302734\n",
      "Eikonal loss: 0.5972576141357422\n",
      "Step 125:\n",
      "Loss: 2.993049383163452\n",
      "Average steps: 11.06879997253418\n",
      "Eikonal loss: 0.5733780264854431\n",
      "Step 130:\n",
      "Loss: 2.8884806632995605\n",
      "Average steps: 10.622200012207031\n",
      "Eikonal loss: 0.5507993698120117\n",
      "Step 135:\n",
      "Loss: 2.7695364952087402\n",
      "Average steps: 10.112099647521973\n",
      "Eikonal loss: 0.5253731608390808\n",
      "Step 140:\n",
      "Loss: 2.634735107421875\n",
      "Average steps: 9.639999389648438\n",
      "Eikonal loss: 0.49691078066825867\n",
      "Step 145:\n",
      "Loss: 2.523486614227295\n",
      "Average steps: 9.195599555969238\n",
      "Eikonal loss: 0.4728381633758545\n",
      "Step 150:\n",
      "Loss: 2.392296552658081\n",
      "Average steps: 8.818399429321289\n",
      "Eikonal loss: 0.4441004693508148\n",
      "Step 155:\n",
      "Loss: 2.2348313331604004\n",
      "Average steps: 8.341499328613281\n",
      "Eikonal loss: 0.4097287654876709\n",
      "Step 160:\n",
      "Loss: 2.052391290664673\n",
      "Average steps: 7.822199821472168\n",
      "Eikonal loss: 0.37104833126068115\n",
      "Step 165:\n",
      "Loss: 1.900976538658142\n",
      "Average steps: 7.331099987030029\n",
      "Eikonal loss: 0.3386929929256439\n",
      "Step 170:\n",
      "Loss: 1.7437764406204224\n",
      "Average steps: 6.927599906921387\n",
      "Eikonal loss: 0.3048528730869293\n",
      "Step 175:\n",
      "Loss: 1.5991010665893555\n",
      "Average steps: 6.600599765777588\n",
      "Eikonal loss: 0.27417439222335815\n",
      "Step 180:\n",
      "Loss: 1.5081100463867188\n",
      "Average steps: 6.429100036621094\n",
      "Eikonal loss: 0.253722608089447\n",
      "Step 185:\n",
      "Loss: 1.4320571422576904\n",
      "Average steps: 6.329699993133545\n",
      "Eikonal loss: 0.2361498922109604\n",
      "Step 190:\n",
      "Loss: 1.3421789407730103\n",
      "Average steps: 6.193199634552002\n",
      "Eikonal loss: 0.2161579132080078\n",
      "Step 195:\n",
      "Loss: 1.2674293518066406\n",
      "Average steps: 6.02810001373291\n",
      "Eikonal loss: 0.19977231323719025\n",
      "Step 200:\n",
      "Loss: 1.2220706939697266\n",
      "Average steps: 5.930699825286865\n",
      "Eikonal loss: 0.1895252913236618\n",
      "Step 205:\n",
      "Loss: 1.1951911449432373\n",
      "Average steps: 5.847499847412109\n",
      "Eikonal loss: 0.1833484023809433\n",
      "Step 210:\n",
      "Loss: 1.138964056968689\n",
      "Average steps: 5.766900062561035\n",
      "Eikonal loss: 0.1708545833826065\n",
      "Step 215:\n",
      "Loss: 1.096520185470581\n",
      "Average steps: 5.708799839019775\n",
      "Eikonal loss: 0.16128626465797424\n",
      "Step 220:\n",
      "Loss: 1.070732831954956\n",
      "Average steps: 5.65339994430542\n",
      "Eikonal loss: 0.1555546075105667\n",
      "Step 225:\n",
      "Loss: 1.0602396726608276\n",
      "Average steps: 5.611099720001221\n",
      "Eikonal loss: 0.15270087122917175\n",
      "Step 230:\n",
      "Loss: 1.0385054349899292\n",
      "Average steps: 5.58329963684082\n",
      "Eikonal loss: 0.14845411479473114\n",
      "Step 235:\n",
      "Loss: 1.0314061641693115\n",
      "Average steps: 5.542599678039551\n",
      "Eikonal loss: 0.1471758782863617\n",
      "Step 240:\n",
      "Loss: 1.010945200920105\n",
      "Average steps: 5.552999973297119\n",
      "Eikonal loss: 0.14353348314762115\n",
      "Step 245:\n",
      "Loss: 0.9815812110900879\n",
      "Average steps: 5.556299686431885\n",
      "Eikonal loss: 0.13841529190540314\n",
      "Step 250:\n",
      "Loss: 0.9772775173187256\n",
      "Average steps: 5.535699844360352\n",
      "Eikonal loss: 0.13769088685512543\n",
      "Step 255:\n",
      "Loss: 0.9567842483520508\n",
      "Average steps: 5.531499862670898\n",
      "Eikonal loss: 0.13370327651500702\n",
      "Step 260:\n",
      "Loss: 0.939833402633667\n",
      "Average steps: 5.509599685668945\n",
      "Eikonal loss: 0.13026566803455353\n",
      "Step 265:\n",
      "Loss: 0.9417957067489624\n",
      "Average steps: 5.518699645996094\n",
      "Eikonal loss: 0.13103778660297394\n",
      "Step 270:\n",
      "Loss: 0.932164192199707\n",
      "Average steps: 5.531899929046631\n",
      "Eikonal loss: 0.12956006824970245\n",
      "Step 275:\n",
      "Loss: 0.9360947012901306\n",
      "Average steps: 5.539000034332275\n",
      "Eikonal loss: 0.13079151511192322\n",
      "Step 280:\n",
      "Loss: 0.9133868217468262\n",
      "Average steps: 5.535699844360352\n",
      "Eikonal loss: 0.1265869438648224\n",
      "Step 285:\n",
      "Loss: 0.8689007759094238\n",
      "Average steps: 5.554999828338623\n",
      "Eikonal loss: 0.11804860830307007\n",
      "Step 290:\n",
      "Loss: 0.8455849289894104\n",
      "Average steps: 5.542299747467041\n",
      "Eikonal loss: 0.11348366737365723\n",
      "Step 295:\n",
      "Loss: 0.89018714427948\n",
      "Average steps: 5.53909969329834\n",
      "Eikonal loss: 0.12254749983549118\n",
      "Step 300:\n",
      "Loss: 0.8644625544548035\n",
      "Average steps: 5.547800064086914\n",
      "Eikonal loss: 0.11787402629852295\n",
      "Step 305:\n",
      "Loss: 0.8745236992835999\n",
      "Average steps: 5.582900047302246\n",
      "Eikonal loss: 0.12065983563661575\n",
      "Step 310:\n",
      "Loss: 0.8621056079864502\n",
      "Average steps: 5.607399940490723\n",
      "Eikonal loss: 0.11881434917449951\n",
      "Step 315:\n",
      "Loss: 0.8183284401893616\n",
      "Average steps: 5.598899841308594\n",
      "Eikonal loss: 0.11021887511014938\n",
      "Step 320:\n",
      "Loss: 0.8514329195022583\n",
      "Average steps: 5.591399669647217\n",
      "Eikonal loss: 0.11659853160381317\n",
      "Step 325:\n",
      "Loss: 0.8242089152336121\n",
      "Average steps: 5.589999675750732\n",
      "Eikonal loss: 0.11155249923467636\n",
      "Step 330:\n",
      "Loss: 0.8349511027336121\n",
      "Average steps: 5.583699703216553\n",
      "Eikonal loss: 0.113980732858181\n",
      "Step 335:\n",
      "Loss: 0.8331265449523926\n",
      "Average steps: 5.638299942016602\n",
      "Eikonal loss: 0.1144295185804367\n",
      "Step 340:\n",
      "Loss: 0.8340804576873779\n",
      "Average steps: 5.603099822998047\n",
      "Eikonal loss: 0.11492563784122467\n",
      "Step 345:\n",
      "Loss: 0.8173301219940186\n",
      "Average steps: 5.576600074768066\n",
      "Eikonal loss: 0.11228738725185394\n",
      "Step 350:\n",
      "Loss: 0.8286298513412476\n",
      "Average steps: 5.606299877166748\n",
      "Eikonal loss: 0.11511176824569702\n",
      "Step 355:\n",
      "Loss: 0.8198158144950867\n",
      "Average steps: 5.639100074768066\n",
      "Eikonal loss: 0.11376649141311646\n",
      "Step 360:\n",
      "Loss: 0.7908563613891602\n",
      "Average steps: 5.62470006942749\n",
      "Eikonal loss: 0.10856468975543976\n",
      "Step 365:\n",
      "Loss: 0.7606920599937439\n",
      "Average steps: 5.629899978637695\n",
      "Eikonal loss: 0.10261304676532745\n",
      "Step 370:\n",
      "Loss: 0.789480447769165\n",
      "Average steps: 5.639400005340576\n",
      "Eikonal loss: 0.1085888147354126\n",
      "Step 375:\n",
      "Loss: 0.8011822700500488\n",
      "Average steps: 5.6626996994018555\n",
      "Eikonal loss: 0.11124654859304428\n",
      "Step 380:\n",
      "Loss: 0.7471926808357239\n",
      "Average steps: 5.696399688720703\n",
      "Eikonal loss: 0.10058113187551498\n",
      "Step 385:\n",
      "Loss: 0.7727052569389343\n",
      "Average steps: 5.691299915313721\n",
      "Eikonal loss: 0.10626652091741562\n",
      "Step 390:\n",
      "Loss: 0.7727280855178833\n",
      "Average steps: 5.698699951171875\n",
      "Eikonal loss: 0.10717585682868958\n",
      "Step 395:\n",
      "Loss: 0.7681428790092468\n",
      "Average steps: 5.745699882507324\n",
      "Eikonal loss: 0.10684306919574738\n",
      "Step 400:\n",
      "Loss: 0.7677670121192932\n",
      "Average steps: 5.728699684143066\n",
      "Eikonal loss: 0.1070857048034668\n",
      "Step 405:\n",
      "Loss: 0.7482233047485352\n",
      "Average steps: 5.716099739074707\n",
      "Eikonal loss: 0.10369668155908585\n",
      "Step 410:\n",
      "Loss: 0.745336651802063\n",
      "Average steps: 5.710099697113037\n",
      "Eikonal loss: 0.10353613644838333\n",
      "Step 415:\n",
      "Loss: 0.7375930547714233\n",
      "Average steps: 5.748499870300293\n",
      "Eikonal loss: 0.10253927856683731\n",
      "Step 420:\n",
      "Loss: 0.7449140548706055\n",
      "Average steps: 5.7627997398376465\n",
      "Eikonal loss: 0.10454369336366653\n",
      "Step 425:\n",
      "Loss: 0.7147248983383179\n",
      "Average steps: 5.7581000328063965\n",
      "Eikonal loss: 0.09873611479997635\n",
      "Step 430:\n",
      "Loss: 0.7215073108673096\n",
      "Average steps: 5.759999752044678\n",
      "Eikonal loss: 0.10072023421525955\n",
      "Step 435:\n",
      "Loss: 0.7212212681770325\n",
      "Average steps: 5.712299823760986\n",
      "Eikonal loss: 0.10093126446008682\n",
      "Step 440:\n",
      "Loss: 0.7064621448516846\n",
      "Average steps: 5.676799774169922\n",
      "Eikonal loss: 0.09810365736484528\n",
      "Step 445:\n",
      "Loss: 0.6941588521003723\n",
      "Average steps: 5.661599636077881\n",
      "Eikonal loss: 0.0958625078201294\n",
      "Step 450:\n",
      "Loss: 0.7024569511413574\n",
      "Average steps: 5.681299686431885\n",
      "Eikonal loss: 0.09765230864286423\n",
      "Step 455:\n",
      "Loss: 0.7015510201454163\n",
      "Average steps: 5.683700084686279\n",
      "Eikonal loss: 0.09788942337036133\n",
      "Step 460:\n",
      "Loss: 0.6874446868896484\n",
      "Average steps: 5.6803998947143555\n",
      "Eikonal loss: 0.09544742107391357\n",
      "Step 465:\n",
      "Loss: 0.6792386770248413\n",
      "Average steps: 5.683300018310547\n",
      "Eikonal loss: 0.09413820505142212\n",
      "Step 470:\n",
      "Loss: 0.6811177134513855\n",
      "Average steps: 5.683399677276611\n",
      "Eikonal loss: 0.09473984688520432\n",
      "Step 475:\n",
      "Loss: 0.6729552149772644\n",
      "Average steps: 5.672699928283691\n",
      "Eikonal loss: 0.0931866317987442\n",
      "Step 480:\n",
      "Loss: 0.6861135959625244\n",
      "Average steps: 5.683399677276611\n",
      "Eikonal loss: 0.09576939046382904\n",
      "Step 485:\n",
      "Loss: 0.6555182933807373\n",
      "Average steps: 5.6875\n",
      "Eikonal loss: 0.0897749587893486\n",
      "Step 490:\n",
      "Loss: 0.6659972071647644\n",
      "Average steps: 5.7191996574401855\n",
      "Eikonal loss: 0.09210307896137238\n",
      "Step 495:\n",
      "Loss: 0.6494826674461365\n",
      "Average steps: 5.7307000160217285\n",
      "Eikonal loss: 0.08876190334558487\n",
      "Step 500:\n",
      "Loss: 0.6460498571395874\n",
      "Average steps: 5.710999965667725\n",
      "Eikonal loss: 0.08798708766698837\n",
      "Step 505:\n",
      "Loss: 0.6395777463912964\n",
      "Average steps: 5.714999675750732\n",
      "Eikonal loss: 0.08675049245357513\n",
      "Step 510:\n",
      "Loss: 0.6378511786460876\n",
      "Average steps: 5.721299648284912\n",
      "Eikonal loss: 0.08665383607149124\n",
      "Step 515:\n",
      "Loss: 0.6468695402145386\n",
      "Average steps: 5.696499824523926\n",
      "Eikonal loss: 0.08893647789955139\n",
      "Step 520:\n",
      "Loss: 0.6310805082321167\n",
      "Average steps: 5.695099830627441\n",
      "Eikonal loss: 0.08606470376253128\n",
      "Step 525:\n",
      "Loss: 0.6294755339622498\n",
      "Average steps: 5.669099807739258\n",
      "Eikonal loss: 0.08557923883199692\n",
      "Step 530:\n",
      "Loss: 0.626923680305481\n",
      "Average steps: 5.6601996421813965\n",
      "Eikonal loss: 0.08503834903240204\n",
      "Step 535:\n",
      "Loss: 0.6294872164726257\n",
      "Average steps: 5.697299957275391\n",
      "Eikonal loss: 0.08586262911558151\n",
      "Step 540:\n",
      "Loss: 0.6046159267425537\n",
      "Average steps: 5.695199966430664\n",
      "Eikonal loss: 0.08085302263498306\n",
      "Step 545:\n",
      "Loss: 0.6152350306510925\n",
      "Average steps: 5.690399646759033\n",
      "Eikonal loss: 0.082730233669281\n",
      "Step 550:\n",
      "Loss: 0.6017653346061707\n",
      "Average steps: 5.705599784851074\n",
      "Eikonal loss: 0.0801837220788002\n",
      "Step 555:\n",
      "Loss: 0.602169930934906\n",
      "Average steps: 5.683599948883057\n",
      "Eikonal loss: 0.08027911186218262\n",
      "Step 560:\n",
      "Loss: 0.6153825521469116\n",
      "Average steps: 5.683099746704102\n",
      "Eikonal loss: 0.08328263461589813\n",
      "Step 565:\n",
      "Loss: 0.5911484956741333\n",
      "Average steps: 5.621099948883057\n",
      "Eikonal loss: 0.07822862267494202\n",
      "Step 570:\n",
      "Loss: 0.5952957272529602\n",
      "Average steps: 5.588799953460693\n",
      "Eikonal loss: 0.07903904467821121\n",
      "Step 575:\n",
      "Loss: 0.5852725505828857\n",
      "Average steps: 5.567399978637695\n",
      "Eikonal loss: 0.07735055685043335\n",
      "Step 580:\n",
      "Loss: 0.5763496160507202\n",
      "Average steps: 5.549599647521973\n",
      "Eikonal loss: 0.07569088041782379\n",
      "Step 585:\n",
      "Loss: 0.5744047164916992\n",
      "Average steps: 5.565899848937988\n",
      "Eikonal loss: 0.07543989270925522\n",
      "Step 590:\n",
      "Loss: 0.5781172513961792\n",
      "Average steps: 5.569900035858154\n",
      "Eikonal loss: 0.07591962069272995\n",
      "Step 595:\n",
      "Loss: 0.562872588634491\n",
      "Average steps: 5.584799766540527\n",
      "Eikonal loss: 0.07303943485021591\n",
      "Step 600:\n",
      "Loss: 0.563956081867218\n",
      "Average steps: 5.575999736785889\n",
      "Eikonal loss: 0.07328685373067856\n",
      "Step 605:\n",
      "Loss: 0.5623785853385925\n",
      "Average steps: 5.580399990081787\n",
      "Eikonal loss: 0.07304053008556366\n",
      "Step 610:\n",
      "Loss: 0.5350860357284546\n",
      "Average steps: 5.588399887084961\n",
      "Eikonal loss: 0.06757047772407532\n",
      "Step 615:\n",
      "Loss: 0.5445899963378906\n",
      "Average steps: 5.588099956512451\n",
      "Eikonal loss: 0.06926320493221283\n",
      "Step 620:\n",
      "Loss: 0.5450405478477478\n",
      "Average steps: 5.5980000495910645\n",
      "Eikonal loss: 0.06967964768409729\n",
      "Step 625:\n",
      "Loss: 0.5334997177124023\n",
      "Average steps: 5.587199687957764\n",
      "Eikonal loss: 0.0676002949476242\n",
      "Step 630:\n",
      "Loss: 0.5428719520568848\n",
      "Average steps: 5.597799777984619\n",
      "Eikonal loss: 0.06972631067037582\n",
      "Step 635:\n",
      "Loss: 0.564174473285675\n",
      "Average steps: 5.625400066375732\n",
      "Eikonal loss: 0.0743543803691864\n",
      "Step 640:\n",
      "Loss: 0.568291962146759\n",
      "Average steps: 5.602799892425537\n",
      "Eikonal loss: 0.07476814836263657\n",
      "Step 645:\n",
      "Loss: 0.5609813928604126\n",
      "Average steps: 5.591899871826172\n",
      "Eikonal loss: 0.07316938042640686\n",
      "Step 650:\n",
      "Loss: 0.5692899227142334\n",
      "Average steps: 5.615900039672852\n",
      "Eikonal loss: 0.07481283694505692\n",
      "Step 655:\n",
      "Loss: 0.5931082367897034\n",
      "Average steps: 5.734099864959717\n",
      "Eikonal loss: 0.0795956552028656\n",
      "Step 660:\n",
      "Loss: 0.566414475440979\n",
      "Average steps: 5.774199962615967\n",
      "Eikonal loss: 0.07331560552120209\n",
      "Step 665:\n",
      "Loss: 0.5733817219734192\n",
      "Average steps: 5.639899730682373\n",
      "Eikonal loss: 0.07299576699733734\n",
      "Step 670:\n",
      "Loss: 0.570059061050415\n",
      "Average steps: 5.608699798583984\n",
      "Eikonal loss: 0.07224428653717041\n",
      "Step 675:\n",
      "Loss: 0.5604480504989624\n",
      "Average steps: 5.611199855804443\n",
      "Eikonal loss: 0.07063139230012894\n",
      "Step 680:\n",
      "Loss: 0.5411123633384705\n",
      "Average steps: 5.5920000076293945\n",
      "Eikonal loss: 0.06672713905572891\n",
      "Step 685:\n",
      "Loss: 0.5479714274406433\n",
      "Average steps: 5.624000072479248\n",
      "Eikonal loss: 0.06880402565002441\n",
      "Step 690:\n",
      "Loss: 0.5477426052093506\n",
      "Average steps: 5.616499900817871\n",
      "Eikonal loss: 0.06885775178670883\n",
      "Step 695:\n",
      "Loss: 0.5452257394790649\n",
      "Average steps: 5.599799633026123\n",
      "Eikonal loss: 0.06845901906490326\n",
      "Step 700:\n",
      "Loss: 0.5522616505622864\n",
      "Average steps: 5.588699817657471\n",
      "Eikonal loss: 0.07017623633146286\n",
      "Step 705:\n",
      "Loss: 0.5529743432998657\n",
      "Average steps: 5.598099708557129\n",
      "Eikonal loss: 0.07046113908290863\n",
      "Step 710:\n",
      "Loss: 0.5509205460548401\n",
      "Average steps: 5.561899662017822\n",
      "Eikonal loss: 0.06935521960258484\n",
      "Step 715:\n",
      "Loss: 0.5515003800392151\n",
      "Average steps: 5.549299716949463\n",
      "Eikonal loss: 0.06920259445905685\n",
      "Step 720:\n",
      "Loss: 0.5356212258338928\n",
      "Average steps: 5.584799766540527\n",
      "Eikonal loss: 0.06662651896476746\n",
      "Step 725:\n",
      "Loss: 0.5348756313323975\n",
      "Average steps: 5.598999977111816\n",
      "Eikonal loss: 0.06695380806922913\n",
      "Step 730:\n",
      "Loss: 0.5315707325935364\n",
      "Average steps: 5.60699987411499\n",
      "Eikonal loss: 0.06670350581407547\n",
      "Step 735:\n",
      "Loss: 0.5337734222412109\n",
      "Average steps: 5.632899761199951\n",
      "Eikonal loss: 0.06761372834444046\n",
      "Step 740:\n",
      "Loss: 0.5253996253013611\n",
      "Average steps: 5.647500038146973\n",
      "Eikonal loss: 0.06589385867118835\n",
      "Step 745:\n",
      "Loss: 0.5277106761932373\n",
      "Average steps: 5.624799728393555\n",
      "Eikonal loss: 0.06624781340360641\n",
      "Step 750:\n",
      "Loss: 0.5379864573478699\n",
      "Average steps: 5.644899845123291\n",
      "Eikonal loss: 0.06838022917509079\n",
      "Step 755:\n",
      "Loss: 0.5231679677963257\n",
      "Average steps: 5.654900074005127\n",
      "Eikonal loss: 0.06543686985969543\n",
      "Step 760:\n",
      "Loss: 0.526450514793396\n",
      "Average steps: 5.579699993133545\n",
      "Eikonal loss: 0.0655643492937088\n",
      "Step 765:\n",
      "Loss: 0.5180389881134033\n",
      "Average steps: 5.61870002746582\n",
      "Eikonal loss: 0.06454676389694214\n",
      "Step 770:\n",
      "Loss: 0.5126228928565979\n",
      "Average steps: 5.597399711608887\n",
      "Eikonal loss: 0.06331045925617218\n",
      "Step 775:\n",
      "Loss: 0.520604133605957\n",
      "Average steps: 5.566099643707275\n",
      "Eikonal loss: 0.06451363861560822\n",
      "Step 780:\n",
      "Loss: 0.5215022563934326\n",
      "Average steps: 5.63100004196167\n",
      "Eikonal loss: 0.06542732566595078\n",
      "Step 785:\n",
      "Loss: 0.5112729072570801\n",
      "Average steps: 5.63100004196167\n",
      "Eikonal loss: 0.06291092932224274\n",
      "Step 790:\n",
      "Loss: 0.5125997066497803\n",
      "Average steps: 5.628599643707275\n",
      "Eikonal loss: 0.06286927312612534\n",
      "Step 795:\n",
      "Loss: 0.5062345266342163\n",
      "Average steps: 5.625\n",
      "Eikonal loss: 0.06195090338587761\n",
      "Step 800:\n",
      "Loss: 0.5119441747665405\n",
      "Average steps: 5.651599884033203\n",
      "Eikonal loss: 0.0637122392654419\n",
      "Step 805:\n",
      "Loss: 0.5053514838218689\n",
      "Average steps: 5.657299995422363\n",
      "Eikonal loss: 0.06286559998989105\n",
      "Step 810:\n",
      "Loss: 0.5030502080917358\n",
      "Average steps: 5.62999963760376\n",
      "Eikonal loss: 0.062199849635362625\n",
      "Step 815:\n",
      "Loss: 0.5053644180297852\n",
      "Average steps: 5.586899757385254\n",
      "Eikonal loss: 0.062285665422677994\n",
      "Step 820:\n",
      "Loss: 0.5025116205215454\n",
      "Average steps: 5.603799819946289\n",
      "Eikonal loss: 0.062333326786756516\n",
      "Step 825:\n",
      "Loss: 0.5029964447021484\n",
      "Average steps: 5.639599800109863\n",
      "Eikonal loss: 0.06308280676603317\n",
      "Step 830:\n",
      "Loss: 0.4985036849975586\n",
      "Average steps: 5.61959981918335\n",
      "Eikonal loss: 0.062088046222925186\n",
      "Step 835:\n",
      "Loss: 0.5017484426498413\n",
      "Average steps: 5.592899799346924\n",
      "Eikonal loss: 0.06247482821345329\n",
      "Step 840:\n",
      "Loss: 0.5012410879135132\n",
      "Average steps: 5.610599994659424\n",
      "Eikonal loss: 0.0625600665807724\n",
      "Step 845:\n",
      "Loss: 0.49947771430015564\n",
      "Average steps: 5.574999809265137\n",
      "Eikonal loss: 0.06222135201096535\n",
      "Step 850:\n",
      "Loss: 0.4963815212249756\n",
      "Average steps: 5.571300029754639\n",
      "Eikonal loss: 0.06196528300642967\n",
      "Step 855:\n",
      "Loss: 0.49638599157333374\n",
      "Average steps: 5.596199989318848\n",
      "Eikonal loss: 0.0623985230922699\n",
      "Step 860:\n",
      "Loss: 0.4928920865058899\n",
      "Average steps: 5.5843000411987305\n",
      "Eikonal loss: 0.06158340722322464\n",
      "Step 865:\n",
      "Loss: 0.49262237548828125\n",
      "Average steps: 5.586299896240234\n",
      "Eikonal loss: 0.06150466203689575\n",
      "Step 870:\n",
      "Loss: 0.49754875898361206\n",
      "Average steps: 5.5518999099731445\n",
      "Eikonal loss: 0.06188197433948517\n",
      "Step 875:\n",
      "Loss: 0.5032379627227783\n",
      "Average steps: 5.606800079345703\n",
      "Eikonal loss: 0.06372997909784317\n",
      "Step 880:\n",
      "Loss: 0.4945051074028015\n",
      "Average steps: 5.602099895477295\n",
      "Eikonal loss: 0.06202435865998268\n",
      "Step 885:\n",
      "Loss: 0.5082541704177856\n",
      "Average steps: 5.529900074005127\n",
      "Eikonal loss: 0.06396164000034332\n",
      "Step 890:\n",
      "Loss: 0.4831838011741638\n",
      "Average steps: 5.577899932861328\n",
      "Eikonal loss: 0.05959731340408325\n",
      "Step 895:\n",
      "Loss: 0.490997850894928\n",
      "Average steps: 5.59529972076416\n",
      "Eikonal loss: 0.06125393137335777\n",
      "Step 900:\n",
      "Loss: 0.4803808033466339\n",
      "Average steps: 5.543799877166748\n",
      "Eikonal loss: 0.058258917182683945\n",
      "Step 905:\n",
      "Loss: 0.4783557653427124\n",
      "Average steps: 5.554800033569336\n",
      "Eikonal loss: 0.05807267129421234\n",
      "Step 910:\n",
      "Loss: 0.4844653606414795\n",
      "Average steps: 5.576600074768066\n",
      "Eikonal loss: 0.05929872393608093\n",
      "Step 915:\n",
      "Loss: 0.4813024401664734\n",
      "Average steps: 5.588500022888184\n",
      "Eikonal loss: 0.05871068686246872\n",
      "Step 920:\n",
      "Loss: 0.4914848804473877\n",
      "Average steps: 5.6057000160217285\n",
      "Eikonal loss: 0.061280202120542526\n",
      "Step 925:\n",
      "Loss: 0.4898577928543091\n",
      "Average steps: 5.615200042724609\n",
      "Eikonal loss: 0.061198581010103226\n",
      "Step 930:\n",
      "Loss: 0.4805335998535156\n",
      "Average steps: 5.582099914550781\n",
      "Eikonal loss: 0.05942574888467789\n",
      "Step 935:\n",
      "Loss: 0.47611239552497864\n",
      "Average steps: 5.565999984741211\n",
      "Eikonal loss: 0.05879983678460121\n",
      "Step 940:\n",
      "Loss: 0.47358882427215576\n",
      "Average steps: 5.552099704742432\n",
      "Eikonal loss: 0.058502741158008575\n",
      "Step 945:\n",
      "Loss: 0.4588942527770996\n",
      "Average steps: 5.558599948883057\n",
      "Eikonal loss: 0.056246910244226456\n",
      "Step 950:\n",
      "Loss: 0.46002858877182007\n",
      "Average steps: 5.546899795532227\n",
      "Eikonal loss: 0.05677260085940361\n",
      "Step 955:\n",
      "Loss: 0.46709778904914856\n",
      "Average steps: 5.538300037384033\n",
      "Eikonal loss: 0.05857851728796959\n",
      "Step 960:\n",
      "Loss: 0.4677521884441376\n",
      "Average steps: 5.512599945068359\n",
      "Eikonal loss: 0.05838144198060036\n",
      "Step 965:\n",
      "Loss: 0.4628680646419525\n",
      "Average steps: 5.523799896240234\n",
      "Eikonal loss: 0.05760582908987999\n",
      "Step 970:\n",
      "Loss: 0.4668317437171936\n",
      "Average steps: 5.543399810791016\n",
      "Eikonal loss: 0.05855899676680565\n",
      "Step 975:\n",
      "Loss: 0.46661657094955444\n",
      "Average steps: 5.567800045013428\n",
      "Eikonal loss: 0.05876852944493294\n",
      "Step 980:\n",
      "Loss: 0.46384385228157043\n",
      "Average steps: 5.565700054168701\n",
      "Eikonal loss: 0.05850977823138237\n",
      "Step 985:\n",
      "Loss: 0.46555477380752563\n",
      "Average steps: 5.5594000816345215\n",
      "Eikonal loss: 0.058724381029605865\n",
      "Step 990:\n",
      "Loss: 0.4671735167503357\n",
      "Average steps: 5.567399978637695\n",
      "Eikonal loss: 0.05890825018286705\n",
      "Step 995:\n",
      "Loss: 0.45788538455963135\n",
      "Average steps: 5.548799991607666\n",
      "Eikonal loss: 0.05686626955866814\n",
      "Step 1000:\n",
      "Loss: 0.4468423128128052\n",
      "Average steps: 5.540800094604492\n",
      "Eikonal loss: 0.05459936335682869\n",
      "Step 1005:\n",
      "Loss: 0.438054621219635\n",
      "Average steps: 5.5355000495910645\n",
      "Eikonal loss: 0.05281573534011841\n",
      "Step 1010:\n",
      "Loss: 0.4507206976413727\n",
      "Average steps: 5.553699970245361\n",
      "Eikonal loss: 0.055540066212415695\n",
      "Step 1015:\n",
      "Loss: 0.45437392592430115\n",
      "Average steps: 5.575900077819824\n",
      "Eikonal loss: 0.056633360683918\n",
      "Step 1020:\n",
      "Loss: 0.4505108594894409\n",
      "Average steps: 5.559700012207031\n",
      "Eikonal loss: 0.055532194674015045\n",
      "Step 1025:\n",
      "Loss: 0.44787490367889404\n",
      "Average steps: 5.567800045013428\n",
      "Eikonal loss: 0.05522682145237923\n",
      "Step 1030:\n",
      "Loss: 0.4519105553627014\n",
      "Average steps: 5.5569000244140625\n",
      "Eikonal loss: 0.0560448132455349\n",
      "Step 1035:\n",
      "Loss: 0.45729368925094604\n",
      "Average steps: 5.541999816894531\n",
      "Eikonal loss: 0.056881800293922424\n",
      "Step 1040:\n",
      "Loss: 0.4470027983188629\n",
      "Average steps: 5.549299716949463\n",
      "Eikonal loss: 0.0551130436360836\n",
      "Step 1045:\n",
      "Loss: 0.44085824489593506\n",
      "Average steps: 5.534499645233154\n",
      "Eikonal loss: 0.05371567979454994\n",
      "Step 1050:\n",
      "Loss: 0.4357660710811615\n",
      "Average steps: 5.525099754333496\n",
      "Eikonal loss: 0.05266835540533066\n",
      "Step 1055:\n",
      "Loss: 0.43945157527923584\n",
      "Average steps: 5.560400009155273\n",
      "Eikonal loss: 0.05387769266963005\n",
      "Step 1060:\n",
      "Loss: 0.43935269117355347\n",
      "Average steps: 5.540800094604492\n",
      "Eikonal loss: 0.05333724990487099\n",
      "Step 1065:\n",
      "Loss: 0.43653404712677\n",
      "Average steps: 5.539599895477295\n",
      "Eikonal loss: 0.05301273986697197\n",
      "Step 1070:\n",
      "Loss: 0.4292827248573303\n",
      "Average steps: 5.503699779510498\n",
      "Eikonal loss: 0.0513906329870224\n",
      "Step 1075:\n",
      "Loss: 0.4249234199523926\n",
      "Average steps: 5.512099742889404\n",
      "Eikonal loss: 0.05089857429265976\n",
      "Step 1080:\n",
      "Loss: 0.428940087556839\n",
      "Average steps: 5.493899822235107\n",
      "Eikonal loss: 0.051969073712825775\n",
      "Step 1085:\n",
      "Loss: 0.4401280879974365\n",
      "Average steps: 5.4828996658325195\n",
      "Eikonal loss: 0.05452169477939606\n",
      "Step 1090:\n",
      "Loss: 0.4282552897930145\n",
      "Average steps: 5.433700084686279\n",
      "Eikonal loss: 0.05187712982296944\n",
      "Step 1095:\n",
      "Loss: 0.4251317083835602\n",
      "Average steps: 5.431299686431885\n",
      "Eikonal loss: 0.05118701979517937\n",
      "Step 1100:\n",
      "Loss: 0.41684842109680176\n",
      "Average steps: 5.448899745941162\n",
      "Eikonal loss: 0.04943391680717468\n",
      "Step 1105:\n",
      "Loss: 0.4357452988624573\n",
      "Average steps: 5.534800052642822\n",
      "Eikonal loss: 0.05369289219379425\n",
      "Step 1110:\n",
      "Loss: 0.4339514970779419\n",
      "Average steps: 5.520599842071533\n",
      "Eikonal loss: 0.05302857980132103\n",
      "Step 1115:\n",
      "Loss: 0.4186570644378662\n",
      "Average steps: 5.41510009765625\n",
      "Eikonal loss: 0.048857398331165314\n",
      "Step 1120:\n",
      "Loss: 0.4170323312282562\n",
      "Average steps: 5.42549991607666\n",
      "Eikonal loss: 0.04868320748209953\n",
      "Step 1125:\n",
      "Loss: 0.4135501980781555\n",
      "Average steps: 5.460399627685547\n",
      "Eikonal loss: 0.04880041629076004\n",
      "Step 1130:\n",
      "Loss: 0.4045361280441284\n",
      "Average steps: 5.458499908447266\n",
      "Eikonal loss: 0.04729514569044113\n",
      "Step 1135:\n",
      "Loss: 0.4287591576576233\n",
      "Average steps: 5.520299911499023\n",
      "Eikonal loss: 0.052889835089445114\n",
      "Step 1140:\n",
      "Loss: 0.4181804955005646\n",
      "Average steps: 5.497299671173096\n",
      "Eikonal loss: 0.05027220398187637\n",
      "Step 1145:\n",
      "Loss: 0.4384365975856781\n",
      "Average steps: 5.474899768829346\n",
      "Eikonal loss: 0.05365921929478645\n",
      "Step 1150:\n",
      "Loss: 0.4376261234283447\n",
      "Average steps: 5.462699890136719\n",
      "Eikonal loss: 0.05303597450256348\n",
      "Step 1155:\n",
      "Loss: 0.4333682656288147\n",
      "Average steps: 5.501999855041504\n",
      "Eikonal loss: 0.05229826271533966\n",
      "Step 1160:\n",
      "Loss: 0.43581897020339966\n",
      "Average steps: 5.496799945831299\n",
      "Eikonal loss: 0.05233355239033699\n",
      "Step 1165:\n",
      "Loss: 0.428073525428772\n",
      "Average steps: 5.520899772644043\n",
      "Eikonal loss: 0.05091416463255882\n",
      "Step 1170:\n",
      "Loss: 0.4346095025539398\n",
      "Average steps: 5.51099967956543\n",
      "Eikonal loss: 0.051816485822200775\n",
      "Step 1175:\n",
      "Loss: 0.43063491582870483\n",
      "Average steps: 5.519399642944336\n",
      "Eikonal loss: 0.05127011239528656\n",
      "Step 1180:\n",
      "Loss: 0.4357875883579254\n",
      "Average steps: 5.631700038909912\n",
      "Eikonal loss: 0.05313980206847191\n",
      "Step 1185:\n",
      "Loss: 0.44960257411003113\n",
      "Average steps: 5.636300086975098\n",
      "Eikonal loss: 0.05576297268271446\n",
      "Step 1190:\n",
      "Loss: 0.441201388835907\n",
      "Average steps: 5.603699684143066\n",
      "Eikonal loss: 0.05389787629246712\n",
      "Step 1195:\n",
      "Loss: 0.42749279737472534\n",
      "Average steps: 5.561399936676025\n",
      "Eikonal loss: 0.0511203333735466\n",
      "Step 1200:\n",
      "Loss: 0.4275229573249817\n",
      "Average steps: 5.496999740600586\n",
      "Eikonal loss: 0.050415683537721634\n",
      "Step 1205:\n",
      "Loss: 0.42092329263687134\n",
      "Average steps: 5.495599746704102\n",
      "Eikonal loss: 0.04918216913938522\n",
      "Step 1210:\n",
      "Loss: 0.4171159267425537\n",
      "Average steps: 5.489500045776367\n",
      "Eikonal loss: 0.04875490069389343\n",
      "Step 1215:\n",
      "Loss: 0.41691410541534424\n",
      "Average steps: 5.515100002288818\n",
      "Eikonal loss: 0.049173805862665176\n",
      "Step 1220:\n",
      "Loss: 0.4098637104034424\n",
      "Average steps: 5.442699909210205\n",
      "Eikonal loss: 0.04742042347788811\n",
      "Step 1225:\n",
      "Loss: 0.4088287055492401\n",
      "Average steps: 5.422800064086914\n",
      "Eikonal loss: 0.04710766673088074\n",
      "Step 1230:\n",
      "Loss: 0.3865569829940796\n",
      "Average steps: 5.440499782562256\n",
      "Eikonal loss: 0.04308195784687996\n",
      "Step 1235:\n",
      "Loss: 0.4095708131790161\n",
      "Average steps: 5.56689977645874\n",
      "Eikonal loss: 0.04852927848696709\n",
      "Step 1240:\n",
      "Loss: 0.45191633701324463\n",
      "Average steps: 5.751800060272217\n",
      "Eikonal loss: 0.05784687399864197\n",
      "Step 1245:\n",
      "Loss: 0.43079984188079834\n",
      "Average steps: 5.70989990234375\n",
      "Eikonal loss: 0.0529773123562336\n",
      "Step 1250:\n",
      "Loss: 0.40715616941452026\n",
      "Average steps: 5.517999649047852\n",
      "Eikonal loss: 0.046829815953969955\n",
      "Step 1255:\n",
      "Loss: 0.39704781770706177\n",
      "Average steps: 5.472799777984619\n",
      "Eikonal loss: 0.044756922870874405\n",
      "Step 1260:\n",
      "Loss: 0.41271090507507324\n",
      "Average steps: 5.479499816894531\n",
      "Eikonal loss: 0.048344310373067856\n",
      "Step 1265:\n",
      "Loss: 0.4137951135635376\n",
      "Average steps: 5.46619987487793\n",
      "Eikonal loss: 0.04860416799783707\n",
      "Step 1270:\n",
      "Loss: 0.41548579931259155\n",
      "Average steps: 5.462399959564209\n",
      "Eikonal loss: 0.04892738536000252\n",
      "Step 1275:\n",
      "Loss: 0.41629093885421753\n",
      "Average steps: 5.5030999183654785\n",
      "Eikonal loss: 0.0496661476790905\n",
      "Step 1280:\n",
      "Loss: 0.41530153155326843\n",
      "Average steps: 5.527100086212158\n",
      "Eikonal loss: 0.04943443089723587\n",
      "Step 1285:\n",
      "Loss: 0.42065858840942383\n",
      "Average steps: 5.513299942016602\n",
      "Eikonal loss: 0.04993985593318939\n",
      "Step 1290:\n",
      "Loss: 0.42025578022003174\n",
      "Average steps: 5.569599628448486\n",
      "Eikonal loss: 0.0503506101667881\n",
      "Step 1295:\n",
      "Loss: 0.41418981552124023\n",
      "Average steps: 5.571399688720703\n",
      "Eikonal loss: 0.04927854612469673\n",
      "Step 1300:\n",
      "Loss: 0.4098219573497772\n",
      "Average steps: 5.536099910736084\n",
      "Eikonal loss: 0.04807719588279724\n",
      "Step 1305:\n",
      "Loss: 0.4140259623527527\n",
      "Average steps: 5.486199855804443\n",
      "Eikonal loss: 0.04857237637042999\n",
      "Step 1310:\n",
      "Loss: 0.40327662229537964\n",
      "Average steps: 5.560800075531006\n",
      "Eikonal loss: 0.04724135622382164\n",
      "Step 1315:\n",
      "Loss: 0.4004429578781128\n",
      "Average steps: 5.538599967956543\n",
      "Eikonal loss: 0.04629684239625931\n",
      "Step 1320:\n",
      "Loss: 0.40388137102127075\n",
      "Average steps: 5.508500099182129\n",
      "Eikonal loss: 0.04679221659898758\n",
      "Step 1325:\n",
      "Loss: 0.4069134593009949\n",
      "Average steps: 5.485899925231934\n",
      "Eikonal loss: 0.04730280116200447\n",
      "Step 1330:\n",
      "Loss: 0.40256768465042114\n",
      "Average steps: 5.476399898529053\n",
      "Eikonal loss: 0.04624126851558685\n",
      "Step 1335:\n",
      "Loss: 0.38833409547805786\n",
      "Average steps: 5.514499664306641\n",
      "Eikonal loss: 0.044028934091329575\n",
      "Step 1340:\n",
      "Loss: 0.38371264934539795\n",
      "Average steps: 5.520400047302246\n",
      "Eikonal loss: 0.043442338705062866\n",
      "Step 1345:\n",
      "Loss: 0.3820721507072449\n",
      "Average steps: 5.554800033569336\n",
      "Eikonal loss: 0.04348670318722725\n",
      "Step 1350:\n",
      "Loss: 0.3715395927429199\n",
      "Average steps: 5.525199890136719\n",
      "Eikonal loss: 0.04152496159076691\n",
      "Step 1355:\n",
      "Loss: 0.37079402804374695\n",
      "Average steps: 5.502699851989746\n",
      "Eikonal loss: 0.04159225896000862\n",
      "Step 1360:\n",
      "Loss: 0.38433191180229187\n",
      "Average steps: 5.494100093841553\n",
      "Eikonal loss: 0.044600170105695724\n",
      "Step 1365:\n",
      "Loss: 0.3815978169441223\n",
      "Average steps: 5.447499752044678\n",
      "Eikonal loss: 0.04386317357420921\n",
      "Step 1370:\n",
      "Loss: 0.3739073574542999\n",
      "Average steps: 5.453099727630615\n",
      "Eikonal loss: 0.04252391681075096\n",
      "Step 1375:\n",
      "Loss: 0.37623143196105957\n",
      "Average steps: 5.396100044250488\n",
      "Eikonal loss: 0.04163962975144386\n",
      "Step 1380:\n",
      "Loss: 0.38747459650039673\n",
      "Average steps: 5.422399997711182\n",
      "Eikonal loss: 0.0442965105175972\n",
      "Step 1385:\n",
      "Loss: 0.3773554861545563\n",
      "Average steps: 5.467299938201904\n",
      "Eikonal loss: 0.0430707223713398\n",
      "Step 1390:\n",
      "Loss: 0.3739832043647766\n",
      "Average steps: 5.459499835968018\n",
      "Eikonal loss: 0.04237429425120354\n",
      "Step 1395:\n",
      "Loss: 0.3701291084289551\n",
      "Average steps: 5.4552998542785645\n",
      "Eikonal loss: 0.041598398238420486\n",
      "Step 1400:\n",
      "Loss: 0.36019307374954224\n",
      "Average steps: 5.467299938201904\n",
      "Eikonal loss: 0.03953308239579201\n",
      "Step 1405:\n",
      "Loss: 0.3725331425666809\n",
      "Average steps: 5.495999813079834\n",
      "Eikonal loss: 0.04229400306940079\n",
      "Step 1410:\n",
      "Loss: 0.36753952503204346\n",
      "Average steps: 5.5381999015808105\n",
      "Eikonal loss: 0.04135669767856598\n",
      "Step 1415:\n",
      "Loss: 0.3629634976387024\n",
      "Average steps: 5.5030999183654785\n",
      "Eikonal loss: 0.04000532254576683\n",
      "Step 1420:\n",
      "Loss: 0.3628324866294861\n",
      "Average steps: 5.500699996948242\n",
      "Eikonal loss: 0.03991277515888214\n",
      "Step 1425:\n",
      "Loss: 0.3631356954574585\n",
      "Average steps: 5.5\n",
      "Eikonal loss: 0.040365345776081085\n",
      "Step 1430:\n",
      "Loss: 0.35846078395843506\n",
      "Average steps: 5.473199844360352\n",
      "Eikonal loss: 0.03944261744618416\n",
      "Step 1435:\n",
      "Loss: 0.3539249002933502\n",
      "Average steps: 5.468100070953369\n",
      "Eikonal loss: 0.038711804896593094\n",
      "Step 1440:\n",
      "Loss: 0.3656873106956482\n",
      "Average steps: 5.495800018310547\n",
      "Eikonal loss: 0.04125566408038139\n",
      "Step 1445:\n",
      "Loss: 0.36105212569236755\n",
      "Average steps: 5.508299827575684\n",
      "Eikonal loss: 0.040113337337970734\n",
      "Step 1450:\n",
      "Loss: 0.3603169918060303\n",
      "Average steps: 5.510799884796143\n",
      "Eikonal loss: 0.03997573256492615\n",
      "Step 1455:\n",
      "Loss: 0.3561520278453827\n",
      "Average steps: 5.494800090789795\n",
      "Eikonal loss: 0.039429910480976105\n",
      "Step 1460:\n",
      "Loss: 0.3628196716308594\n",
      "Average steps: 5.517999649047852\n",
      "Eikonal loss: 0.041531458497047424\n",
      "Step 1465:\n",
      "Loss: 0.3564313054084778\n",
      "Average steps: 5.462299823760986\n",
      "Eikonal loss: 0.03982270881533623\n",
      "Step 1470:\n",
      "Loss: 0.3566402196884155\n",
      "Average steps: 5.44350004196167\n",
      "Eikonal loss: 0.03970262408256531\n",
      "Step 1475:\n",
      "Loss: 0.35109835863113403\n",
      "Average steps: 5.448699951171875\n",
      "Eikonal loss: 0.038938265293836594\n",
      "Step 1480:\n",
      "Loss: 0.3520028591156006\n",
      "Average steps: 5.432699680328369\n",
      "Eikonal loss: 0.038935452699661255\n",
      "Step 1485:\n",
      "Loss: 0.3479149341583252\n",
      "Average steps: 5.460000038146973\n",
      "Eikonal loss: 0.03833530470728874\n",
      "Step 1490:\n",
      "Loss: 0.35082927346229553\n",
      "Average steps: 5.508899688720703\n",
      "Eikonal loss: 0.03925907239317894\n",
      "Step 1495:\n",
      "Loss: 0.36000123620033264\n",
      "Average steps: 5.524699687957764\n",
      "Eikonal loss: 0.04092845320701599\n",
      "Step 1500:\n",
      "Loss: 0.3529900908470154\n",
      "Average steps: 5.554100036621094\n",
      "Eikonal loss: 0.03967783600091934\n",
      "Step 1505:\n",
      "Loss: 0.35811105370521545\n",
      "Average steps: 5.536799907684326\n",
      "Eikonal loss: 0.04037660360336304\n",
      "Step 1510:\n",
      "Loss: 0.3670423924922943\n",
      "Average steps: 5.5625996589660645\n",
      "Eikonal loss: 0.0423588789999485\n",
      "Step 1515:\n",
      "Loss: 0.3787956237792969\n",
      "Average steps: 5.566299915313721\n",
      "Eikonal loss: 0.04464827850461006\n",
      "Step 1520:\n",
      "Loss: 0.37991589307785034\n",
      "Average steps: 5.573800086975098\n",
      "Eikonal loss: 0.04511567950248718\n",
      "Step 1525:\n",
      "Loss: 0.3610644042491913\n",
      "Average steps: 5.549599647521973\n",
      "Eikonal loss: 0.04126420617103577\n",
      "Step 1530:\n",
      "Loss: 0.3677991032600403\n",
      "Average steps: 5.564899921417236\n",
      "Eikonal loss: 0.04282144457101822\n",
      "Step 1535:\n",
      "Loss: 0.3576345443725586\n",
      "Average steps: 5.560800075531006\n",
      "Eikonal loss: 0.040726643055677414\n",
      "Step 1540:\n",
      "Loss: 0.358822226524353\n",
      "Average steps: 5.529099941253662\n",
      "Eikonal loss: 0.040741000324487686\n",
      "Step 1545:\n",
      "Loss: 0.35026460886001587\n",
      "Average steps: 5.570699691772461\n",
      "Eikonal loss: 0.039656657725572586\n",
      "Step 1550:\n",
      "Loss: 0.35397398471832275\n",
      "Average steps: 5.590799808502197\n",
      "Eikonal loss: 0.04051957651972771\n",
      "Step 1555:\n",
      "Loss: 0.35282212495803833\n",
      "Average steps: 5.589099884033203\n",
      "Eikonal loss: 0.04027501493692398\n",
      "Step 1560:\n",
      "Loss: 0.3541088104248047\n",
      "Average steps: 5.535699844360352\n",
      "Eikonal loss: 0.04003274813294411\n",
      "Step 1565:\n",
      "Loss: 0.35075175762176514\n",
      "Average steps: 5.506799697875977\n",
      "Eikonal loss: 0.039032015949487686\n",
      "Step 1570:\n",
      "Loss: 0.3571556806564331\n",
      "Average steps: 5.548999786376953\n",
      "Eikonal loss: 0.04107082635164261\n",
      "Step 1575:\n",
      "Loss: 0.3631832003593445\n",
      "Average steps: 5.582499980926514\n",
      "Eikonal loss: 0.04269414395093918\n",
      "Step 1580:\n",
      "Loss: 0.36226028203964233\n",
      "Average steps: 5.5655999183654785\n",
      "Eikonal loss: 0.042167287319898605\n",
      "Step 1585:\n",
      "Loss: 0.3570539057254791\n",
      "Average steps: 5.534899711608887\n",
      "Eikonal loss: 0.04059610888361931\n",
      "Step 1590:\n",
      "Loss: 0.35485708713531494\n",
      "Average steps: 5.552000045776367\n",
      "Eikonal loss: 0.04061293229460716\n",
      "Step 1595:\n",
      "Loss: 0.3519495725631714\n",
      "Average steps: 5.546799659729004\n",
      "Eikonal loss: 0.040171265602111816\n",
      "Step 1600:\n",
      "Loss: 0.3578253388404846\n",
      "Average steps: 5.56119966506958\n",
      "Eikonal loss: 0.04201778769493103\n",
      "Step 1605:\n",
      "Loss: 0.35174381732940674\n",
      "Average steps: 5.554999828338623\n",
      "Eikonal loss: 0.04110576584935188\n",
      "Step 1610:\n",
      "Loss: 0.3498741686344147\n",
      "Average steps: 5.508999824523926\n",
      "Eikonal loss: 0.04058128595352173\n",
      "Step 1615:\n",
      "Loss: 0.3405461311340332\n",
      "Average steps: 5.51039981842041\n",
      "Eikonal loss: 0.038827765733003616\n",
      "Step 1620:\n",
      "Loss: 0.35488161444664\n",
      "Average steps: 5.529399871826172\n",
      "Eikonal loss: 0.04161657392978668\n",
      "Step 1625:\n",
      "Loss: 0.34541237354278564\n",
      "Average steps: 5.547699928283691\n",
      "Eikonal loss: 0.039809949696063995\n",
      "Step 1630:\n",
      "Loss: 0.3728711009025574\n",
      "Average steps: 5.532899856567383\n",
      "Eikonal loss: 0.045156415551900864\n",
      "Step 1635:\n",
      "Loss: 0.34409913420677185\n",
      "Average steps: 5.522500038146973\n",
      "Eikonal loss: 0.039603590965270996\n",
      "Step 1640:\n",
      "Loss: 0.3328816294670105\n",
      "Average steps: 5.536299705505371\n",
      "Eikonal loss: 0.03786502406001091\n",
      "Step 1645:\n",
      "Loss: 0.3304017186164856\n",
      "Average steps: 5.489699840545654\n",
      "Eikonal loss: 0.037277042865753174\n",
      "Step 1650:\n",
      "Loss: 0.33955925703048706\n",
      "Average steps: 5.490799903869629\n",
      "Eikonal loss: 0.03914637118577957\n",
      "Step 1655:\n",
      "Loss: 0.33888575434684753\n",
      "Average steps: 5.509099960327148\n",
      "Eikonal loss: 0.03915562108159065\n",
      "Step 1660:\n",
      "Loss: 0.33384183049201965\n",
      "Average steps: 5.490299701690674\n",
      "Eikonal loss: 0.03803658112883568\n",
      "Step 1665:\n",
      "Loss: 0.3343522250652313\n",
      "Average steps: 5.499799728393555\n",
      "Eikonal loss: 0.03794589638710022\n",
      "Step 1670:\n",
      "Loss: 0.3376539945602417\n",
      "Average steps: 5.51200008392334\n",
      "Eikonal loss: 0.038509197533130646\n",
      "Step 1675:\n",
      "Loss: 0.33445000648498535\n",
      "Average steps: 5.5690999031066895\n",
      "Eikonal loss: 0.037974726408720016\n",
      "Step 1680:\n",
      "Loss: 0.3353741466999054\n",
      "Average steps: 5.548599720001221\n",
      "Eikonal loss: 0.03806586191058159\n",
      "Step 1685:\n",
      "Loss: 0.3321874141693115\n",
      "Average steps: 5.5548996925354\n",
      "Eikonal loss: 0.03743705153465271\n",
      "Step 1690:\n",
      "Loss: 0.34358906745910645\n",
      "Average steps: 5.592899799346924\n",
      "Eikonal loss: 0.03992204740643501\n",
      "Step 1695:\n",
      "Loss: 0.3425399661064148\n",
      "Average steps: 5.586999893188477\n",
      "Eikonal loss: 0.039392560720443726\n",
      "Step 1700:\n",
      "Loss: 0.3450152277946472\n",
      "Average steps: 5.580399990081787\n",
      "Eikonal loss: 0.03983050584793091\n",
      "Step 1705:\n",
      "Loss: 0.3410504460334778\n",
      "Average steps: 5.54449987411499\n",
      "Eikonal loss: 0.039125707000494\n",
      "Step 1710:\n",
      "Loss: 0.3389987349510193\n",
      "Average steps: 5.560400009155273\n",
      "Eikonal loss: 0.03899571672081947\n",
      "Step 1715:\n",
      "Loss: 0.33212679624557495\n",
      "Average steps: 5.548299789428711\n",
      "Eikonal loss: 0.037642981857061386\n",
      "Step 1720:\n",
      "Loss: 0.3373628854751587\n",
      "Average steps: 5.5615997314453125\n",
      "Eikonal loss: 0.038766808807849884\n",
      "Step 1725:\n",
      "Loss: 0.3364718556404114\n",
      "Average steps: 5.513499736785889\n",
      "Eikonal loss: 0.038166895508766174\n",
      "Step 1730:\n",
      "Loss: 0.3340410590171814\n",
      "Average steps: 5.540999889373779\n",
      "Eikonal loss: 0.03791743144392967\n",
      "Step 1735:\n",
      "Loss: 0.3576507568359375\n",
      "Average steps: 5.537099838256836\n",
      "Eikonal loss: 0.042729832231998444\n",
      "Step 1740:\n",
      "Loss: 0.35623395442962646\n",
      "Average steps: 5.551300048828125\n",
      "Eikonal loss: 0.04260951280593872\n",
      "Step 1745:\n",
      "Loss: 0.3629765510559082\n",
      "Average steps: 5.598700046539307\n",
      "Eikonal loss: 0.04428481310606003\n",
      "Step 1750:\n",
      "Loss: 0.34285810589790344\n",
      "Average steps: 5.501100063323975\n",
      "Eikonal loss: 0.03966907411813736\n",
      "Step 1755:\n",
      "Loss: 0.35025107860565186\n",
      "Average steps: 5.463599681854248\n",
      "Eikonal loss: 0.04091868922114372\n",
      "Step 1760:\n",
      "Loss: 0.3459274470806122\n",
      "Average steps: 5.535799980163574\n",
      "Eikonal loss: 0.040406908839941025\n",
      "Step 1765:\n",
      "Loss: 0.333007276058197\n",
      "Average steps: 5.598999977111816\n",
      "Eikonal loss: 0.0381646454334259\n",
      "Step 1770:\n",
      "Loss: 0.33341479301452637\n",
      "Average steps: 5.5548996925354\n",
      "Eikonal loss: 0.037943217903375626\n",
      "Step 1775:\n",
      "Loss: 0.3288377821445465\n",
      "Average steps: 5.555099964141846\n",
      "Eikonal loss: 0.03716477379202843\n",
      "Step 1780:\n",
      "Loss: 0.3350619673728943\n",
      "Average steps: 5.5569000244140625\n",
      "Eikonal loss: 0.03844593092799187\n",
      "Step 1785:\n",
      "Loss: 0.32727450132369995\n",
      "Average steps: 5.577300071716309\n",
      "Eikonal loss: 0.036917392164468765\n",
      "Step 1790:\n",
      "Loss: 0.34156879782676697\n",
      "Average steps: 5.591799736022949\n",
      "Eikonal loss: 0.03984074294567108\n",
      "Step 1795:\n",
      "Loss: 0.33785489201545715\n",
      "Average steps: 5.574399948120117\n",
      "Eikonal loss: 0.039044905453920364\n",
      "Step 1800:\n",
      "Loss: 0.3401031494140625\n",
      "Average steps: 5.590099811553955\n",
      "Eikonal loss: 0.03963495418429375\n",
      "Step 1805:\n",
      "Loss: 0.3392488360404968\n",
      "Average steps: 5.577300071716309\n",
      "Eikonal loss: 0.03937673941254616\n",
      "Step 1810:\n",
      "Loss: 0.33824071288108826\n",
      "Average steps: 5.5553998947143555\n",
      "Eikonal loss: 0.03906514495611191\n",
      "Step 1815:\n",
      "Loss: 0.3352331519126892\n",
      "Average steps: 5.531099796295166\n",
      "Eikonal loss: 0.038377970457077026\n",
      "Step 1820:\n",
      "Loss: 0.3280688524246216\n",
      "Average steps: 5.583099842071533\n",
      "Eikonal loss: 0.037424154579639435\n",
      "Step 1825:\n",
      "Loss: 0.34022819995880127\n",
      "Average steps: 5.630899906158447\n",
      "Eikonal loss: 0.04007250815629959\n",
      "Step 1830:\n",
      "Loss: 0.3389396667480469\n",
      "Average steps: 5.599699974060059\n",
      "Eikonal loss: 0.03936762735247612\n",
      "Step 1835:\n",
      "Loss: 0.3327493667602539\n",
      "Average steps: 5.45959997177124\n",
      "Eikonal loss: 0.03732982650399208\n",
      "Step 1840:\n",
      "Loss: 0.33389249444007874\n",
      "Average steps: 5.527899742126465\n",
      "Eikonal loss: 0.038454875349998474\n",
      "Step 1845:\n",
      "Loss: 0.3662773668766022\n",
      "Average steps: 5.726999759674072\n",
      "Eikonal loss: 0.04518909007310867\n",
      "Step 1850:\n",
      "Loss: 0.34952646493911743\n",
      "Average steps: 5.700599670410156\n",
      "Eikonal loss: 0.04160032048821449\n",
      "Step 1855:\n",
      "Loss: 0.34293797612190247\n",
      "Average steps: 5.49370002746582\n",
      "Eikonal loss: 0.03987221047282219\n",
      "Step 1860:\n",
      "Loss: 0.3381626605987549\n",
      "Average steps: 5.441699981689453\n",
      "Eikonal loss: 0.03890543803572655\n",
      "Step 1865:\n",
      "Loss: 0.3379785418510437\n",
      "Average steps: 5.570499897003174\n",
      "Eikonal loss: 0.03966914117336273\n",
      "Step 1870:\n",
      "Loss: 0.3461992144584656\n",
      "Average steps: 5.688899993896484\n",
      "Eikonal loss: 0.041200630366802216\n",
      "Step 1875:\n",
      "Loss: 0.3575226366519928\n",
      "Average steps: 5.719799995422363\n",
      "Eikonal loss: 0.04295037314295769\n",
      "Step 1880:\n",
      "Loss: 0.33695679903030396\n",
      "Average steps: 5.598700046539307\n",
      "Eikonal loss: 0.038938969373703\n",
      "Step 1885:\n",
      "Loss: 0.3387623429298401\n",
      "Average steps: 5.613499641418457\n",
      "Eikonal loss: 0.03931228443980217\n",
      "Step 1890:\n",
      "Loss: 0.33487266302108765\n",
      "Average steps: 5.659499645233154\n",
      "Eikonal loss: 0.03858296945691109\n",
      "Step 1895:\n",
      "Loss: 0.3410196602344513\n",
      "Average steps: 5.646699905395508\n",
      "Eikonal loss: 0.039510563015937805\n",
      "Step 1900:\n",
      "Loss: 0.3407793939113617\n",
      "Average steps: 5.6433000564575195\n",
      "Eikonal loss: 0.039494745433330536\n",
      "Step 1905:\n",
      "Loss: 0.3331121802330017\n",
      "Average steps: 5.668799877166748\n",
      "Eikonal loss: 0.038217443972826004\n",
      "Step 1910:\n",
      "Loss: 0.33567190170288086\n",
      "Average steps: 5.641299724578857\n",
      "Eikonal loss: 0.038640376180410385\n",
      "Step 1915:\n",
      "Loss: 0.32874053716659546\n",
      "Average steps: 5.622099876403809\n",
      "Eikonal loss: 0.037236351519823074\n",
      "Step 1920:\n",
      "Loss: 0.3255046010017395\n",
      "Average steps: 5.641499996185303\n",
      "Eikonal loss: 0.03675172105431557\n",
      "Step 1925:\n",
      "Loss: 0.32990869879722595\n",
      "Average steps: 5.638400077819824\n",
      "Eikonal loss: 0.037659499794244766\n",
      "Step 1930:\n",
      "Loss: 0.32910478115081787\n",
      "Average steps: 5.586199760437012\n",
      "Eikonal loss: 0.037540990859270096\n",
      "Step 1935:\n",
      "Loss: 0.33000263571739197\n",
      "Average steps: 5.630300045013428\n",
      "Eikonal loss: 0.03778575733304024\n",
      "Step 1940:\n",
      "Loss: 0.3274608254432678\n",
      "Average steps: 5.647500038146973\n",
      "Eikonal loss: 0.037218816578388214\n",
      "Step 1945:\n",
      "Loss: 0.3244727849960327\n",
      "Average steps: 5.619499683380127\n",
      "Eikonal loss: 0.03656959906220436\n",
      "Step 1950:\n",
      "Loss: 0.3252851068973541\n",
      "Average steps: 5.634399890899658\n",
      "Eikonal loss: 0.03658689931035042\n",
      "Step 1955:\n",
      "Loss: 0.32947102189064026\n",
      "Average steps: 5.661900043487549\n",
      "Eikonal loss: 0.03741079568862915\n",
      "Step 1960:\n",
      "Loss: 0.3314773142337799\n",
      "Average steps: 5.654099941253662\n",
      "Eikonal loss: 0.03775901719927788\n",
      "Step 1965:\n",
      "Loss: 0.3227284848690033\n",
      "Average steps: 5.614799976348877\n",
      "Eikonal loss: 0.036043617874383926\n",
      "Step 1970:\n",
      "Loss: 0.32573166489601135\n",
      "Average steps: 5.623499870300293\n",
      "Eikonal loss: 0.03676426783204079\n",
      "Step 1975:\n",
      "Loss: 0.32901138067245483\n",
      "Average steps: 5.630899906158447\n",
      "Eikonal loss: 0.03731860965490341\n",
      "Step 1980:\n",
      "Loss: 0.33894872665405273\n",
      "Average steps: 5.6417999267578125\n",
      "Eikonal loss: 0.039128873497247696\n",
      "Step 1985:\n",
      "Loss: 0.3272913098335266\n",
      "Average steps: 5.63700008392334\n",
      "Eikonal loss: 0.03681352362036705\n",
      "Step 1990:\n",
      "Loss: 0.3323025107383728\n",
      "Average steps: 5.587699890136719\n",
      "Eikonal loss: 0.03778627887368202\n",
      "Step 1995:\n",
      "Loss: 0.3344317674636841\n",
      "Average steps: 5.602399826049805\n",
      "Eikonal loss: 0.03814321011304855\n",
      "Step 2000:\n",
      "Loss: 0.3340173661708832\n",
      "Average steps: 5.654799938201904\n",
      "Eikonal loss: 0.03816893324255943\n",
      "Step 2005:\n",
      "Loss: 0.3282114565372467\n",
      "Average steps: 5.621799945831299\n",
      "Eikonal loss: 0.037043195217847824\n",
      "Step 2010:\n",
      "Loss: 0.32925254106521606\n",
      "Average steps: 5.603899955749512\n",
      "Eikonal loss: 0.037489037960767746\n",
      "Step 2015:\n",
      "Loss: 0.3256738781929016\n",
      "Average steps: 5.589999675750732\n",
      "Eikonal loss: 0.03677661716938019\n",
      "Step 2020:\n",
      "Loss: 0.32974159717559814\n",
      "Average steps: 5.6666998863220215\n",
      "Eikonal loss: 0.037526171654462814\n",
      "Step 2025:\n",
      "Loss: 0.32855430245399475\n",
      "Average steps: 5.597899913787842\n",
      "Eikonal loss: 0.03728692978620529\n",
      "Step 2030:\n",
      "Loss: 0.33307909965515137\n",
      "Average steps: 5.614500045776367\n",
      "Eikonal loss: 0.038270991295576096\n",
      "Step 2035:\n",
      "Loss: 0.3290177583694458\n",
      "Average steps: 5.628299713134766\n",
      "Eikonal loss: 0.037676308304071426\n",
      "Step 2040:\n",
      "Loss: 0.32786327600479126\n",
      "Average steps: 5.671999931335449\n",
      "Eikonal loss: 0.03747529536485672\n",
      "Step 2045:\n",
      "Loss: 0.3293992578983307\n",
      "Average steps: 5.689399719238281\n",
      "Eikonal loss: 0.037646736949682236\n",
      "Step 2050:\n",
      "Loss: 0.3225111961364746\n",
      "Average steps: 5.605499744415283\n",
      "Eikonal loss: 0.03638888895511627\n",
      "Step 2055:\n",
      "Loss: 0.32188189029693604\n",
      "Average steps: 5.617799758911133\n",
      "Eikonal loss: 0.03657945618033409\n",
      "Step 2060:\n",
      "Loss: 0.3212854266166687\n",
      "Average steps: 5.627699851989746\n",
      "Eikonal loss: 0.036474522203207016\n",
      "Step 2065:\n",
      "Loss: 0.3287714719772339\n",
      "Average steps: 5.678499698638916\n",
      "Eikonal loss: 0.0376073457300663\n",
      "Step 2070:\n",
      "Loss: 0.3241824209690094\n",
      "Average steps: 5.672599792480469\n",
      "Eikonal loss: 0.03661006689071655\n",
      "Step 2075:\n",
      "Loss: 0.3243972063064575\n",
      "Average steps: 5.677999973297119\n",
      "Eikonal loss: 0.03659500181674957\n",
      "Step 2080:\n",
      "Loss: 0.3174283504486084\n",
      "Average steps: 5.648200035095215\n",
      "Eikonal loss: 0.03542156144976616\n",
      "Step 2085:\n",
      "Loss: 0.32280415296554565\n",
      "Average steps: 5.604599952697754\n",
      "Eikonal loss: 0.036531466990709305\n",
      "Step 2090:\n",
      "Loss: 0.31863105297088623\n",
      "Average steps: 5.636199951171875\n",
      "Eikonal loss: 0.03589349240064621\n",
      "Step 2095:\n",
      "Loss: 0.316406786441803\n",
      "Average steps: 5.619899749755859\n",
      "Eikonal loss: 0.035502348095178604\n",
      "Step 2100:\n",
      "Loss: 0.32867804169654846\n",
      "Average steps: 5.669300079345703\n",
      "Eikonal loss: 0.03753446042537689\n",
      "Step 2105:\n",
      "Loss: 0.31918466091156006\n",
      "Average steps: 5.616600036621094\n",
      "Eikonal loss: 0.03603239357471466\n",
      "Step 2110:\n",
      "Loss: 0.3152720332145691\n",
      "Average steps: 5.585700035095215\n",
      "Eikonal loss: 0.035342685878276825\n",
      "Step 2115:\n",
      "Loss: 0.31314748525619507\n",
      "Average steps: 5.582299709320068\n",
      "Eikonal loss: 0.034987010061740875\n",
      "Step 2120:\n",
      "Loss: 0.3070800304412842\n",
      "Average steps: 5.603300094604492\n",
      "Eikonal loss: 0.03372681513428688\n",
      "Step 2125:\n",
      "Loss: 0.30277466773986816\n",
      "Average steps: 5.617799758911133\n",
      "Eikonal loss: 0.032775215804576874\n",
      "Step 2130:\n",
      "Loss: 0.30797815322875977\n",
      "Average steps: 5.592199802398682\n",
      "Eikonal loss: 0.03375871479511261\n",
      "Step 2135:\n",
      "Loss: 0.3191218972206116\n",
      "Average steps: 5.5954999923706055\n",
      "Eikonal loss: 0.03607693314552307\n",
      "Step 2140:\n",
      "Loss: 0.3087335228919983\n",
      "Average steps: 5.596799850463867\n",
      "Eikonal loss: 0.03410959243774414\n",
      "Step 2145:\n",
      "Loss: 0.30626723170280457\n",
      "Average steps: 5.651000022888184\n",
      "Eikonal loss: 0.033402007073163986\n",
      "Step 2150:\n",
      "Loss: 0.3160340189933777\n",
      "Average steps: 5.6514997482299805\n",
      "Eikonal loss: 0.03507588803768158\n",
      "Step 2155:\n",
      "Loss: 0.31192275881767273\n",
      "Average steps: 5.621099948883057\n",
      "Eikonal loss: 0.03445569798350334\n",
      "Step 2160:\n",
      "Loss: 0.311718225479126\n",
      "Average steps: 5.6479997634887695\n",
      "Eikonal loss: 0.0345025472342968\n",
      "Step 2165:\n",
      "Loss: 0.30003100633621216\n",
      "Average steps: 5.577600002288818\n",
      "Eikonal loss: 0.03222490847110748\n",
      "Step 2170:\n",
      "Loss: 0.3003348410129547\n",
      "Average steps: 5.582200050354004\n",
      "Eikonal loss: 0.03230438381433487\n",
      "Step 2175:\n",
      "Loss: 0.303866446018219\n",
      "Average steps: 5.621099948883057\n",
      "Eikonal loss: 0.03292212635278702\n",
      "Step 2180:\n",
      "Loss: 0.32807859778404236\n",
      "Average steps: 5.7093000411987305\n",
      "Eikonal loss: 0.0371924564242363\n",
      "Step 2185:\n",
      "Loss: 0.31387192010879517\n",
      "Average steps: 5.624899864196777\n",
      "Eikonal loss: 0.03460995852947235\n",
      "Step 2190:\n",
      "Loss: 0.3098594546318054\n",
      "Average steps: 5.592799663543701\n",
      "Eikonal loss: 0.03398723900318146\n",
      "Step 2195:\n",
      "Loss: 0.31644028425216675\n",
      "Average steps: 5.605000019073486\n",
      "Eikonal loss: 0.03552921861410141\n",
      "Step 2200:\n",
      "Loss: 0.3166956901550293\n",
      "Average steps: 5.682199954986572\n",
      "Eikonal loss: 0.0352298878133297\n",
      "Step 2205:\n",
      "Loss: 0.32433021068573\n",
      "Average steps: 5.683099746704102\n",
      "Eikonal loss: 0.03639115020632744\n",
      "Step 2210:\n",
      "Loss: 0.3186110258102417\n",
      "Average steps: 5.682799816131592\n",
      "Eikonal loss: 0.03522663936018944\n",
      "Step 2215:\n",
      "Loss: 0.3144005835056305\n",
      "Average steps: 5.690000057220459\n",
      "Eikonal loss: 0.034554045647382736\n",
      "Step 2220:\n",
      "Loss: 0.3163081407546997\n",
      "Average steps: 5.624199867248535\n",
      "Eikonal loss: 0.035535119473934174\n",
      "Step 2225:\n",
      "Loss: 0.3167182207107544\n",
      "Average steps: 5.642600059509277\n",
      "Eikonal loss: 0.035826463252305984\n",
      "Step 2230:\n",
      "Loss: 0.3170562982559204\n",
      "Average steps: 5.659599781036377\n",
      "Eikonal loss: 0.035752516239881516\n",
      "Step 2235:\n",
      "Loss: 0.3115881383419037\n",
      "Average steps: 5.548099994659424\n",
      "Eikonal loss: 0.035045456141233444\n",
      "Step 2240:\n",
      "Loss: 0.31152820587158203\n",
      "Average steps: 5.52869987487793\n",
      "Eikonal loss: 0.03525872528553009\n",
      "Step 2245:\n",
      "Loss: 0.30499783158302307\n",
      "Average steps: 5.574100017547607\n",
      "Eikonal loss: 0.03397250175476074\n",
      "Step 2250:\n",
      "Loss: 0.30474621057510376\n",
      "Average steps: 5.582900047302246\n",
      "Eikonal loss: 0.033678896725177765\n",
      "Step 2255:\n",
      "Loss: 0.30493712425231934\n",
      "Average steps: 5.625499725341797\n",
      "Eikonal loss: 0.033087242394685745\n",
      "Step 2260:\n",
      "Loss: 0.3149430751800537\n",
      "Average steps: 5.67009973526001\n",
      "Eikonal loss: 0.034848179668188095\n",
      "Step 2265:\n",
      "Loss: 0.297251433134079\n",
      "Average steps: 5.63129997253418\n",
      "Eikonal loss: 0.03173696994781494\n",
      "Step 2270:\n",
      "Loss: 0.3072003722190857\n",
      "Average steps: 5.6072998046875\n",
      "Eikonal loss: 0.03382392227649689\n",
      "Step 2275:\n",
      "Loss: 0.2954511046409607\n",
      "Average steps: 5.6153998374938965\n",
      "Eikonal loss: 0.031419143080711365\n",
      "Step 2280:\n",
      "Loss: 0.318663626909256\n",
      "Average steps: 5.710899829864502\n",
      "Eikonal loss: 0.03551029786467552\n",
      "Step 2285:\n",
      "Loss: 0.3138532042503357\n",
      "Average steps: 5.65939998626709\n",
      "Eikonal loss: 0.034983325749635696\n",
      "Step 2290:\n",
      "Loss: 0.3006434440612793\n",
      "Average steps: 5.638000011444092\n",
      "Eikonal loss: 0.03230743482708931\n",
      "Step 2295:\n",
      "Loss: 0.3053063750267029\n",
      "Average steps: 5.652299880981445\n",
      "Eikonal loss: 0.033519674092531204\n",
      "Step 2300:\n",
      "Loss: 0.30182942748069763\n",
      "Average steps: 5.65339994430542\n",
      "Eikonal loss: 0.03288465738296509\n",
      "Step 2305:\n",
      "Loss: 0.3117704391479492\n",
      "Average steps: 5.705399990081787\n",
      "Eikonal loss: 0.0344674326479435\n",
      "Step 2310:\n",
      "Loss: 0.30016839504241943\n",
      "Average steps: 5.639800071716309\n",
      "Eikonal loss: 0.03249159827828407\n",
      "Step 2315:\n",
      "Loss: 0.3048219084739685\n",
      "Average steps: 5.583699703216553\n",
      "Eikonal loss: 0.033825282007455826\n",
      "Step 2320:\n",
      "Loss: 0.30015891790390015\n",
      "Average steps: 5.632999897003174\n",
      "Eikonal loss: 0.03275466710329056\n",
      "Step 2325:\n",
      "Loss: 0.2947666049003601\n",
      "Average steps: 5.6240997314453125\n",
      "Eikonal loss: 0.03177293390035629\n",
      "Step 2330:\n",
      "Loss: 0.3036747872829437\n",
      "Average steps: 5.5366997718811035\n",
      "Eikonal loss: 0.033919673413038254\n",
      "Step 2335:\n",
      "Loss: 0.30659401416778564\n",
      "Average steps: 5.615099906921387\n",
      "Eikonal loss: 0.03406389430165291\n",
      "Step 2340:\n",
      "Loss: 0.32460686564445496\n",
      "Average steps: 5.707299709320068\n",
      "Eikonal loss: 0.036605626344680786\n",
      "Step 2345:\n",
      "Loss: 0.3138737082481384\n",
      "Average steps: 5.7216997146606445\n",
      "Eikonal loss: 0.034432779997587204\n",
      "Step 2350:\n",
      "Loss: 0.3006448745727539\n",
      "Average steps: 5.68179988861084\n",
      "Eikonal loss: 0.03245655447244644\n",
      "Step 2355:\n",
      "Loss: 0.3007262647151947\n",
      "Average steps: 5.637599945068359\n",
      "Eikonal loss: 0.03287360444664955\n",
      "Step 2360:\n",
      "Loss: 0.29713350534439087\n",
      "Average steps: 5.6026997566223145\n",
      "Eikonal loss: 0.032374557107686996\n",
      "Step 2365:\n",
      "Loss: 0.29573768377304077\n",
      "Average steps: 5.627399921417236\n",
      "Eikonal loss: 0.03197691589593887\n",
      "Step 2370:\n",
      "Loss: 0.3082682490348816\n",
      "Average steps: 5.690299987792969\n",
      "Eikonal loss: 0.033614013344049454\n",
      "Step 2375:\n",
      "Loss: 0.31170520186424255\n",
      "Average steps: 5.691499710083008\n",
      "Eikonal loss: 0.03405255079269409\n",
      "Step 2380:\n",
      "Loss: 0.3076120913028717\n",
      "Average steps: 5.688399791717529\n",
      "Eikonal loss: 0.03341018781065941\n",
      "Step 2385:\n",
      "Loss: 0.3057573437690735\n",
      "Average steps: 5.715399742126465\n",
      "Eikonal loss: 0.03271115943789482\n",
      "Step 2390:\n",
      "Loss: 0.3132026791572571\n",
      "Average steps: 5.741699695587158\n",
      "Eikonal loss: 0.033869385719299316\n",
      "Step 2395:\n",
      "Loss: 0.3220546841621399\n",
      "Average steps: 5.791399955749512\n",
      "Eikonal loss: 0.03482512757182121\n",
      "Step 2400:\n",
      "Loss: 0.3115265965461731\n",
      "Average steps: 5.765500068664551\n",
      "Eikonal loss: 0.033529091626405716\n",
      "Step 2405:\n",
      "Loss: 0.306939035654068\n",
      "Average steps: 5.719299793243408\n",
      "Eikonal loss: 0.03354547917842865\n",
      "Step 2410:\n",
      "Loss: 0.29881855845451355\n",
      "Average steps: 5.660899639129639\n",
      "Eikonal loss: 0.032478414475917816\n",
      "Step 2415:\n",
      "Loss: 0.29654064774513245\n",
      "Average steps: 5.701099872589111\n",
      "Eikonal loss: 0.03162546455860138\n",
      "Step 2420:\n",
      "Loss: 0.3005152940750122\n",
      "Average steps: 5.713200092315674\n",
      "Eikonal loss: 0.0321183018386364\n",
      "Step 2425:\n",
      "Loss: 0.3165086507797241\n",
      "Average steps: 5.783199787139893\n",
      "Eikonal loss: 0.033852469176054\n",
      "Step 2430:\n",
      "Loss: 0.31266576051712036\n",
      "Average steps: 5.771100044250488\n",
      "Eikonal loss: 0.033013876527547836\n",
      "Step 2435:\n",
      "Loss: 0.2990530729293823\n",
      "Average steps: 5.638700008392334\n",
      "Eikonal loss: 0.03222471475601196\n",
      "Step 2440:\n",
      "Loss: 0.2948235273361206\n",
      "Average steps: 5.586400032043457\n",
      "Eikonal loss: 0.031832169741392136\n",
      "Step 2445:\n",
      "Loss: 0.29514721035957336\n",
      "Average steps: 5.660599708557129\n",
      "Eikonal loss: 0.031177015975117683\n",
      "Step 2450:\n",
      "Loss: 0.30542662739753723\n",
      "Average steps: 5.737799644470215\n",
      "Eikonal loss: 0.032136302441358566\n",
      "Step 2455:\n",
      "Loss: 0.30461329221725464\n",
      "Average steps: 5.727499961853027\n",
      "Eikonal loss: 0.032068680971860886\n",
      "Step 2460:\n",
      "Loss: 0.307147353887558\n",
      "Average steps: 5.732800006866455\n",
      "Eikonal loss: 0.03268576040863991\n",
      "Step 2465:\n",
      "Loss: 0.3171587586402893\n",
      "Average steps: 5.778500080108643\n",
      "Eikonal loss: 0.03414255380630493\n",
      "Step 2470:\n",
      "Loss: 0.3051425814628601\n",
      "Average steps: 5.769799709320068\n",
      "Eikonal loss: 0.031962182372808456\n",
      "Step 2475:\n",
      "Loss: 0.29637381434440613\n",
      "Average steps: 5.591300010681152\n",
      "Eikonal loss: 0.032488834112882614\n",
      "Step 2480:\n",
      "Loss: 0.29758507013320923\n",
      "Average steps: 5.608599662780762\n",
      "Eikonal loss: 0.03297651559114456\n",
      "Step 2485:\n",
      "Loss: 0.30010080337524414\n",
      "Average steps: 5.749300003051758\n",
      "Eikonal loss: 0.032341890037059784\n",
      "Step 2490:\n",
      "Loss: 0.310033917427063\n",
      "Average steps: 5.779599666595459\n",
      "Eikonal loss: 0.033146969974040985\n",
      "Step 2495:\n",
      "Loss: 0.3140941262245178\n",
      "Average steps: 5.792799949645996\n",
      "Eikonal loss: 0.0334000401198864\n",
      "Training done !\n",
      "Video saved successfully as depth_map_training.mp4\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "L = 5\n",
    "lr = 1e-4\n",
    "num_iters = 2500\n",
    "\n",
    "# Misc parameters\n",
    "display_every = 5\n",
    "\n",
    "# Model\n",
    "model = VeryTinyNeDFModel(L=L)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Seed RNG\n",
    "seed = 9458\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Setup training\n",
    "target_depth = testimg.float().to(device)\n",
    "target_tform_cam2world = testpose\n",
    "ray_origins, ray_directions = get_ray_bundle(height, width, focal_length, target_tform_cam2world)\n",
    "\n",
    "# Save depth maps for video\n",
    "depth_maps = []\n",
    "\n",
    "eikonal_penalty_weight = 5  # Weight for eikonal loss\n",
    "\n",
    "step_penalty_weight = 0  # Adjust weight to balance depth loss and step penalty\n",
    "step_penalty_alpha = 0.05  # Exponential growth factor for step penalty\n",
    "\n",
    "for i in range(num_iters):\n",
    "    # Use NeDF sphere tracing to compute the depth map and step count\n",
    "    depth_predicted, steps, query_points, query_results = render_depth_sphere_tracing(model, ray_origins, ray_directions, target_depth, near_thresh)\n",
    "\n",
    "    gradients = compute_gradients(model, query_points)\n",
    "    eikonal_loss = compute_eikonal_loss(gradients)\n",
    "\n",
    "    # Compute the mean-squared error loss between predicted and true depth map\n",
    "    depth_loss = torch.nn.functional.mse_loss(depth_predicted, target_depth)\n",
    "\n",
    "    # Exponential penalty for step count to prioritize reducing steps (not working)\n",
    "    # step_penalty = torch.mean(torch.exp(step_penalty_alpha * steps))\n",
    "    \n",
    "    # Use eikonal loss instead, very effective\n",
    "    eikonal_penalty = eikonal_penalty_weight * eikonal_loss\n",
    "\n",
    "    # Combine the depth loss and step penalty\n",
    "    loss = depth_loss + eikonal_penalty\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Display progress\n",
    "    if i % display_every == 0:\n",
    "        print(f\"Step {i}:\")\n",
    "        print(f\"Loss: {loss}\")\n",
    "        print(f\"Average steps: {steps.mean().item()}\")\n",
    "        print(f\"Eikonal loss: {eikonal_loss}\")\n",
    "        \n",
    "        # Convert depth map to image (normalize for visualization)\n",
    "        depth_img = depth_predicted.detach().cpu().numpy()\n",
    "        depth_img_normalized = (depth_img - depth_img.min()) / (depth_img.max() - depth_img.min()) * 255.0\n",
    "        depth_maps.append(depth_img_normalized.astype(np.uint8))\n",
    "\n",
    "        # Display the current depth map\n",
    "        # plt.imshow(depth_img_normalized, cmap='inferno')\n",
    "        # plt.title(f\"Iteration {i} - Depth Map\")\n",
    "        # plt.colorbar()\n",
    "        # plt.show()\n",
    "\n",
    "print('Training done !')\n",
    "\n",
    "# Save the video using OpenCV\n",
    "output_file = \"depth_map_training.mp4\"\n",
    "height, width = depth_maps[0].shape\n",
    "video_writer = cv2.VideoWriter(\n",
    "    output_file, cv2.VideoWriter_fourcc(*'mp4v'), 5, (width, height), isColor=True\n",
    ")\n",
    "\n",
    "# Write each depth map to the video\n",
    "for depth_map in depth_maps:\n",
    "    # Ensure grayscale map is expanded to (H, W, 3) to represent RGB channels\n",
    "    frame = np.stack([depth_map]*3, axis=-1)  # Duplicate grayscale into R, G, B\n",
    "    video_writer.write(frame)\n",
    "\n",
    "video_writer.release()\n",
    "print(f\"Video saved successfully as {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
