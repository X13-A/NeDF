{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZh3aklt3ZET"
   },
   "source": [
    "# NeDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ptTYjWao3VsM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "# Import all the good stuff\n",
    "import os\n",
    "os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"]=\"1\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from settings import *\n",
    "from utils import *\n",
    "from dataset import *\n",
    "from model import *\n",
    "from dataset import *\n",
    "from estimate_distance import *\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ierxw2dsL3pU"
   },
   "source": [
    "### GPU vs CPU ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading depth maps and generating rays...\n",
      "success\n",
      "success\n",
      "success\n",
      "Successfully generated 3 entries!\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Ray directions as RGB\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m axes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mray_rgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m())\n\u001b[0;32m     23\u001b[0m axes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRay Directions (Index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m axes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8EAAAH/CAYAAAB3p9/0AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAANe5JREFUeJzt3Xt4ldWdL/AftyQoJoBIuBhBsK03CgrCcPHYC8oZUYf2tDLqCKIWbzgqT62glXhpjVOrQ6sIFW1xrBTUUWurgxeUaWekoig92hGqVcR2DIpo4jWB5D1/eMx0m3DPlfX5PM9+fNba6333by/AnW/WXu/bLsuyLAAAACAB7Vu6AAAAAGguQjAAAADJEIIBAABIhhAMAABAMoRgAAAAkiEEAwAAkAwhGAAAgGQIwQAAACRDCAYAACAZQjAAAADJEIIBoJX5zW9+E8cff3z06dMn2rVrF/fff/82j1m2bFkcfvjhkZ+fHwcccEAsWLCgyesEgLZICAaAVuaDDz6IwYMHx5w5c7Zr/Kuvvhrjx4+PL3/5y7Fq1aq48MIL48wzz4yHH364iSsFgLanXZZlWUsXAQA0rF27dnHffffFhAkTtjjmkksuiQcffDBeeOGFur6///u/j3fffTeWLFnSDFUCQNvRsaULAAB2zfLly2Ps2LE5fePGjYsLL7xwi8dUVVVFVVVVXbu2tjY2btwYe++9d7Rr166pSgWA7ZZlWbz33nvRp0+faN++8b7ELAQDQBtXXl4excXFOX3FxcVRWVkZH330UXTu3LneMWVlZXHllVc2V4kAsNNef/312HfffRvtfEIwACRo5syZMX369Lp2RUVF7LfffvH6669HYWFhC1YGAJ+orKyMkpKS2GuvvRr1vEIwALRxvXr1ivXr1+f0rV+/PgoLCxtcBY6IyM/Pj/z8/Hr9hYWFQjAArUpjb9NxdWgAaONGjhwZS5cuzel79NFHY+TIkS1UEQC0XkIwALQy77//fqxatSpWrVoVEZ/cAmnVqlWxbt26iPjkq8yTJk2qG3/22WfHK6+8Et/5zndi9erVcfPNN8ddd90VF110UUuUDwCtmhAMAK3MM888E4cddlgcdthhERExffr0OOyww2LWrFkREfHGG2/UBeKIiP333z8efPDBePTRR2Pw4MFx/fXXx6233hrjxo1rkfoBoDVzn2AAICorK6OoqCgqKirsCQagVWiqzyYrwQAAACRDCAYAACAZQjAAAADJEIIBAABIhhAMAABAMoRgAAAAkiEEAwAAkAwhGAAAgGQIwQAAACRDCAYAACAZQjAAAADJEIIBAABIhhAMAABAMoRgAAAAkiEEAwAAkAwhGAAAgGQIwQAAACRDCAYAACAZQjAAAADJEIIBAABIhhAMAABAMoRgAAAAkiEEAwAAkAwhGAAAgGQIwQAAACRDCAYAACAZQjAAAADJEIIBAABIhhAMAABAMoRgAAAAkiEEAwAAkAwhGAAAgGQIwQAAACRDCAYAACAZQjAAAADJEIIBAABIhhAMAABAMoRgAAAAkiEEAwAAkAwhGAAAgGQIwQAAACRDCAYAACAZQjAAAADJEIIBAABIhhAMAABAMoRgAAAAkiEEAwAAkAwhGAAAgGQIwQAAACRDCAYAACAZQjAAAADJEIIBAABIhhAMAABAMoRgAAAAkiEEAwAAkAwhGAAAgGQIwQAAACRDCAYAACAZQjAAAADJEIIBAABIhhAMAABAMoRgAAAAkiEEAwAAkAwhGAAAgGQIwQAAACRDCAYAACAZQjAAAADJEIIBAABIhhAMAABAMoRgAAAAkiEEAwAAkAwhGAAAgGQIwQAAACRDCAYAACAZQjAAAADJEIIBAABIhhAMAABAMoRgAAAAkiEEAwAAkAwhGAAAgGQIwQAAACRDCAaAVmjOnDnRv3//KCgoiBEjRsSKFSu2On727NnxhS98ITp37hwlJSVx0UUXxccff9xM1QJA2yEEA0Ars3jx4pg+fXqUlpbGs88+G4MHD45x48bFm2++2eD4hQsXxowZM6K0tDRefPHFuO2222Lx4sVx6aWXNnPlAND6CcEA0MrccMMN8a1vfSumTJkSBx98cMybNy/22GOP+OlPf9rg+CeffDJGjx4dJ598cvTv3z+OOeaYOOmkk7a5egwAKRKCafMWLFgQ7dq1i2eeeaalS2lSd911V3Tv3j3ef//9Jn2d0047Lfr379+kr7Er5s2bF/vtt19UVVW1dCnQJKqrq2PlypUxduzYur727dvH2LFjY/ny5Q0eM2rUqFi5cmVd6H3llVfioYceimOPPbZZagaAtkQIZps+DZmfPgoKCqJPnz4xbty4+PGPfxzvvfdes9Rx8803x4IFCxr9vFdccUW0a9cu2rdvH6+//nq95ysrK6Nz587Rrl27mDZtWqO//vaoqamJ0tLSOP/886NLly51/f3794/jjjuuRWpqCk8++WSMGTMm9thjj+jVq1f84z/+Y73Qf9ppp0V1dXX85Cc/aaEqoWlt2LAhampqori4OKe/uLg4ysvLGzzm5JNPjquuuirGjBkTnTp1ioEDB8aXvvSlrX4duqqqKiorK3MeAJACIZjtdtVVV8Udd9wRc+fOjfPPPz8iIi688MIYNGhQ/N//+3+b/PWbKgR/Kj8/P37xi1/U67/33nub7DW3169+9atYs2ZNTJ06taVLaTKrVq2Kr371q/Hhhx/GDTfcEGeeeWbccsst8c1vfjNnXEFBQUyePDluuOGGyLKshaqF1mXZsmVxzTXXxM033xzPPvts3HvvvfHggw/G1VdfvcVjysrKoqioqO5RUlLSjBUDQMsRgtluf/u3fxv/8A//EFOmTImZM2fGww8/HI899li8+eabccIJJ8RHH33U0iXukmOPPbbBELxw4cIYP358C1T0P372s5/F6NGjo2/fvi1aR1O69NJLo1u3brFs2bI4++yz43vf+17cdNNNsWTJknjkkUdyxp544onx2muvxRNPPNFC1ULT6dGjR3To0CHWr1+f079+/fro1atXg8dcfvnlceqpp8aZZ54ZgwYNiq997WtxzTXXRFlZWdTW1jZ4zMyZM6OioqLu0dA3YQBgdyQEs0u+8pWvxOWXXx6vvfZa/PznP895bvXq1fGNb3wjunfvHgUFBTFs2LB44IEHcsZ8+lXr3/zmN3HWWWfF3nvvHYWFhTFp0qR455136sb1798//vCHP8S///u/130t+0tf+lLOuaqqqmL69Omxzz77xJ577hlf+9rX4q233tru93LyySfHqlWrYvXq1XV95eXl8fjjj8fJJ59cb3x1dXXMmjUrhg4dGkVFRbHnnnvGkUceWS+YrV27Ntq1axc//OEP45//+Z+jX79+0blz5zjqqKPihRde2GZdH3/8cSxZsiRnf+CW/PVr3XLLLTFw4MDIz8+PI444Ip5++ul64++///449NBDo6CgIA499NC47777GjxvbW1tzJ49Ow455JAoKCiI4uLiOOuss3L+jEpLS6N9+/axdOnSnGOnTp0aeXl58fvf/36LdVdWVsajjz4a//AP/xCFhYV1/ZMmTYouXbrEXXfdlTN+6NCh0b179/jlL3+5zTmBtiYvLy+GDh2a82+ptrY2li5dGiNHjmzwmA8//DDat8/9SO/QoUNExBa/MZGfnx+FhYU5DwBIgRDMLjv11FMjInJW6/7whz/E3/zN38SLL74YM2bMiOuvvz723HPPmDBhQoNBa9q0afHiiy/GFVdcEZMmTYo777wzJkyYUPfD2+zZs2PfffeNAw88MO64446444474rLLLss5x/nnnx+///3vo7S0NM4555z41a9+tUN7eP/X//pfse+++8bChQvr+hYvXhxdunRpcCW4srIybr311vjSl74U//RP/xRXXHFFvPXWWzFu3LhYtWpVvfH/8i//Ej/+8Y/jvPPOi5kzZ8YLL7wQX/nKV+qt9nzWypUro7q6Og4//PDtfi8LFy6M6667Ls4666z43ve+F2vXro2vf/3rsWnTproxjzzySPyf//N/ol27dlFWVhYTJkyIKVOmNHiBsbPOOisuvvjiGD16dPzoRz+KKVOmxJ133hnjxo2rO+d3v/vdGDJkSJxxxhl1+8QffvjhmD9/fsyaNSsGDx68xXqff/752Lx5cwwbNiynPy8vL4YMGRLPPfdcvWMOP/zw+M///M/tnhNoS6ZPnx7z58+P22+/PV588cU455xz4oMPPogpU6ZExCe/IJo5c2bd+OOPPz7mzp0bixYtildffTUeffTRuPzyy+P444+vC8MAwP+XwTb87Gc/yyIie/rpp7c4pqioKDvssMPq2l/96lezQYMGZR9//HFdX21tbTZq1Kjsc5/7XL1zDx06NKuurq7r/8EPfpBFRPbLX/6yru+QQw7JjjrqqC3WN3bs2Ky2trau/6KLLso6dOiQvfvuu1t9f6WlpVlEZG+99Vb27W9/OzvggAPqnjviiCOyKVOmZFmWZRGRnXfeeXXPbd68Oauqqso51zvvvJMVFxdnp59+el3fq6++mkVE1rlz5+zPf/5zXf9TTz2VRUR20UUXbbW+W2+9NYuI7Pnnn6/3XL9+/bLx48fXe629994727hxY13/L3/5yywisl/96ld1fUOGDMl69+6dMz+PPPJIFhFZv3796vp++9vfZhGR3XnnnTmvvWTJknr9zz//fJaXl5edeeaZ2TvvvJP17ds3GzZsWLZp06atvse77747i4jsN7/5Tb3nvvnNb2a9evWq1z916tSsc+fOWz0vtGU33nhjtt9++2V5eXnZ8OHDs9/97nd1zx111FHZ5MmT69qbNm3KrrjiimzgwIFZQUFBVlJSkp177rnZO++8s92vV1FRkUVEVlFR0YjvAgB2XlN9NlkJplF06dKlbvVv48aN8fjjj8eJJ54Y7733XmzYsCE2bNgQb7/9dowbNy5eeuml+Mtf/pJz/NSpU6NTp0517XPOOSc6duwYDz300HbXMHXq1GjXrl1d+8gjj4yampp47bXXtvscJ598crz88svx9NNP1/23oa9CR3zyVcO8vLyI+OSrihs3bqxbzXz22WfrjZ8wYULOnt7hw4fHiBEjtvke33777YiI6Nat23a/j4kTJ+aMP/LIIyPik9umRES88cYbsWrVqpg8eXIUFRXVjTv66KPj4IMPzjnX3XffHUVFRXH00UfX/Vlu2LAhhg4dGl26dMn5+vehhx4aV155Zdx6660xbty42LBhQ9x+++3RsWPHrdb76X7y/Pz8es8VFBQ0uN+8W7du8dFHH8WHH364remANmnatGnx2muvRVVVVTz11FMxYsSIuueWLVuWc6HAjh07Rmlpabz88svx0Ucfxbp162LOnDnRtWvX5i8cAFq5rf9kCtvp/fffj549e0ZExMsvvxxZlsXll18el19+eYPj33zzzZxA+LnPfS7n+S5dukTv3r1j7dq1213Dfvvtl9P+NAT+9b7VbTnssMPiwAMPjIULF0bXrl2jV69e8ZWvfGWL42+//fa4/vrrY/Xq1TlfNd5///3rjf3se4yI+PznP19vv+uWZDtwJeRtzcWnvxhoqKYvfOELOSH+pZdeioqKiro/38968803c9oXX3xxLFq0KFasWBHXXHNNvVDdkM6dO0dENHjv348//rju+b/26Xz89S8+AABgW4Rgdtmf//znqKioiAMOOCAiou5KpN/+9rdj3LhxDR7z6djGtKV9bzsSHiM+WQ2eO3du7LXXXjFx4sR6F5v51M9//vM47bTTYsKECXHxxRdHz549o0OHDlFWVhZ/+tOfdrj+Ldl7770j4pMAu++++27XMY01FxGf/Hn27Nkz7rzzzgaf32effXLar7zySrz00ksR8cle3+3Ru3fviPhkhfqz3njjjejTp0+9/nfeeSf22GOPBgMyAABsiRDMLrvjjjsiIuoC74ABAyIiolOnTtt1ReOIT1Ybv/zlL9e133///XjjjTfi2GOPretrrhW/k08+OWbNmhVvvPFG3XtryD333BMDBgyIe++9N6e20tLSBsd/Ggz/2h//+Mfo37//Vus58MADIyLi1VdfjUGDBm3HO9i2fv36bbGmNWvW5LQHDhwYjz32WIwePXqbgbO2tjZOO+20KCwsjAsvvDCuueaa+MY3vhFf//rXt3rcoYceGh07doxnnnkmTjzxxLr+6urqWLVqVU7fp1599dU46KCDtnpeAAD4LHuC2SWPP/54XH311bH//vvHKaecEhERPXv2jC996Uvxk5/8pMGVvYZuW3TLLbfkfJ147ty5sXnz5vjbv/3bur4999wz3n333cZ/E58xcODAmD17dpSVlcXw4cO3OK6h24889dRTsXz58gbH33///Tl7oVesWBFPPfVUzntsyNChQyMvL6/BqzbvrN69e8eQIUPi9ttvj4qKirr+Rx99NP7rv/4rZ+yJJ54YNTU1cfXVV9c7z+bNm3P+TG644YZ48skn45Zbbomrr746Ro0aFeecc05s2LBhq/UUFRXF2LFj4+c//3nd3vKIT37B8v7778c3v/nNesc8++yzMWrUqO19ywAAEBFWgtkB//Zv/xarV6+OzZs3x/r16+Pxxx+PRx99NPr16xcPPPBAFBQU1I2dM2dOjBkzJgYNGhTf+ta3YsCAAbF+/fpYvnx5/PnPf653z9jq6ur46le/GieeeGKsWbMmbr755hgzZkyccMIJdWOGDh0ac+fOje9973txwAEHRM+ePbe6X3dXXHDBBdscc9xxx8W9994bX/va12L8+PHx6quvxrx58+Lggw+O999/v974Aw44IMaMGRPnnHNOVFVVxezZs2PvvfeO73znO1t9nYKCgjjmmGPisccei6uuumqn39NnlZWVxfjx42PMmDFx+umnx8aNG+PGG2+MQw45JKf+o446Ks4666woKyuLVatWxTHHHBOdOnWKl156Ke6+++740Y9+FN/4xjfixRdfjMsvvzxOO+20OP744yPik/tADxkyJM4999xt7n3+/ve/H6NGjYqjjjoqpk6dGn/+85/j+uuvj2OOOSb+9//+3zljV65cGRs3boy/+7u/a7T5AAAgEY16rWl2S5/egujTR15eXtarV6/s6KOPzn70ox9llZWVDR73pz/9KZs0aVLWq1evrFOnTlnfvn2z4447Lrvnnnvqnfvf//3fs6lTp2bdunXLunTpkp1yyinZ22+/nXO+8vLybPz48dlee+2VRUTd7ZK2dAunJ554IouI7Iknntjq+/vrWyRtTXzmFkm1tbXZNddck/Xr1y/Lz8/PDjvssOzXv/51Nnny5JxbDH1626Lrrrsuu/7667OSkpIsPz8/O/LII7Pf//73W33NT917771Zu3btsnXr1uX0b+kWSdddd12D9ZeWlub0/eu//mt20EEHZfn5+dnBBx+c3XvvvfXq/9Qtt9ySDR06NOvcuXO21157ZYMGDcq+853vZP/93/+dbd68OTviiCOyfffdt94tqX70ox9lEZEtXrx4m+/zt7/9bTZq1KisoKAg22effbLzzjuvwb9fl1xySbbffvvl3BIL2DVukQRAa9NUn03tsmwnrpQDjWTBggUxZcqUePrpp2PYsGEtXU6TWLt2bey///5x3XXXxbe//e2dOkdNTU0cfPDBceKJJzb4teSUVFVVRf/+/WPGjBnbtWIPbJ/KysooKiqKioqKKCwsbOlyAKDJPpvsCYY2oEOHDnHVVVfFnDlzGvyqdUp+9rOfRadOneLss89u6VIAAGiDhGBoIyZOnBgbN26MLl26tHQpLerss8+OdevWRX5+fkuXAgBAGyQEAwAAkAx7ggEAe4IBaHXsCQYAAIBdJAQDAACQDCEYAACAZAjBAAAAJEMIBgAAIBlCMAAAAMkQggEAAEiGEAwAAEAyhGAAAACSIQQDAACQDCEYAACAZAjBAAAAJEMIBgAAIBlCMAAAAMkQggEAAEiGEAwAAEAyhGAAAACSIQQDAACQDCEYAACAZAjBAAAAJEMIBgAAIBlCMAAAAMkQggEAAEiGEAwAAEAyhGAAAACSIQQDAACQDCEYAACAZAjBAAAAJEMIBgAAIBlCMAAAAMkQggEAAEiGEAwAAEAyhGAAAACSIQQDAACQDCEYAACAZAjBAAAAJEMIBgAAIBlCMAAAAMkQggEAAEiGEAwAAEAyhGAAAACSIQQDAACQDCEYAACAZAjBAAAAJEMIBgAAIBlCMAAAAMkQggEAAEiGEAwAAEAyhGAAAACSIQQDAACQDCEYAACAZAjBAAAAJEMIBgAAIBlCMAAAAMkQggEAAEiGEAwAAEAyhGAAAACSIQQDAACQDCEYAACAZAjBAAAAJEMIBgAAIBlCMAAAAMno2NIFANu2cuXKnPbtt99eb0zHjrn/nM8///yc9v7779/4hQEAQBtjJRgAAIBkCMEAAAAkQwgGAAAgGfYEQzMrLy+v13fnnXfmtBcsWJDTfuGFF3b4dX784x/ntE844YSc9vTp0+sdM2bMmB1+HQAAaEusBAMAAJAMIRgAAIBkCMEAAAAkQwgGAAAgGe2yLMtaughoy6qrq3Pav/rVr3Lan73I1ZIlS+qdY/PmzY1e184YNmxYTvuiiy7KaX/zm9/MaXfq1KnJa4JUzZkzJ6677rooLy+PwYMHx4033hjDhw/f4vh33303Lrvssrj33ntj48aN0a9fv5g9e3Yce+yx2/V6lZWVUVRUFBUVFVFYWNhYbwMAdlpTfTa5OjQAtDKLFy+O6dOnx7x582LEiBExe/bsGDduXKxZsyZ69uxZb3x1dXUcffTR0bNnz7jnnnuib9++8dprr0XXrl2bv3gAaOWEYABoZW644Yb41re+FVOmTImIiHnz5sWDDz4YP/3pT2PGjBn1xv/0pz+NjRs3xpNPPln3DY3+/fs3Z8kA0GbYEwwArUh1dXWsXLkyxo4dW9fXvn37GDt2bCxfvrzBYx544IEYOXJknHfeeVFcXByHHnpoXHPNNVFTU7PF16mqqorKysqcBwCkwEowbMUzzzxTr++ze3wXLVqU03777bebsqQm9dn3e8opp+S0L7nkkpz2tGnTctpTp06td85u3bo1UnWQhg0bNkRNTU0UFxfn9BcXF8fq1asbPOaVV16Jxx9/PE455ZR46KGH4uWXX45zzz03Nm3aFKWlpQ0eU1ZWFldeeWWj1w8ArZ2VYABo42pra6Nnz55xyy23xNChQ2PixIlx2WWXxbx587Z4zMyZM6OioqLu8frrrzdjxQDQcqwEA0Ar0qNHj+jQoUOsX78+p3/9+vXRq1evBo/p3bt3dOrUKTp06FDXd9BBB0V5eXlUV1dHXl5evWPy8/MjPz+/cYsHgDbASjAAtCJ5eXkxdOjQWLp0aV1fbW1tLF26NEaOHNngMaNHj46XX345amtr6/r++Mc/Ru/evRsMwACQMvcJJilvvPFGTvvnP/95Tvv222/Paf/hD39o8pp2J3vuuWe9vkmTJuW0L7jggpz2F77whSatCdqixYsXx+TJk+MnP/lJDB8+PGbPnh133XVXrF69OoqLi2PSpEnRt2/fKCsri4iI119/PQ455JCYPHlynH/++fHSSy/F6aefHv/4j/8Yl1122Xa9pvsEA9DauE8wACRi4sSJ8dZbb8WsWbOivLw8hgwZEkuWLKm7WNa6deuiffv/+TJXSUlJPPzww3HRRRfFF7/4xejbt29ccMEF9S5mBwAIwQDQKk2bNq3eFdg/tWzZsnp9I0eOjN/97ndNXBUAtH32BAMAAJAMe4Jps6qqqnLaDzzwQE77s/t7IyIefvjhnPbmzZsbvzC26q+/whkRce655+a0b7zxxuYsB/j/7AkGoLVpqs8mK8EAAAAkQwgGAAAgGUIwAAAAyRCCAQAASIZbJNFqrVixIqe9YMGCnPaiRYty2u+8805Tl0QjqK2tzWl//PHHLVQJAAApshIMAABAMoRgAAAAkiEEAwAAkAx7gmkW//3f/53TvuOOO3Lan93vGxGxevXqpiyJVmLw4MEtXQIAAAmxEgwAAEAyhGAAAACSIQQDAACQDHuC2WX/9m//Vq/vxhtvzGk/8sgjOe2ampomrYm24/DDD2/pEgAASIiVYAAAAJIhBAMAAJAMIRgAAIBk2BPMLjv99NPr9ZWXl7dAJbQF7dvn/u7ti1/8YgtVAgBAiqwEAwAAkAwhGAAAgGQIwQAAACRDCAYAACAZLozFLisqKqrX58JYbMmAAQNy2l26dGmhSgAASJGVYAAAAJIhBAMAAJAMIRgAAIBk2BPMLuvWrVtLl0Abcvjhh7d0CQAAJMxKMAAAAMkQggEAAEiGEAwAAEAy7Alml/Xo0aOlS6ANGTJkSEuXAABAwqwEAwAAkAwhGAAAgGQIwQAAACTDnmB2mfsEsyPsCQYAoCVZCQYAACAZQjAAAADJEIIBAABIhj3B7DJ7gtkRhx12WEuXAABAwqwEAwAAkAwhGAAAgGQIwQAAACRDCAYAACAZLozFLuvRo0dLl0ArVlxcnNPu1atXC1UCAABWggEAAEiIEAwAAEAyhGAAAACSYU8wu6xbt24tXQKt2GGHHdbSJQAAQB0rwQAAACRDCAYAACAZQjAAAADJsCeYXda1a9eWLoFWzJ5gAABaEyvBAAAAJEMIBgAAIBlCMAAAAMmwJ5hd1rNnz5YugVYiPz+/Xt8xxxzTApUAAEDDrAQDAACQDCEYAACAZAjBAAAAJEMIBgAAIBkujMUu69q1a0uXQAsZPXp0Tnv+/Pn1xhx00EHNVQ4AAGyTlWAAAACSIQQDAACQDCEYAACAZNgTzC7r3r17S5dAEyksLMxpl5WV5bTPPvvsnHb79n6vBgBA6+YnVgAAAJIhBAMAAJAMIRgAAIBk2BPMLuvRo0dLl0AjOe6443Lac+fOzWnvu+++zVkOAAA0OivBAAAAJEMIBgAAIBlCMAAAAMmwJ5hdttdee9Xr69ChQ067pqamucphC7p27ZrTnjdvXr0xEydObKZqAACgZVgJBgAAIBlCMAAAAMkQggEAAEiGEAwAAEAyXBiLXfbZi2BFRNx222057bPOOiunXVVV1aQ1UV9FRUVO+5577qk35vDDD89pf+5zn2vSmgAAoLlZCQYAACAZQjAAAADJEIIBoJWaM2dO9O/fPwoKCmLEiBGxYsWK7Tpu0aJF0a5du5gwYULTFggAbVC7LMuyli6C3d9TTz2V0/7sD2bl5eXNWA1b0qlTp5z2Z/dyz5o1K6e9zz77NHlNkKrFixfHpEmTYt68eTFixIiYPXt23H333bFmzZro2bPnFo9bu3ZtjBkzJgYMGBDdu3eP+++/f7ter7KyMoqKiqKioiIKCwsb6V0AwM5rqs8mK8EA0ArdcMMN8a1vfSumTJkSBx98cMybNy/22GOP+OlPf7rFY2pqauKUU06JK6+8MgYMGNCM1QJA2yEEA0ArU11dHStXroyxY8fW9bVv3z7Gjh0by5cv3+JxV111VfTs2TPOOOOMbb5GVVVVVFZW5jwAIAVCMAC0Mhs2bIiampooLi7O6S8uLt7i9pH/+I//iNtuuy3mz5+/Xa9RVlYWRUVFdY+SkpJdrhsA2gL3CaZZjBgxIqf99NNP57S/9rWv1TvmmWeeadKaqG/Tpk057Ztuuimn/S//8i857YsvvjinPXPmzHrnbOg+0kDjeu+99+LUU0+N+fPnR48ePbbrmJkzZ8b06dPr2pWVlYIwAEkQggGglenRo0d06NAh1q9fn9O/fv366NWrV73xf/rTn2Lt2rVx/PHH1/XV1tZGRETHjh1jzZo1MXDgwJxj8vPzIz8/vwmqB4DWzdehAaCVycvLi6FDh8bSpUvr+mpra2Pp0qUxcuTIeuMPPPDAeP7552PVqlV1jxNOOCG+/OUvx6pVq6zwAsBfsRIMAK3Q9OnTY/LkyTFs2LAYPnx4zJ49Oz744IOYMmVKRERMmjQp+vbtG2VlZVFQUBCHHnpozvFdu3aNiKjXDwCpE4JpEfvuu29O+7e//W29MaeffnpO+xe/+EWT1sS2ffbqsZdffnlO++tf/3q9Yw4++OAmrQl2VxMnToy33norZs2aFeXl5TFkyJBYsmRJ3cWy1q1bF+3b+0IXAOwoIRgAWqlp06bFtGnTGnxu2bJlWz12wYIFjV8QAOwG/AoZAACAZAjBAAAAJEMIBgAAIBn2BNMqFBQU1OtbuHBhTnvw4ME57UsvvTSn/ek9MWk5Xbp0aekSAABgq6wEAwAAkAwhGAAAgGQIwQAAACTDnmDajEsuuSSnfcghh+S0TznllJx2ZWVlk9eUug4dOuS0e/Xq1UKVAADA9rESDAAAQDKEYAAAAJIhBAMAAJAMe4Jps4477ric9u9+97uc9gknnFDvmJdffrlJa0pNjx49ctp5eXktVAkAAGwfK8EAAAAkQwgGAAAgGUIwAAAAybAnmN3GQQcdlNOeO3duvTFHH310c5WThN69e7d0CQAAsEOsBAMAAJAMIRgAAIBkCMEAAAAkw55gdlvdunVr6RJ2e/YEAwDQ1lgJBgAAIBlCMAAAAMkQggEAAEiGEAwAAEAyXBiL3VbXrl1buoTdXt++fVu6BAAA2CFWggEAAEiGEAwAAEAyhGAAAACSYU8wu63u3bu3dAm7vd69e7d0CQAAsEOsBAMAAJAMIRgAAIBkCMEAAAAkw55gdluFhYX1+jp06JDTrqmpaa5ydkt9+vRp6RIAAGCHWAkGAAAgGUIwAAAAyRCCAQAASIY9wey2Prv/NyKiS5cuOe2KiormKme35D7BAAC0NVaCAQAASIYQDAAAQDKEYAAAAJIhBAMAAJAMF8Zit7V+/fp6fVVVVS1Qye6rT58+LV0CAADsECvBAAAAJEMIBgAAIBlCMAAAAMmwJ5jd1gUXXFCv7+OPP26BSnZfvXv3bukSAABgh1gJBgAAIBlCMAAAAMkQggEAAEiGPcHsNn7961/ntBcvXtxCley+OnTokNPu1atXC1UCAAA7x0owAAAAyRCCAQAASIYQDAAAQDLsCabNev/993Pa06ZNa6FK0rHPPvvktDt29L8QAADaFivBAAAAJEMIBgAAIBlCMAAAAMkQggEAAEiGq9rQZn33u9/Nab/22mvbPKagoCCn/fHHHzdqTbu7Pn36tHQJAACwS6wEAwAAkAwhGAAAgGQIwQAAACTDnmDajBUrVuS0b7rppq2OP/zww+v1jR49Oqd944037nphCendu3dLlwAAALvESjAAAADJEIIBAABIhhAMAABAMtplWZa1dBEAQMuqrKyMoqKiqKioiMLCwpYuBwCa7LPJSjAAAADJEIIBAABIhhAMAABAMoRgAAAAkiEEAwAAkAwhGAAAgGQIwQAAACRDCAYAACAZQjAAAADJEIIBAABIhhAMAABAMoRgAAAAkiEEAwAAkAwhGAAAgGQIwQAAACRDCAaAVmrOnDnRv3//KCgoiBEjRsSKFSu2OHb+/Plx5JFHRrdu3aJbt24xduzYrY4HgFQJwQDQCi1evDimT58epaWl8eyzz8bgwYNj3Lhx8eabbzY4ftmyZXHSSSfFE088EcuXL4+SkpI45phj4i9/+UszVw4ArVu7LMuyli4CAMg1YsSIOOKII+Kmm26KiIja2tooKSmJ888/P2bMmLHN42tqaqJbt25x0003xaRJk7Y5vrKyMoqKiqKioiIKCwt3uX4A2FVN9dlkJRgAWpnq6upYuXJljB07tq6vffv2MXbs2Fi+fPl2nePDDz+MTZs2Rffu3Rt8vqqqKiorK3MeAJACIRgAWpkNGzZETU1NFBcX5/QXFxdHeXn5dp3jkksuiT59+uQE6b9WVlYWRUVFdY+SkpJdrhsA2gIhGAB2M9dee20sWrQo7rvvvigoKGhwzMyZM6OioqLu8frrrzdzlQDQMjq2dAEAQK4ePXpEhw4dYv369Tn969evj169em312B/+8Idx7bXXxmOPPRZf/OIXtzguPz8/8vPzG6VeAGhLrAQDQCuTl5cXQ4cOjaVLl9b11dbWxtKlS2PkyJFbPO4HP/hBXH311bFkyZIYNmxYc5QKAG2OlWAAaIWmT58ekydPjmHDhsXw4cNj9uzZ8cEHH8SUKVMiImLSpEnRt2/fKCsri4iIf/qnf4pZs2bFwoULo3///nV7h7t06RJdunRpsfcBAK2NEAwArdDEiRPjrbfeilmzZkV5eXkMGTIklixZUnexrHXr1kX79v/zha65c+dGdXV1fOMb38g5T2lpaVxxxRXNWToAtGruEwwAuE8wAK2O+wQDAADALhKCAQAASIYQDAAAQDKEYAAAAJIhBAMAAJAMIRgAAIBkCMEAAAAkQwgGAAAgGUIwAAAAyRCCAQAASIYQDAAAQDKEYAAAAJIhBAMAAJAMIRgAAIBkCMEAAAAkQwgGAAAgGUIwAAAAyRCCAQAASIYQDAAAQDKEYAAAAJIhBAMAAJAMIRgAAIBkCMEAAAAkQwgGAAAgGUIwAAAAyRCCAQAASIYQDAAAQDKEYAAAAJIhBAMAAJAMIRgAAIBkCMEAAAAkQwgGAAAgGUIwAAAAyRCCAQAASIYQDAAAQDKEYAAAAJIhBAMAAJAMIRgAAIBkCMEAAAAkQwgGAAAgGUIwAAAAyRCCAQAASIYQDAAAQDKEYAAAAJIhBAMAAJAMIRgAAIBkCMEAAAAkQwgGAAAgGUIwAAAAyRCCAQAASIYQDAAAQDKEYAAAAJIhBAMAAJAMIRgAAIBkCMEAAAAkQwgGAAAgGUIwAAAAyRCCAQAASIYQDAAAQDKEYAAAAJIhBAMAAJAMIRgAAIBkCMEAAAAkQwgGAAAgGUIwAAAAyRCCAQAASIYQDAAAQDKEYAAAAJIhBAMAAJAMIRgAAIBkCMEAAAAkQwgGAAAgGUIwAAAAyRCCAQAASIYQDAAAQDKEYAAAAJIhBAMAAJAMIRgAAIBkCMEAAAAkQwgGAAAgGUIwAAAAyRCCAaCVmjNnTvTv3z8KCgpixIgRsWLFiq2Ov/vuu+PAAw+MgoKCGDRoUDz00EPNVCkAtB1CMAC0QosXL47p06dHaWlpPPvsszF48OAYN25cvPnmmw2Of/LJJ+Okk06KM844I5577rmYMGFCTJgwIV544YVmrhwAWrd2WZZlLV0EAJBrxIgRccQRR8RNN90UERG1tbVRUlIS559/fsyYMaPe+IkTJ8YHH3wQv/71r+v6/uZv/iaGDBkS8+bN2+brVVZWRlFRUVRUVERhYWHjvREA2ElN9dnUsdHOBAA0iurq6li5cmXMnDmzrq99+/YxduzYWL58eYPHLF++PKZPn57TN27cuLj//vsbHF9VVRVVVVV17YqKioj45AcOAGgNPv1Maux1WyEYAFqZDRs2RE1NTRQXF+f0FxcXx+rVqxs8pry8vMHx5eXlDY4vKyuLK6+8sl5/SUnJTlYNAE3j7bffjqKiokY7nxAMAAmaOXNmzsrxu+++G/369Yt169Y16g8aKausrIySkpJ4/fXXfcW8EZjPxmdOG5f5bHwVFRWx3377Rffu3Rv1vEIwALQyPXr0iA4dOsT69etz+tevXx+9evVq8JhevXrt0Pj8/PzIz8+v119UVOSHt0ZWWFhoThuR+Wx85rRxmc/G1759417P2dWhAaCVycvLi6FDh8bSpUvr+mpra2Pp0qUxcuTIBo8ZOXJkzviIiEcffXSL4wEgVVaCAaAVmj59ekyePDmGDRsWw4cPj9mzZ8cHH3wQU6ZMiYiISZMmRd++faOsrCwiIi644II46qij4vrrr4/x48fHokWL4plnnolbbrmlJd8GALQ6QjAAtEITJ06Mt956K2bNmhXl5eUxZMiQWLJkSd3Fr9atW5fz9bBRo0bFwoUL47vf/W5ceuml8bnPfS7uv//+OPTQQ7fr9fLz86O0tLTBr0izc8xp4zKfjc+cNi7z2fiaak7dJxgAAIBk2BMMAABAMoRgAAAAkiEEAwAAkAwhGAAAgGQIwQCQiDlz5kT//v2joKAgRowYEStWrNjq+LvvvjsOPPDAKCgoiEGDBsVDDz3UTJW2HTsyp/Pnz48jjzwyunXrFt26dYuxY8du888gNTv6d/RTixYtinbt2sWECROatsA2aEfn9N13343zzjsvevfuHfn5+fH5z3/ev/2/sqPzOXv27PjCF74QnTt3jpKSkrjooovi448/bqZqW7/f/OY3cfzxx0efPn2iXbt2cf/992/zmGXLlsXhhx8e+fn5ccABB8SCBQt2+HWFYABIwOLFi2P69OlRWloazz77bAwePDjGjRsXb775ZoPjn3zyyTjppJPijDPOiOeeey4mTJgQEyZMiBdeeKGZK2+9dnROly1bFieddFI88cQTsXz58igpKYljjjkm/vKXvzRz5a3Tjs7np9auXRvf/va348gjj2ymStuOHZ3T6urqOProo2Pt2rVxzz33xJo1a2L+/PnRt2/fZq68ddrR+Vy4cGHMmDEjSktL48UXX4zbbrstFi9eHJdeemkzV956ffDBBzF48OCYM2fOdo1/9dVXY/z48fHlL385Vq1aFRdeeGGceeaZ8fDDD+/YC2cAwG5v+PDh2XnnnVfXrqmpyfr06ZOVlZU1OP7EE0/Mxo8fn9M3YsSI7KyzzmrSOtuSHZ3Tz9q8eXO21157ZbfffntTldim7Mx8bt68ORs1alR26623ZpMnT87+7u/+rhkqbTt2dE7nzp2bDRgwIKuurm6uEtuUHZ3P8847L/vKV76S0zd9+vRs9OjRTVpnWxUR2X333bfVMd/5zneyQw45JKdv4sSJ2bhx43botawEA8Burrq6OlauXBljx46t62vfvn2MHTs2li9f3uAxy5cvzxkfETFu3Lgtjk/NzszpZ3344YexadOm6N69e1OV2Wbs7HxeddVV0bNnzzjjjDOao8w2ZWfm9IEHHoiRI0fGeeedF8XFxXHooYfGNddcEzU1Nc1Vdqu1M/M5atSoWLlyZd1Xpl955ZV46KGH4thjj22WmndHjfXZ1LExiwIAWp8NGzZETU1NFBcX5/QXFxfH6tWrGzymvLy8wfHl5eVNVmdbsjNz+lmXXHJJ9OnTp94PdCnamfn8j//4j7jtttti1apVzVBh27Mzc/rKK6/E448/Hqeccko89NBD8fLLL8e5554bmzZtitLS0uYou9Xamfk8+eSTY8OGDTFmzJjIsiw2b94cZ599tq9D74ItfTZVVlbGRx99FJ07d96u81gJBgBoZtdee20sWrQo7rvvvigoKGjpctqc9957L0499dSYP39+9OjRo6XL2W3U1tZGz54945ZbbomhQ4fGxIkT47LLLot58+a1dGlt0rJly+Kaa66Jm2++OZ599tm4995748EHH4yrr766pUtLnpVgANjN9ejRIzp06BDr16/P6V+/fn306tWrwWN69eq1Q+NTszNz+qkf/vCHce2118Zjjz0WX/ziF5uyzDZjR+fzT3/6U6xduzaOP/74ur7a2tqIiOjYsWOsWbMmBg4c2LRFt3I783e0d+/e0alTp+jQoUNd30EHHRTl5eVRXV0deXl5TVpza7Yz83n55ZfHqaeeGmeeeWZERAwaNCg++OCDmDp1alx22WXRvr31yB21pc+mwsLC7V4FjrASDAC7vby8vBg6dGgsXbq0rq+2tjaWLl0aI0eObPCYkSNH5oyPiHj00Ue3OD41OzOnERE/+MEP4uqrr44lS5bEsGHDmqPUNmFH5/PAAw+M559/PlatWlX3OOGEE+quGFtSUtKc5bdKO/N3dPTo0fHyyy/X/UIhIuKPf/xj9O7dO+kAHLFz8/nhhx/WC7qf/oLhk+tAsaMa7bNpx67ZBQC0RYsWLcry8/OzBQsWZP/1X/+VTZ06NevatWtWXl6eZVmWnXrqqdmMGTPqxv/nf/5n1rFjx+yHP/xh9uKLL2alpaVZp06dsueff76l3kKrs6Nzeu2112Z5eXnZPffck73xxht1j/fee6+l3kKrsqPz+VmuDl3fjs7punXrsr322iubNm1atmbNmuzXv/511rNnz+x73/teS72FVmVH57O0tDTba6+9sl/84hfZK6+8kj3yyCPZwIEDsxNPPLGl3kKr895772XPPfdc9txzz2URkd1www3Zc889l7322mtZlmXZjBkzslNPPbVu/CuvvJLtscce2cUXX5y9+OKL2Zw5c7IOHTpkS5Ys2aHXFYIBIBE33nhjtt9++2V5eXnZ8OHDs9/97nd1zx111FHZ5MmTc8bfdddd2ec///ksLy8vO+SQQ7IHH3ywmStu/XZkTvv165dFRL1HaWlp8xfeSu3o39G/JgQ3bEfn9Mknn8xGjBiR5efnZwMGDMi+//3vZ5s3b27mqluvHZnPTZs2ZVdccUU2cODArKCgICspKcnOPffc7J133mn+wlupJ554osH/L346j5MnT86OOuqoescMGTIky8vLywYMGJD97Gc/2+HXbZdl1uIBAABIgz3BAAAAJEMIBgAAIBlCMAAAAMkQggEAAEiGEAwAAEAyhGAAAACSIQQDAACQDCEYAACAZAjBAAAAJEMIBgAAIBlCMAAAAMkQggEAAEjG/wMBWXDIXLqVhgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_every = 10\n",
    "for idx, data in dataset.items():\n",
    "    if idx % display_every == 0:\n",
    "        depth_image = data[DEPTH_MAP_ENTRY]\n",
    "        ray_origins = data[RAYS_ENTRY][RAY_ORIGINS_ENTRY]\n",
    "        ray_directions = data[RAYS_ENTRY][RAY_DIRECTIONS_ENTRY]\n",
    "        camera_pos = data[CAMERA_POS_ENTRY]\n",
    "        camera_angle = data[CAMERA_ANGLE_ENTRY]\n",
    "\n",
    "        # Create a 2x1 plot\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        # Depth map\n",
    "        axes[0].imshow(depth_image, cmap='gray')\n",
    "        axes[0].set_title(f\"Depth Map (Index {idx})\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        # Ray directions as RGB\n",
    "        axes[1].imshow(ray_directions)\n",
    "        axes[1].set_title(f\"Ray Directions (Index {idx})\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        # plt.suptitle(f\"Camera Pos: {camera_pos}, Angle: {camera_angle}\")\n",
    "        # Format the camera angle to two decimal places\n",
    "        formatted_angle = ', '.join([f\"{angle:.2f}\" for angle in camera_angle])\n",
    "        formatted_pos = ', '.join([f\"{pos:.2f}\" for pos in camera_pos])\n",
    "\n",
    "        # Update the plot title\n",
    "        plt.suptitle(f\"Pos: {formatted_pos}, Angle: {formatted_angle}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply post processing\n",
    "dataset = post_process_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFUaajNpNNgJ"
   },
   "source": [
    "## Train !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sphere tracing\n",
    "This method renders a depth map using the model's predictions to dynamically adjust step size during ray marching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_depth_sphere_tracing(\n",
    "    distance_field_model: torch.nn.Module,\n",
    "    ray_origins: torch.Tensor,\n",
    "    ray_directions: torch.Tensor,\n",
    "    depth_map: torch.Tensor, # TODO: remove param, no longer useful\n",
    "    near_thresh: float,\n",
    "    max_iterations: int = 50,\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    # Create a tensor to track active rays\n",
    "    active_mask = torch.ones_like(depth_map, dtype=torch.bool) # TODO: use ray_origins shape instead of depth_map\n",
    "\n",
    "    # Predicted depth map\n",
    "    dstTravelled = torch.full_like(depth_map, 0, requires_grad=True) # TODO: use ray_origins shape instead of depth_map\n",
    "    steps = torch.zeros_like(active_mask, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    all_query_points = []\n",
    "    all_predicted_distances = []\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # Stop if no active rays remain\n",
    "        if not torch.any(active_mask):\n",
    "            break\n",
    "        \n",
    "        # Compute the query points\n",
    "        query_points = ray_origins + ray_directions * dstTravelled[..., None]\n",
    "        query_points.requires_grad_()\n",
    "        \n",
    "        # Predict distances using the model\n",
    "        predicted_distances = distance_field_model(query_points).squeeze(-1)\n",
    "        \n",
    "        # Increment the steps for active rays\n",
    "        steps = torch.where(active_mask, steps + 1, steps)\n",
    "\n",
    "        # Mask for rays that are within the surface threshold\n",
    "        hit_mask = (predicted_distances < near_thresh) & active_mask\n",
    "        \n",
    "        # Store active query points and predictions\n",
    "        all_query_points.append(query_points[~hit_mask])\n",
    "        all_predicted_distances.append(predicted_distances[~hit_mask])\n",
    "\n",
    "        # Update active mask to deactivate rays that hit\n",
    "        active_mask = active_mask & ~hit_mask\n",
    "        \n",
    "        # Update the depth map only for rays that have hit\n",
    "        dstTravelled = torch.where(active_mask, dstTravelled + predicted_distances, dstTravelled)\n",
    "\n",
    "    # Concatenate query points and distances\n",
    "    all_query_points = torch.cat(all_query_points, dim=0)\n",
    "    all_predicted_distances = torch.cat(all_predicted_distances, dim=0)\n",
    "\n",
    "    return dstTravelled, steps, all_query_points, all_predicted_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eikonal loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(model, query_points, eps=1e-4, sample_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Computes numerical gradients of SDF using finite differences.\n",
    "\n",
    "    Args:\n",
    "    - model: The implicit function.\n",
    "    - query_points: Tensor of shape [N, 3], query points.\n",
    "    - eps: Small step for finite differences.\n",
    "    - sample_ratio: Fraction of points to sample for gradient computation.\n",
    "\n",
    "    Returns:\n",
    "    - gradients: Tensor of shape [sampled_N, 3], computed gradients.\n",
    "    \"\"\"\n",
    "    N = query_points.shape[0]\n",
    "    device = query_points.device\n",
    "\n",
    "    # Randomly sample query points\n",
    "    nb_samples = min(int(N * sample_ratio), 5000)\n",
    "    sampled_indices = torch.randperm(N, device=device)[:nb_samples]\n",
    "    sampled_points = query_points[sampled_indices]\n",
    "\n",
    "    # Allocate memory for gradients\n",
    "    gradients = torch.zeros_like(sampled_points, device=device)\n",
    "\n",
    "    for i in range(3):  # Compute gradient w.r.t x, y, z\n",
    "        offset = torch.zeros_like(sampled_points, device=device)\n",
    "        offset[:, i] = eps\n",
    "\n",
    "        forward_points = sampled_points + offset\n",
    "        backward_points = sampled_points - offset\n",
    "\n",
    "        forward_sdf = model(forward_points).squeeze(-1)\n",
    "        backward_sdf = model(backward_points).squeeze(-1)\n",
    "\n",
    "        # Central finite difference\n",
    "        gradients[:, i] = (forward_sdf - backward_sdf) / (2 * eps)\n",
    "\n",
    "    return gradients\n",
    "\n",
    "def compute_eikonal_loss(gradients):\n",
    "    \"\"\"\n",
    "    Computes the Eikonal loss using the computed gradients.\n",
    "\n",
    "    Args:\n",
    "    - gradients (torch.Tensor): Tensor of shape [N, 3] containing the gradients of SDF predictions.\n",
    "\n",
    "    Returns:\n",
    "    - loss (torch.Tensor): Computed Eikonal loss.\n",
    "    \"\"\"\n",
    "    # ||grad|| should be close to 1, compute the deviation from 1\n",
    "    loss = torch.mean(torch.abs(torch.norm(gradients, dim=-1) - 1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint saving function\n",
    "def save_checkpoint(model, optimizer, epoch, loss, checkpoint_path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, checkpoint_path)\n",
    "    # print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "# Checkpoint loading function\n",
    "def load_checkpoint(model, optimizer, checkpoint_path):\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f\"Loaded checkpoint from {checkpoint_path}: Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "        return epoch, loss\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {checkpoint_path}\")\n",
    "        return 0, float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Learning parameters\n",
    "L = 5\n",
    "lr = 1e-5\n",
    "\n",
    "# Model and optimizer\n",
    "model = NeDFModel(L=L).to(device)\n",
    "model.apply(initialize_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Seed RNG\n",
    "seed = 9458\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Load previous checkpoint if available\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "start_epoch, best_loss = load_checkpoint(model, optimizer, os.path.join(checkpoint_dir, CHECKPOINT_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model with random points\n",
    "with torch.no_grad():\n",
    "    scale = 100\n",
    "    test_points = torch.rand(1000, 3, device=device) * scale - torch.tensor([scale/2, scale/2, scale/2], device=device)\n",
    "    # predicted_distances = model(test_points).squeeze(-1).detach().cpu().numpy()\n",
    "    estimate_distances(test_points, dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JovhcSy1NIhr",
    "outputId": "50f322e3-552b-4802-e03e-1e5237a64ecf"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "display_every = 1\n",
    "plot_every = 1\n",
    "save_every = 50  # Save every n iterations\n",
    "batch_size = 1024*8  # Number of rays per batch\n",
    "\n",
    "exceed_depth_threshold = 1.2  # Factor threshold for exceeding sampled depth\n",
    "max_ray_marching_steps = 10\n",
    "\n",
    "# Loss weights\n",
    "exceed_depth_threshold = 1.1\n",
    "exceed_depth_penalty_weight = 1\n",
    "eikonal_penalty_weight = 1\n",
    "depth_penalty_weight = 1\n",
    "steps_penalty_weight = 5\n",
    "\n",
    "num_iters = start_epoch + 1 # Change this here for longer sessions\n",
    "near_thresh = 0.01\n",
    "\n",
    "# Training Loop\n",
    "for i in range(start_epoch, num_iters):\n",
    "    # Sample random dataset indices\n",
    "    indices = torch.randint(0, len(dataset), (batch_size,))\n",
    "    # Extract data for all sampled indices\n",
    "    data_batch = [list(dataset.values())[idx] for idx in indices.cpu().numpy()]\n",
    "\n",
    "    # Flatten ray and depth arrays for uniform sampling\n",
    "    target_depth = torch.cat([torch.tensor(data[FILTERED_DEPTH_MAP_ENTRY], dtype=torch.float32) for data in data_batch]).to(device)\n",
    "    ray_origins = torch.cat([torch.tensor(data[RAYS_ENTRY][RAY_ORIGINS_ENTRY], dtype=torch.float32) for data in data_batch]).to(device)\n",
    "    ray_directions = torch.cat([torch.tensor(data[RAYS_ENTRY][RAY_DIRECTIONS_ENTRY], dtype=torch.float32) for data in data_batch]).to(device)\n",
    "\n",
    "    # Convert to GLM coordinates\n",
    "    ray_origins = blender_to_opengl(ray_origins).to(device)\n",
    "    ray_directions = blender_to_opengl(ray_directions).to(device)\n",
    "\n",
    "    # Sample random pixel coordinates from the flattened array\n",
    "    rand_coords = torch.randint(0, target_depth.shape[0], (batch_size,))\n",
    "\n",
    "    # Gather samples\n",
    "    sampled_depth = target_depth[rand_coords]\n",
    "    sampled_ray_origins = ray_origins[rand_coords]\n",
    "    sampled_ray_directions = ray_directions[rand_coords]\n",
    "\n",
    "    # Ensure there are valid samples left after sampling\n",
    "    if sampled_ray_origins.shape[0] == 0:\n",
    "        print(\"No valid samples found, skipping this batch.\")\n",
    "        continue\n",
    "\n",
    "    # Perform sphere tracing\n",
    "    depth_predicted, steps, query_points, query_results = render_depth_sphere_tracing(\n",
    "        model, sampled_ray_origins, sampled_ray_directions, sampled_depth, near_thresh, max_ray_marching_steps\n",
    "    )\n",
    "\n",
    "    # Select a subset of query points for distance estimation\n",
    "    loss_samples = 5\n",
    "    query_points_for_loss = query_points[:loss_samples]\n",
    "    query_results_for_loss = query_results[:loss_samples]\n",
    "    estimated_distances = estimate_distances(query_points_for_loss, dataset, device)\n",
    "    print(f\"### Query points:\\n{query_points_for_loss}\")\n",
    "    print(f\"### Query results:\\n{query_results_for_loss}\")\n",
    "    print(f\"### Estimated distances:\\n{estimated_distances}\")\n",
    "\n",
    "    # Correctly reshape to the largest possible square\n",
    "    valid_size = int(torch.sqrt(torch.tensor(sampled_depth.shape[0], dtype=torch.float32)).floor().item())\n",
    "    reshaped_predicted = depth_predicted[:valid_size**2].view(valid_size, valid_size).detach().cpu().numpy()\n",
    "    reshaped_target = sampled_depth[:valid_size**2].view(valid_size, valid_size).detach().cpu().numpy()\n",
    "\n",
    "    # Display predicted vs target depth\n",
    "    if i % plot_every == 0:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        axes[0].imshow(reshaped_target, cmap='gray')\n",
    "        axes[0].set_title(\"Target Depth\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(reshaped_predicted, cmap='gray')\n",
    "        axes[1].set_title(\"Predicted Depth\")\n",
    "        axes[1].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    # Combine Losses\n",
    "    average_steps = steps.mean()\n",
    "    steps_loss = average_steps / max_ray_marching_steps \n",
    "\n",
    "    gradients = compute_gradients(model, query_points)\n",
    "    eikonal_loss = compute_eikonal_loss(gradients)\n",
    "\n",
    "    exceed_depth_diff = torch.clamp(depth_predicted - sampled_depth * exceed_depth_threshold, min=0)\n",
    "    exceed_depth_loss = exceed_depth_diff.mean()\n",
    "\n",
    "    depth_loss = torch.nn.functional.mse_loss(depth_predicted, sampled_depth)\n",
    "    total_loss = depth_loss * depth_penalty_weight\n",
    "    total_loss = total_loss + eikonal_loss * eikonal_penalty_weight\n",
    "    total_loss = total_loss + exceed_depth_loss * exceed_depth_penalty_weight\n",
    "    total_loss = total_loss + steps_loss * steps_penalty_weight\n",
    "\n",
    "    if (total_loss == float('inf') or total_loss == float('nan')):\n",
    "        print(\"Loss is infinite or NaN, skipping this batch.\")\n",
    "        continue\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Display Progress\n",
    "    if i % display_every == 0:\n",
    "        print(f\"\\n### Step {i} ###\")\n",
    "        print(f\"Average distance: {query_results.mean().item():.2f}\")\n",
    "        print(f\"Average steps: {steps.mean().item():.2f}\")\n",
    "        print(f\"Total loss: {total_loss.item():.2f} (Depth: {depth_loss.item():.2f}, Eikonal: {eikonal_loss.item():.2f}, Exceed: {exceed_depth_loss.item():.2f})\")\n",
    "        print(f\"Best loss: {best_loss:.2f}\")\n",
    "\n",
    "    # Save checkpoint periodically or if loss improves\n",
    "    if i % save_every == 0 or total_loss.item() < best_loss:\n",
    "        if total_loss.item() < best_loss:\n",
    "            best_loss = total_loss.item()\n",
    "        save_checkpoint(model, optimizer, i, total_loss.item(), os.path.join(checkpoint_dir, CHECKPOINT_NAME))\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "L = 5\n",
    "model = NeDFModel(L=L).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "start_epoch, best_loss = load_checkpoint(model, optimizer, os.path.join(checkpoint_dir, CHECKPOINT_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(DATASET_SIZE//4):\n",
    "    # Select one dataset entry\n",
    "    data_entry = dataset[i]\n",
    "\n",
    "    # Extract old depth and ray data\n",
    "    old_target_depth = torch.tensor(data_entry[\"OLD\" + DEPTH_MAP_ENTRY], dtype=torch.float32, device=device)\n",
    "    old_ray_origins = torch.tensor(data_entry[\"OLD\" + RAYS_ENTRY][RAY_ORIGINS_ENTRY], dtype=torch.float32, device=device)\n",
    "    old_ray_directions = torch.tensor(data_entry[\"OLD\" + RAYS_ENTRY][RAY_DIRECTIONS_ENTRY], dtype=torch.float32, device=device)\n",
    "\n",
    "    # Perform inference on full data\n",
    "    width, height = old_ray_origins.shape[1], old_ray_origins.shape[0]\n",
    "    predicted_depth, _, _, _ = render_depth_sphere_tracing(\n",
    "        model, old_ray_origins, old_ray_directions, old_target_depth, near_thresh\n",
    "    )\n",
    "\n",
    "    # Initialize full depth maps with placeholder\n",
    "    width, height = old_ray_origins.shape[1], old_ray_origins.shape[0]\n",
    "    full_predicted_depth = predicted_depth.view(-1).clone()\n",
    "    full_target_depth = old_target_depth.view(-1).clone()\n",
    "\n",
    "    # Reshape to 2D\n",
    "    full_predicted_depth = full_predicted_depth.view(height, width)\n",
    "    full_target_depth = full_target_depth.view(height, width)\n",
    "\n",
    "    # Display predicted vs target depth\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(full_target_depth.detach().cpu().numpy(), cmap='gray')\n",
    "    axes[0].set_title(\"Target Depth\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(full_predicted_depth.detach().cpu().numpy(), cmap='gray')\n",
    "    axes[1].set_title(\"Predicted Depth\")\n",
    "    axes[1].axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tesing on new camera angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we would separate the dataset in two test / train sets instead of using new orientations. That way we could compute the loss numerically on top of observing the visual difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(DATASET_SIZE//4):\n",
    "    # Interpolate between two camera positions\n",
    "    data_entry1 = dataset[i]\n",
    "    data_entry2 = dataset[i + 1]\n",
    "\n",
    "    cam_pos1 = torch.tensor(data_entry1[CAMERA_POS_ENTRY], dtype=torch.float32, device=device)\n",
    "    cam_pos2 = torch.tensor(data_entry2[CAMERA_POS_ENTRY], dtype=torch.float32, device=device)\n",
    "\n",
    "    # Interpolation factor\n",
    "    alpha = 0.5\n",
    "    interpolated_cam_pos = cam_pos1 * (1 - alpha) + cam_pos2 * alpha\n",
    "\n",
    "    # Interpolate camera angles\n",
    "    cam_dir1 = torch.tensor(data_entry1[CAMERA_ANGLE_ENTRY], dtype=torch.float32, device=device)\n",
    "    cam_dir2 = torch.tensor(data_entry2[CAMERA_ANGLE_ENTRY], dtype=torch.float32, device=device)\n",
    "    interpolated_cam_dir = cam_dir1 * (1 - alpha) + cam_dir2 * alpha\n",
    "\n",
    "    # Generate new rays using the ray generation function\n",
    "    width, height = 100, 100\n",
    "    focal_length = fov_to_focal_length(FOV, width)\n",
    "    tform_cam2world = pos_angle_to_tform_cam2world(interpolated_cam_pos, interpolated_cam_dir)\n",
    "    ray_origins, ray_directions = get_ray_bundle(height, width, focal_length, tform_cam2world)\n",
    "    ray_origins = ray_origins.to(device)\n",
    "    ray_directions = ray_directions.to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    infinite_depth_map = torch.full((height, width), 10000.0, device=device) # Needed in sphere tracing to let rays go as far as they want\n",
    "    predicted_depth, _, _, _ = render_depth_sphere_tracing(\n",
    "        model, ray_origins, ray_directions, infinite_depth_map, near_thresh\n",
    "    )\n",
    "\n",
    "    # Reshape and visualize\n",
    "    predicted_depth_2d = predicted_depth.view(height, width)\n",
    "\n",
    "    # Extract target depth for comparison from first entry\n",
    "    target_depth_2d = torch.tensor(data_entry1[\"OLD\" + DEPTH_MAP_ENTRY], dtype=torch.float32, device=device)\n",
    "\n",
    "    # Display predicted vs target depth\n",
    "    formatted_pos1 = ', '.join([f\"{pos:.2f}\" for pos in cam_pos1])\n",
    "    formatted_pos2 = ', '.join([f\"{pos:.2f}\" for pos in cam_pos2])\n",
    "    formatted_interpolated_cam_pos = ', '.join([f\"{pos:.2f}\" for pos in interpolated_cam_pos])\n",
    "    formatted_dir1 = ', '.join([f\"{dir:.2f}\" for dir in cam_dir1])\n",
    "    formatted_dir2 = ', '.join([f\"{dir:.2f}\" for dir in cam_dir2])\n",
    "    formatted_interpolated_cam_dir = ', '.join([f\"{dir:.2f}\" for dir in interpolated_cam_dir])\n",
    "\n",
    "    print(f\"Interpolating between indices {i} and {i + 1}...\")\n",
    "    print(f\"pos1: {formatted_pos1}, pos2: {formatted_pos2}, interpolated: {formatted_interpolated_cam_pos}\")\n",
    "    print(f\"dir1: {formatted_dir1}, dir2: {formatted_dir2}, interpolated: {formatted_interpolated_cam_dir}\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    axes[0].imshow(target_depth_2d.detach().cpu().numpy(), cmap='gray')\n",
    "    axes[0].set_title(\"Target Depth\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(predicted_depth_2d.detach().cpu().numpy(), cmap='gray')\n",
    "    axes[1].set_title(\"Predicted Depth\")\n",
    "    axes[1].axis('off')\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
