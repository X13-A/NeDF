{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZh3aklt3ZET"
   },
   "source": [
    "## Tiny NeDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ptTYjWao3VsM"
   },
   "outputs": [],
   "source": [
    "# Import all the good stuff\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilitary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meshgrid_xy(tensor1: torch.Tensor, tensor2: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"Mimick np.meshgrid(..., indexing=\"xy\") in pytorch. torch.meshgrid only allows \"ij\" indexing.\n",
    "    (If you're unsure what this means, safely skip trying to understand this, and run a tiny example!)\n",
    "\n",
    "    Args:\n",
    "      tensor1 (torch.Tensor): Tensor whose elements define the first dimension of the returned meshgrid.\n",
    "      tensor2 (torch.Tensor): Tensor whose elements define the second dimension of the returned meshgrid.\n",
    "    \"\"\"\n",
    "    # TESTED\n",
    "    ii, jj = torch.meshgrid(tensor1, tensor2)\n",
    "    return ii.transpose(-1, -2), jj.transpose(-1, -2)\n",
    "\n",
    "\n",
    "def cumprod_exclusive(tensor: torch.Tensor) -> torch.Tensor:\n",
    "  r\"\"\"Mimick functionality of tf.math.cumprod(..., exclusive=True), as it isn't available in PyTorch.\n",
    "\n",
    "  Args:\n",
    "    tensor (torch.Tensor): Tensor whose cumprod (cumulative product, see `torch.cumprod`) along dim=-1\n",
    "      is to be computed.\n",
    "\n",
    "  Returns:\n",
    "    cumprod (torch.Tensor): cumprod of Tensor along dim=-1, mimiciking the functionality of\n",
    "      tf.math.cumprod(..., exclusive=True) (see `tf.math.cumprod` for details).\n",
    "  \"\"\"\n",
    "  # TESTED\n",
    "  # Only works for the last dimension (dim=-1)\n",
    "  dim = -1\n",
    "  # Compute regular cumprod first (this is equivalent to `tf.math.cumprod(..., exclusive=False)`).\n",
    "  cumprod = torch.cumprod(tensor, dim)\n",
    "  # \"Roll\" the elements along dimension 'dim' by 1 element.\n",
    "  cumprod = torch.roll(cumprod, 1, dim)\n",
    "  # Replace the first element by \"1\" as this is what tf.cumprod(..., exclusive=True) does.\n",
    "  cumprod[..., 0] = 1.\n",
    "\n",
    "  return cumprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EHNwlsOT7NTp"
   },
   "outputs": [],
   "source": [
    "def get_ray_bundle(height: int, width: int, focal_length: float, tform_cam2world: torch.Tensor):\n",
    "  r\"\"\"Compute the bundle of rays passing through all pixels of an image (one ray per pixel).\n",
    "\n",
    "  Args:\n",
    "    height (int): Height of an image (number of pixels).\n",
    "    width (int): Width of an image (number of pixels).\n",
    "    focal_length (float or torch.Tensor): Focal length (number of pixels, i.e., calibrated intrinsics).\n",
    "    tform_cam2world (torch.Tensor): A 6-DoF rigid-body transform (shape: :math:`(4, 4)`) that\n",
    "      transforms a 3D point from the camera frame to the \"world\" frame for the current example.\n",
    "\n",
    "  Returns:\n",
    "    ray_origins (torch.Tensor): A tensor of shape :math:`(width, height, 3)` denoting the centers of\n",
    "      each ray. `ray_origins[i][j]` denotes the origin of the ray passing through pixel at\n",
    "      row index `j` and column index `i`.\n",
    "      (TODO: double check if explanation of row and col indices convention is right).\n",
    "    ray_directions (torch.Tensor): A tensor of shape :math:`(width, height, 3)` denoting the\n",
    "      direction of each ray (a unit vector). `ray_directions[i][j]` denotes the direction of the ray\n",
    "      passing through the pixel at row index `j` and column index `i`.\n",
    "      (TODO: double check if explanation of row and col indices convention is right).\n",
    "  \"\"\"\n",
    "  # TESTED\n",
    "  ii, jj = meshgrid_xy(\n",
    "      torch.arange(width).to(tform_cam2world),\n",
    "      torch.arange(height).to(tform_cam2world)\n",
    "  )\n",
    "  directions = torch.stack([(ii - width * .5) / focal_length,\n",
    "                            -(jj - height * .5) / focal_length,\n",
    "                            -torch.ones_like(ii)\n",
    "                           ], dim=-1)\n",
    "  ray_directions = torch.sum(directions[..., None, :] * tform_cam2world[:3, :3], dim=-1)\n",
    "  ray_origins = tform_cam2world[:3, -1].expand(ray_directions.shape)\n",
    "  return ray_origins, ray_directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgoNR03iIs7R"
   },
   "source": [
    "### Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UjFN6FNzIqxl"
   },
   "outputs": [],
   "source": [
    "class VeryTinyNeDFModel(torch.nn.Module):\n",
    "    r\"\"\"Define a \"very tiny\" NeDF model comprising three fully connected layers.\"\"\"\n",
    "    def __init__(self, filter_size=128, far_thresh=1000.0, L=10):\n",
    "        self.L = L\n",
    "        # self.input_dim = 3 + 2 * L * 3 # Due to positional encoding\n",
    "        self.input_dim = 3 # Since we don't use it, just x,y,z\n",
    "        super(VeryTinyNeDFModel, self).__init__()\n",
    "        # Input layer (default: 3 + 2 * L * 3 -> 128 if positional encoding is enabled, 3 -> 128 otherwise)\n",
    "        self.layer1 = torch.nn.Linear(self.input_dim, filter_size)\n",
    "        # Layer 2 (default: 128 -> 128)\n",
    "        self.layer2 = torch.nn.Linear(filter_size, filter_size)\n",
    "        # Layer 3 (default: 128 -> 1) for predicting distance\n",
    "        self.layer3 = torch.nn.Linear(filter_size, 1)\n",
    "        # Short hand for torch.nn.functional.relu\n",
    "        self.relu = torch.nn.functional.relu\n",
    "        self.far_thresh = far_thresh\n",
    "\n",
    "    # Not convincing currently\n",
    "    def positional_encoding(self, x):\n",
    "        encoded = [x]\n",
    "        for i in range(self.L):\n",
    "            encoded.append(torch.sin(2**i * x))\n",
    "            encoded.append(torch.cos(2**i * x))\n",
    "        return torch.cat(encoded, dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.positional_encoding(x)\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x) # Output is raw distance\n",
    "        return torch.clamp(x, min=0.0, max=self.far_thresh) # Clamp output to prevent crazy / negative distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ierxw2dsL3pU"
   },
   "source": [
    "### GPU vs CPU ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uKNiPtnML8i9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zB3NGalaLlN1"
   },
   "source": [
    "### Load input images, poses, intrinsics, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1w2QkjCkLc9Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100])\n"
     ]
    }
   ],
   "source": [
    "# Load input images, poses, and intrinsics\n",
    "data = np.load(\"data/tiny_nedf/depth_map_test.npz\")\n",
    "\n",
    "# Camera extrinsics (poses)\n",
    "testpose = data[\"pose\"]\n",
    "testpose = torch.from_numpy(testpose).to(device)\n",
    "\n",
    "# Focal length (intrinsics)\n",
    "focal_length = data[\"focal\"]\n",
    "focal_length = torch.from_numpy(focal_length).to(device)\n",
    "\n",
    "testimg = data[\"depth_map\"]\n",
    "# testimg = np.resize(testimg, (6, 6))\n",
    "testimg = torch.from_numpy(testimg).to(device)\n",
    "\n",
    "print(testimg.shape)\n",
    "# Height and width depth map\n",
    "height, width = testimg.shape[:2]\n",
    "\n",
    "# Near and far clipping thresholds for depth values.\n",
    "near_thresh = 0.01\n",
    "far_thresh = 500.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVsKUODnM9KK"
   },
   "source": [
    "### Display the image used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "_04FamFHM7l8",
    "outputId": "f4999258-65d6-475d-a17c-a0836b8062c0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(testimg.detach().cpu().numpy(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFUaajNpNNgJ"
   },
   "source": [
    "### Train !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method renders a depth map using the model's predictions to dynamically adjust step size during ray marching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_depth_sphere_tracing(\n",
    "    distance_field_model: torch.nn.Module,\n",
    "    ray_origins: torch.Tensor,\n",
    "    ray_directions: torch.Tensor,\n",
    "    depth_map: torch.Tensor,\n",
    "    near_thresh: float,\n",
    "    max_iterations: int = 50\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    # Create a tensor to track active rays\n",
    "    active_mask = torch.ones_like(depth_map, dtype=torch.bool)\n",
    "\n",
    "    # Predicted depth map\n",
    "    dstTravelled = torch.full_like(depth_map, 0, requires_grad=True)\n",
    "    steps = torch.zeros_like(active_mask, dtype=torch.float32)\n",
    "    \n",
    "    all_query_points = []\n",
    "    all_predicted_distances = []\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # Stop if no active rays remain\n",
    "        if not torch.any(active_mask):\n",
    "            break\n",
    "        \n",
    "        # Compute the query points\n",
    "        query_points = ray_origins + ray_directions * dstTravelled[..., None]\n",
    "        query_points.requires_grad_()\n",
    "        \n",
    "        # Predict distances using the model\n",
    "        predicted_distances = distance_field_model(query_points).squeeze(-1)\n",
    "\n",
    "        # Increment the steps for active rays\n",
    "        steps = torch.where(active_mask, steps + 1, steps)\n",
    "\n",
    "        # Mask for rays that are within the surface threshold or exceeding far_thresh\n",
    "        hit_mask = ((predicted_distances < near_thresh) | (dstTravelled >= depth_map)) & active_mask\n",
    "\n",
    "        # Store active query points and predictions\n",
    "        all_query_points.append(query_points[~hit_mask])\n",
    "        all_predicted_distances.append(predicted_distances[~hit_mask])\n",
    "\n",
    "        # Update active mask to deactivate rays that hit\n",
    "        active_mask = active_mask & ~hit_mask\n",
    "        \n",
    "        # Update the depth map only for rays that have hit\n",
    "        dstTravelled = torch.where(active_mask, dstTravelled + predicted_distances, dstTravelled)\n",
    "\n",
    "    # Concatenate query points and distances\n",
    "    all_query_points = torch.cat(all_query_points, dim=0)\n",
    "    all_predicted_distances = torch.cat(all_predicted_distances, dim=0)\n",
    "\n",
    "    return dstTravelled, steps, all_query_points, all_predicted_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(model, query_points, eps=1e-4, sample_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Computes numerical gradients of SDF using finite differences.\n",
    "\n",
    "    Args:\n",
    "    - model: The implicit function.\n",
    "    - query_points: Tensor of shape [N, 3], query points.\n",
    "    - eps: Small step for finite differences.\n",
    "    - sample_ratio: Fraction of points to sample for gradient computation.\n",
    "\n",
    "    Returns:\n",
    "    - gradients: Tensor of shape [sampled_N, 3], computed gradients.\n",
    "    \"\"\"\n",
    "    N = query_points.shape[0]\n",
    "    device = query_points.device\n",
    "\n",
    "    # Randomly sample query points\n",
    "    nb_samples = min(int(N * sample_ratio), 5000)\n",
    "    sampled_indices = torch.randperm(N, device=device)[:nb_samples]\n",
    "    sampled_points = query_points[sampled_indices]\n",
    "\n",
    "    # Allocate memory for gradients\n",
    "    gradients = torch.zeros_like(sampled_points, device=device)\n",
    "\n",
    "    for i in range(3):  # Compute gradient w.r.t x, y, z\n",
    "        offset = torch.zeros_like(sampled_points, device=device)\n",
    "        offset[:, i] = eps\n",
    "\n",
    "        forward_points = sampled_points + offset\n",
    "        backward_points = sampled_points - offset\n",
    "\n",
    "        forward_sdf = model(forward_points).squeeze(-1)\n",
    "        backward_sdf = model(backward_points).squeeze(-1)\n",
    "\n",
    "        # Central finite difference\n",
    "        gradients[:, i] = (forward_sdf - backward_sdf) / (2 * eps)\n",
    "\n",
    "    return gradients\n",
    "\n",
    "def compute_eikonal_loss(gradients):\n",
    "    \"\"\"\n",
    "    Computes the Eikonal loss using the computed gradients.\n",
    "\n",
    "    Args:\n",
    "    - gradients (torch.Tensor): Tensor of shape [N, 3] containing the gradients of SDF predictions.\n",
    "\n",
    "    Returns:\n",
    "    - loss (torch.Tensor): Computed Eikonal loss.\n",
    "    \"\"\"\n",
    "    # ||grad|| should be close to 1, compute the deviation from 1\n",
    "    loss = torch.mean(torch.abs(torch.norm(gradients, dim=-1) - 1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JovhcSy1NIhr",
    "outputId": "50f322e3-552b-4802-e03e-1e5237a64ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:\n",
      "Loss: 6.885506629943848\n",
      "Average steps: 45.96039962768555\n",
      "Eikonal loss: 0.9384042620658875\n",
      "Step 5:\n",
      "Loss: 5.683990001678467\n",
      "Average steps: 38.471797943115234\n",
      "Eikonal loss: 0.9152386784553528\n",
      "Step 10:\n",
      "Loss: 4.960142612457275\n",
      "Average steps: 32.16339874267578\n",
      "Eikonal loss: 0.8988671898841858\n",
      "Step 15:\n",
      "Loss: 4.641495227813721\n",
      "Average steps: 28.435998916625977\n",
      "Eikonal loss: 0.8882904052734375\n",
      "Step 20:\n",
      "Loss: 4.4679856300354\n",
      "Average steps: 26.381500244140625\n",
      "Eikonal loss: 0.8835778832435608\n",
      "Step 25:\n",
      "Loss: 4.39393424987793\n",
      "Average steps: 24.38519859313965\n",
      "Eikonal loss: 0.8764650225639343\n",
      "Step 30:\n",
      "Loss: 4.34262228012085\n",
      "Average steps: 22.64219856262207\n",
      "Eikonal loss: 0.8659908175468445\n",
      "Step 35:\n",
      "Loss: 4.29560661315918\n",
      "Average steps: 21.769298553466797\n",
      "Eikonal loss: 0.855269193649292\n",
      "Step 40:\n",
      "Loss: 4.25394868850708\n",
      "Average steps: 21.557899475097656\n",
      "Eikonal loss: 0.8447630405426025\n",
      "Step 45:\n",
      "Loss: 4.213818550109863\n",
      "Average steps: 21.46769905090332\n",
      "Eikonal loss: 0.8338454961776733\n",
      "Step 50:\n",
      "Loss: 4.187311172485352\n",
      "Average steps: 21.3528995513916\n",
      "Eikonal loss: 0.8249068260192871\n",
      "Step 55:\n",
      "Loss: 4.164594650268555\n",
      "Average steps: 21.145599365234375\n",
      "Eikonal loss: 0.8184822797775269\n",
      "Step 60:\n",
      "Loss: 4.136742115020752\n",
      "Average steps: 20.88559913635254\n",
      "Eikonal loss: 0.8123131990432739\n",
      "Step 65:\n",
      "Loss: 4.09530782699585\n",
      "Average steps: 20.414600372314453\n",
      "Eikonal loss: 0.8046440482139587\n",
      "Step 70:\n",
      "Loss: 4.0486273765563965\n",
      "Average steps: 19.67919921875\n",
      "Eikonal loss: 0.796414852142334\n",
      "Step 75:\n",
      "Loss: 4.005537986755371\n",
      "Average steps: 19.04640007019043\n",
      "Eikonal loss: 0.788087010383606\n",
      "Step 80:\n",
      "Loss: 3.9568636417388916\n",
      "Average steps: 18.6122989654541\n",
      "Eikonal loss: 0.7774943113327026\n",
      "Step 85:\n",
      "Loss: 3.9086122512817383\n",
      "Average steps: 17.993398666381836\n",
      "Eikonal loss: 0.7672712802886963\n",
      "Step 90:\n",
      "Loss: 3.8466124534606934\n",
      "Average steps: 17.230600357055664\n",
      "Eikonal loss: 0.7547226548194885\n",
      "Step 95:\n",
      "Loss: 3.7786717414855957\n",
      "Average steps: 16.578100204467773\n",
      "Eikonal loss: 0.740455150604248\n",
      "Step 100:\n",
      "Loss: 3.733107566833496\n",
      "Average steps: 16.132400512695312\n",
      "Eikonal loss: 0.7301504611968994\n",
      "Step 105:\n",
      "Loss: 3.678365707397461\n",
      "Average steps: 15.672800064086914\n",
      "Eikonal loss: 0.7187530398368835\n",
      "Step 110:\n",
      "Loss: 3.614595651626587\n",
      "Average steps: 15.20419979095459\n",
      "Eikonal loss: 0.704873263835907\n",
      "Step 115:\n",
      "Loss: 3.5587551593780518\n",
      "Average steps: 14.658099174499512\n",
      "Eikonal loss: 0.6922129988670349\n",
      "Step 120:\n",
      "Loss: 3.467708110809326\n",
      "Average steps: 14.098099708557129\n",
      "Eikonal loss: 0.6725999116897583\n",
      "Step 125:\n",
      "Loss: 3.421316146850586\n",
      "Average steps: 13.638199806213379\n",
      "Eikonal loss: 0.6619700789451599\n",
      "Step 130:\n",
      "Loss: 3.3610386848449707\n",
      "Average steps: 13.227699279785156\n",
      "Eikonal loss: 0.6488537788391113\n",
      "Step 135:\n",
      "Loss: 3.2891297340393066\n",
      "Average steps: 12.844900131225586\n",
      "Eikonal loss: 0.6328983306884766\n",
      "Step 140:\n",
      "Loss: 3.2028679847717285\n",
      "Average steps: 12.409599304199219\n",
      "Eikonal loss: 0.6157116889953613\n",
      "Step 145:\n",
      "Loss: 3.1136300563812256\n",
      "Average steps: 11.935199737548828\n",
      "Eikonal loss: 0.5978385806083679\n",
      "Step 150:\n",
      "Loss: 3.049313545227051\n",
      "Average steps: 11.52869987487793\n",
      "Eikonal loss: 0.5844441056251526\n",
      "Step 155:\n",
      "Loss: 2.9684948921203613\n",
      "Average steps: 11.156000137329102\n",
      "Eikonal loss: 0.5679425597190857\n",
      "Step 160:\n",
      "Loss: 2.8748221397399902\n",
      "Average steps: 10.813399314880371\n",
      "Eikonal loss: 0.5477598309516907\n",
      "Step 165:\n",
      "Loss: 2.7966625690460205\n",
      "Average steps: 10.487500190734863\n",
      "Eikonal loss: 0.5310853123664856\n",
      "Step 170:\n",
      "Loss: 2.7137906551361084\n",
      "Average steps: 10.120299339294434\n",
      "Eikonal loss: 0.5133457183837891\n",
      "Step 175:\n",
      "Loss: 2.6207027435302734\n",
      "Average steps: 9.796699523925781\n",
      "Eikonal loss: 0.49365344643592834\n",
      "Step 180:\n",
      "Loss: 2.5208263397216797\n",
      "Average steps: 9.480799674987793\n",
      "Eikonal loss: 0.472694993019104\n",
      "Step 185:\n",
      "Loss: 2.41033935546875\n",
      "Average steps: 9.085700035095215\n",
      "Eikonal loss: 0.44787558913230896\n",
      "Step 190:\n",
      "Loss: 2.288639545440674\n",
      "Average steps: 8.72760009765625\n",
      "Eikonal loss: 0.4214615523815155\n",
      "Step 195:\n",
      "Loss: 2.1581358909606934\n",
      "Average steps: 8.34570026397705\n",
      "Eikonal loss: 0.39266830682754517\n",
      "Step 200:\n",
      "Loss: 2.0438153743743896\n",
      "Average steps: 8.008199691772461\n",
      "Eikonal loss: 0.36747729778289795\n",
      "Step 205:\n",
      "Loss: 1.9420976638793945\n",
      "Average steps: 7.73799991607666\n",
      "Eikonal loss: 0.3461538553237915\n",
      "Step 210:\n",
      "Loss: 1.8156784772872925\n",
      "Average steps: 7.412899971008301\n",
      "Eikonal loss: 0.31963035464286804\n",
      "Step 215:\n",
      "Loss: 1.6585772037506104\n",
      "Average steps: 7.108599662780762\n",
      "Eikonal loss: 0.2862081825733185\n",
      "Step 220:\n",
      "Loss: 1.5111604928970337\n",
      "Average steps: 6.809599876403809\n",
      "Eikonal loss: 0.2539688050746918\n",
      "Step 225:\n",
      "Loss: 1.4636921882629395\n",
      "Average steps: 6.649199962615967\n",
      "Eikonal loss: 0.2416144460439682\n",
      "Step 230:\n",
      "Loss: 1.4192439317703247\n",
      "Average steps: 6.591300010681152\n",
      "Eikonal loss: 0.2307264655828476\n",
      "Step 235:\n",
      "Loss: 1.3663947582244873\n",
      "Average steps: 6.52810001373291\n",
      "Eikonal loss: 0.21917074918746948\n",
      "Step 240:\n",
      "Loss: 1.3425555229187012\n",
      "Average steps: 6.430899620056152\n",
      "Eikonal loss: 0.21292255818843842\n",
      "Step 245:\n",
      "Loss: 1.3022350072860718\n",
      "Average steps: 6.314300060272217\n",
      "Eikonal loss: 0.20431283116340637\n",
      "Step 250:\n",
      "Loss: 1.2319636344909668\n",
      "Average steps: 6.177599906921387\n",
      "Eikonal loss: 0.188988596200943\n",
      "Step 255:\n",
      "Loss: 1.221239686012268\n",
      "Average steps: 6.110099792480469\n",
      "Eikonal loss: 0.18588490784168243\n",
      "Step 260:\n",
      "Loss: 1.167952060699463\n",
      "Average steps: 6.040299892425537\n",
      "Eikonal loss: 0.17409203946590424\n",
      "Step 265:\n",
      "Loss: 1.148409128189087\n",
      "Average steps: 6.014499664306641\n",
      "Eikonal loss: 0.1692637950181961\n",
      "Step 270:\n",
      "Loss: 1.1188974380493164\n",
      "Average steps: 6.015199661254883\n",
      "Eikonal loss: 0.1624075174331665\n",
      "Step 275:\n",
      "Loss: 1.0824413299560547\n",
      "Average steps: 5.985599994659424\n",
      "Eikonal loss: 0.15395265817642212\n",
      "Step 280:\n",
      "Loss: 1.0613093376159668\n",
      "Average steps: 5.924299716949463\n",
      "Eikonal loss: 0.1491096466779709\n",
      "Step 285:\n",
      "Loss: 1.0388820171356201\n",
      "Average steps: 5.9004998207092285\n",
      "Eikonal loss: 0.14460060000419617\n",
      "Step 290:\n",
      "Loss: 0.9904731512069702\n",
      "Average steps: 5.844799995422363\n",
      "Eikonal loss: 0.13532792031764984\n",
      "Step 295:\n",
      "Loss: 0.9893895387649536\n",
      "Average steps: 5.843100070953369\n",
      "Eikonal loss: 0.13578075170516968\n",
      "Step 300:\n",
      "Loss: 0.9346344470977783\n",
      "Average steps: 5.805099964141846\n",
      "Eikonal loss: 0.12511196732521057\n",
      "Step 305:\n",
      "Loss: 0.9161504507064819\n",
      "Average steps: 5.807799816131592\n",
      "Eikonal loss: 0.12187255918979645\n",
      "Step 310:\n",
      "Loss: 0.914164662361145\n",
      "Average steps: 5.78849983215332\n",
      "Eikonal loss: 0.12219953536987305\n",
      "Step 315:\n",
      "Loss: 0.8755844831466675\n",
      "Average steps: 5.78279972076416\n",
      "Eikonal loss: 0.11492297053337097\n",
      "Step 320:\n",
      "Loss: 0.8599875569343567\n",
      "Average steps: 5.76069974899292\n",
      "Eikonal loss: 0.11206257343292236\n",
      "Step 325:\n",
      "Loss: 0.8473694324493408\n",
      "Average steps: 5.784800052642822\n",
      "Eikonal loss: 0.11005059629678726\n",
      "Step 330:\n",
      "Loss: 0.8544597625732422\n",
      "Average steps: 5.830499649047852\n",
      "Eikonal loss: 0.11242809891700745\n",
      "Step 335:\n",
      "Loss: 0.8338066935539246\n",
      "Average steps: 5.787899971008301\n",
      "Eikonal loss: 0.10868772864341736\n",
      "Step 340:\n",
      "Loss: 0.8213930726051331\n",
      "Average steps: 5.784800052642822\n",
      "Eikonal loss: 0.10635202378034592\n",
      "Step 345:\n",
      "Loss: 0.8192621469497681\n",
      "Average steps: 5.797800064086914\n",
      "Eikonal loss: 0.10625356435775757\n",
      "Step 350:\n",
      "Loss: 0.8025233149528503\n",
      "Average steps: 5.746299743652344\n",
      "Eikonal loss: 0.10289982706308365\n",
      "Step 355:\n",
      "Loss: 0.8025382161140442\n",
      "Average steps: 5.7230000495910645\n",
      "Eikonal loss: 0.103115513920784\n",
      "Step 360:\n",
      "Loss: 0.8298516273498535\n",
      "Average steps: 5.729599952697754\n",
      "Eikonal loss: 0.10922130942344666\n",
      "Step 365:\n",
      "Loss: 0.8047171831130981\n",
      "Average steps: 5.746699810028076\n",
      "Eikonal loss: 0.10468322783708572\n",
      "Step 370:\n",
      "Loss: 0.7894350290298462\n",
      "Average steps: 5.749000072479248\n",
      "Eikonal loss: 0.10189873725175858\n",
      "Step 375:\n",
      "Loss: 0.7771068811416626\n",
      "Average steps: 5.741600036621094\n",
      "Eikonal loss: 0.09966521710157394\n",
      "Step 380:\n",
      "Loss: 0.7867509126663208\n",
      "Average steps: 5.741399765014648\n",
      "Eikonal loss: 0.10173159837722778\n",
      "Step 385:\n",
      "Loss: 0.763550877571106\n",
      "Average steps: 5.733299732208252\n",
      "Eikonal loss: 0.09708131104707718\n",
      "Step 390:\n",
      "Loss: 0.7737374305725098\n",
      "Average steps: 5.73799991607666\n",
      "Eikonal loss: 0.09873299300670624\n",
      "Step 395:\n",
      "Loss: 0.7664508819580078\n",
      "Average steps: 5.789299964904785\n",
      "Eikonal loss: 0.09780587255954742\n",
      "Step 400:\n",
      "Loss: 0.774962306022644\n",
      "Average steps: 5.815199851989746\n",
      "Eikonal loss: 0.09967432916164398\n",
      "Step 405:\n",
      "Loss: 0.764397144317627\n",
      "Average steps: 5.8028998374938965\n",
      "Eikonal loss: 0.09694577753543854\n",
      "Step 410:\n",
      "Loss: 0.7760030031204224\n",
      "Average steps: 5.778500080108643\n",
      "Eikonal loss: 0.09901230037212372\n",
      "Step 415:\n",
      "Loss: 0.7575459480285645\n",
      "Average steps: 5.7714996337890625\n",
      "Eikonal loss: 0.09550884366035461\n",
      "Step 420:\n",
      "Loss: 0.7449264526367188\n",
      "Average steps: 5.759199619293213\n",
      "Eikonal loss: 0.09458886831998825\n",
      "Step 425:\n",
      "Loss: 0.7286670207977295\n",
      "Average steps: 5.755499839782715\n",
      "Eikonal loss: 0.09301017224788666\n",
      "Step 430:\n",
      "Loss: 0.7496960163116455\n",
      "Average steps: 5.75629997253418\n",
      "Eikonal loss: 0.09810703247785568\n",
      "Step 435:\n",
      "Loss: 0.7279030084609985\n",
      "Average steps: 5.7592997550964355\n",
      "Eikonal loss: 0.09402238577604294\n",
      "Step 440:\n",
      "Loss: 0.7253966927528381\n",
      "Average steps: 5.829999923706055\n",
      "Eikonal loss: 0.09417296200990677\n",
      "Step 445:\n",
      "Loss: 0.753407895565033\n",
      "Average steps: 5.922599792480469\n",
      "Eikonal loss: 0.09970428049564362\n",
      "Step 450:\n",
      "Loss: 0.712002158164978\n",
      "Average steps: 5.832499980926514\n",
      "Eikonal loss: 0.09162724018096924\n",
      "Step 455:\n",
      "Loss: 0.7085682153701782\n",
      "Average steps: 5.791100025177002\n",
      "Eikonal loss: 0.09079494327306747\n",
      "Step 460:\n",
      "Loss: 0.7081867456436157\n",
      "Average steps: 5.807699680328369\n",
      "Eikonal loss: 0.09136798232793808\n",
      "Step 465:\n",
      "Loss: 0.6987471580505371\n",
      "Average steps: 5.844799995422363\n",
      "Eikonal loss: 0.09029648452997208\n",
      "Step 470:\n",
      "Loss: 0.7169910669326782\n",
      "Average steps: 5.830399990081787\n",
      "Eikonal loss: 0.09437547624111176\n",
      "Step 475:\n",
      "Loss: 0.6921342015266418\n",
      "Average steps: 5.896100044250488\n",
      "Eikonal loss: 0.08999619632959366\n",
      "Step 480:\n",
      "Loss: 0.6981126070022583\n",
      "Average steps: 5.883399963378906\n",
      "Eikonal loss: 0.09162754565477371\n",
      "Step 485:\n",
      "Loss: 0.6633320450782776\n",
      "Average steps: 5.855999946594238\n",
      "Eikonal loss: 0.08509532362222672\n",
      "Step 490:\n",
      "Loss: 0.696937084197998\n",
      "Average steps: 5.870100021362305\n",
      "Eikonal loss: 0.09225790947675705\n",
      "Step 495:\n",
      "Loss: 0.6569353938102722\n",
      "Average steps: 5.896199703216553\n",
      "Eikonal loss: 0.08435431867837906\n",
      "Step 500:\n",
      "Loss: 0.6556742191314697\n",
      "Average steps: 5.880199909210205\n",
      "Eikonal loss: 0.08425750583410263\n",
      "Step 505:\n",
      "Loss: 0.6808884143829346\n",
      "Average steps: 5.883699893951416\n",
      "Eikonal loss: 0.089372418820858\n",
      "Step 510:\n",
      "Loss: 0.6596053242683411\n",
      "Average steps: 5.913300037384033\n",
      "Eikonal loss: 0.0855194553732872\n",
      "Step 515:\n",
      "Loss: 0.6476109623908997\n",
      "Average steps: 5.9315996170043945\n",
      "Eikonal loss: 0.08353085815906525\n",
      "Step 520:\n",
      "Loss: 0.6510037183761597\n",
      "Average steps: 5.953000068664551\n",
      "Eikonal loss: 0.08440936356782913\n",
      "Step 525:\n",
      "Loss: 0.6581313610076904\n",
      "Average steps: 5.986799716949463\n",
      "Eikonal loss: 0.08579045534133911\n",
      "Step 530:\n",
      "Loss: 0.6456863284111023\n",
      "Average steps: 5.957900047302246\n",
      "Eikonal loss: 0.08367165178060532\n",
      "Step 535:\n",
      "Loss: 0.6478765606880188\n",
      "Average steps: 5.996399879455566\n",
      "Eikonal loss: 0.08392844349145889\n",
      "Step 540:\n",
      "Loss: 0.6607180833816528\n",
      "Average steps: 6.052599906921387\n",
      "Eikonal loss: 0.08614905923604965\n",
      "Step 545:\n",
      "Loss: 0.6360360980033875\n",
      "Average steps: 5.961400032043457\n",
      "Eikonal loss: 0.08060004562139511\n",
      "Step 550:\n",
      "Loss: 0.6366991996765137\n",
      "Average steps: 5.904599666595459\n",
      "Eikonal loss: 0.08063816279172897\n",
      "Step 555:\n",
      "Loss: 0.6246823072433472\n",
      "Average steps: 5.884599685668945\n",
      "Eikonal loss: 0.07883159071207047\n",
      "Step 560:\n",
      "Loss: 0.6528322100639343\n",
      "Average steps: 5.892999649047852\n",
      "Eikonal loss: 0.0849430114030838\n",
      "Step 565:\n",
      "Loss: 0.6378585696220398\n",
      "Average steps: 5.905399799346924\n",
      "Eikonal loss: 0.08246968686580658\n",
      "Step 570:\n",
      "Loss: 0.6541531682014465\n",
      "Average steps: 5.903899669647217\n",
      "Eikonal loss: 0.08619250357151031\n",
      "Step 575:\n",
      "Loss: 0.6221175789833069\n",
      "Average steps: 5.896699905395508\n",
      "Eikonal loss: 0.07972133904695511\n",
      "Step 580:\n",
      "Loss: 0.6343990564346313\n",
      "Average steps: 5.8690996170043945\n",
      "Eikonal loss: 0.08151154220104218\n",
      "Step 585:\n",
      "Loss: 0.6069519519805908\n",
      "Average steps: 5.841700077056885\n",
      "Eikonal loss: 0.07534711807966232\n",
      "Step 590:\n",
      "Loss: 0.6113901138305664\n",
      "Average steps: 5.856100082397461\n",
      "Eikonal loss: 0.07666240632534027\n",
      "Step 595:\n",
      "Loss: 0.6195530891418457\n",
      "Average steps: 5.8404998779296875\n",
      "Eikonal loss: 0.0786709114909172\n",
      "Step 600:\n",
      "Loss: 0.5995065569877625\n",
      "Average steps: 5.839099884033203\n",
      "Eikonal loss: 0.07507317513227463\n",
      "Step 605:\n",
      "Loss: 0.59272700548172\n",
      "Average steps: 5.826699733734131\n",
      "Eikonal loss: 0.073873370885849\n",
      "Step 610:\n",
      "Loss: 0.6125823259353638\n",
      "Average steps: 5.890100002288818\n",
      "Eikonal loss: 0.07856916636228561\n",
      "Step 615:\n",
      "Loss: 0.5953428745269775\n",
      "Average steps: 5.901899814605713\n",
      "Eikonal loss: 0.07564037293195724\n",
      "Step 620:\n",
      "Loss: 0.6120338439941406\n",
      "Average steps: 5.877199649810791\n",
      "Eikonal loss: 0.07891622185707092\n",
      "Step 625:\n",
      "Loss: 0.5936740636825562\n",
      "Average steps: 5.872099876403809\n",
      "Eikonal loss: 0.07506804168224335\n",
      "Step 630:\n",
      "Loss: 0.5854088068008423\n",
      "Average steps: 5.8526997566223145\n",
      "Eikonal loss: 0.07351155579090118\n",
      "Step 635:\n",
      "Loss: 0.5917919874191284\n",
      "Average steps: 5.84119987487793\n",
      "Eikonal loss: 0.07508577406406403\n",
      "Step 640:\n",
      "Loss: 0.5693295001983643\n",
      "Average steps: 5.843299865722656\n",
      "Eikonal loss: 0.07102564722299576\n",
      "Step 645:\n",
      "Loss: 0.5721919536590576\n",
      "Average steps: 5.847899913787842\n",
      "Eikonal loss: 0.07203236222267151\n",
      "Step 650:\n",
      "Loss: 0.5766257643699646\n",
      "Average steps: 5.838799953460693\n",
      "Eikonal loss: 0.07301302254199982\n",
      "Step 655:\n",
      "Loss: 0.5672268867492676\n",
      "Average steps: 5.836199760437012\n",
      "Eikonal loss: 0.07110903412103653\n",
      "Step 660:\n",
      "Loss: 0.586716890335083\n",
      "Average steps: 5.851999759674072\n",
      "Eikonal loss: 0.07528399676084518\n",
      "Step 665:\n",
      "Loss: 0.560667097568512\n",
      "Average steps: 5.804699897766113\n",
      "Eikonal loss: 0.06982268393039703\n",
      "Step 670:\n",
      "Loss: 0.5595025420188904\n",
      "Average steps: 5.809199810028076\n",
      "Eikonal loss: 0.06995148956775665\n",
      "Step 675:\n",
      "Loss: 0.5482015609741211\n",
      "Average steps: 5.861199855804443\n",
      "Eikonal loss: 0.06802186369895935\n",
      "Step 680:\n",
      "Loss: 0.5608653426170349\n",
      "Average steps: 5.846299648284912\n",
      "Eikonal loss: 0.07074793428182602\n",
      "Step 685:\n",
      "Loss: 0.5605493187904358\n",
      "Average steps: 5.813499927520752\n",
      "Eikonal loss: 0.070453941822052\n",
      "Step 690:\n",
      "Loss: 0.5362752079963684\n",
      "Average steps: 5.786799907684326\n",
      "Eikonal loss: 0.06593254208564758\n",
      "Step 695:\n",
      "Loss: 0.5476760864257812\n",
      "Average steps: 5.779399871826172\n",
      "Eikonal loss: 0.06882856041193008\n",
      "Step 700:\n",
      "Loss: 0.5461821556091309\n",
      "Average steps: 5.806699752807617\n",
      "Eikonal loss: 0.06900980323553085\n",
      "Step 705:\n",
      "Loss: 0.5882046818733215\n",
      "Average steps: 6.009599685668945\n",
      "Eikonal loss: 0.07803376019001007\n",
      "Step 710:\n",
      "Loss: 0.604859471321106\n",
      "Average steps: 6.125499725341797\n",
      "Eikonal loss: 0.08075090497732162\n",
      "Step 715:\n",
      "Loss: 0.5416586399078369\n",
      "Average steps: 5.92579984664917\n",
      "Eikonal loss: 0.06693008542060852\n",
      "Step 720:\n",
      "Loss: 0.5330194234848022\n",
      "Average steps: 5.87999963760376\n",
      "Eikonal loss: 0.06466028094291687\n",
      "Step 725:\n",
      "Loss: 0.5458531975746155\n",
      "Average steps: 5.849599838256836\n",
      "Eikonal loss: 0.06687978655099869\n",
      "Step 730:\n",
      "Loss: 0.5288740396499634\n",
      "Average steps: 5.832900047302246\n",
      "Eikonal loss: 0.0636204183101654\n",
      "Step 735:\n",
      "Loss: 0.5395517349243164\n",
      "Average steps: 5.835099697113037\n",
      "Eikonal loss: 0.06641068309545517\n",
      "Step 740:\n",
      "Loss: 0.5357715487480164\n",
      "Average steps: 5.878699779510498\n",
      "Eikonal loss: 0.06592357903718948\n",
      "Step 745:\n",
      "Loss: 0.533661961555481\n",
      "Average steps: 5.861999988555908\n",
      "Eikonal loss: 0.06557059288024902\n",
      "Step 750:\n",
      "Loss: 0.5135118961334229\n",
      "Average steps: 5.830599784851074\n",
      "Eikonal loss: 0.06152455136179924\n",
      "Step 755:\n",
      "Loss: 0.5159016251564026\n",
      "Average steps: 5.8364996910095215\n",
      "Eikonal loss: 0.062120385468006134\n",
      "Step 760:\n",
      "Loss: 0.5188046097755432\n",
      "Average steps: 5.8090996742248535\n",
      "Eikonal loss: 0.06300409883260727\n",
      "Step 765:\n",
      "Loss: 0.5166324377059937\n",
      "Average steps: 5.857100009918213\n",
      "Eikonal loss: 0.06301304697990417\n",
      "Step 770:\n",
      "Loss: 0.5097528696060181\n",
      "Average steps: 5.833499908447266\n",
      "Eikonal loss: 0.061578307300806046\n",
      "Step 775:\n",
      "Loss: 0.5129297971725464\n",
      "Average steps: 5.808700084686279\n",
      "Eikonal loss: 0.06223873794078827\n",
      "Step 780:\n",
      "Loss: 0.5106289386749268\n",
      "Average steps: 5.763899803161621\n",
      "Eikonal loss: 0.061479602009058\n",
      "Step 785:\n",
      "Loss: 0.5245086550712585\n",
      "Average steps: 5.743099689483643\n",
      "Eikonal loss: 0.0641043409705162\n",
      "Step 790:\n",
      "Loss: 0.5220171213150024\n",
      "Average steps: 5.720099925994873\n",
      "Eikonal loss: 0.0635673925280571\n",
      "Step 795:\n",
      "Loss: 0.5293781757354736\n",
      "Average steps: 5.751800060272217\n",
      "Eikonal loss: 0.0655636340379715\n",
      "Step 800:\n",
      "Loss: 0.5031126141548157\n",
      "Average steps: 5.807699680328369\n",
      "Eikonal loss: 0.06069539487361908\n",
      "Step 805:\n",
      "Loss: 0.5112435221672058\n",
      "Average steps: 5.870899677276611\n",
      "Eikonal loss: 0.06230965256690979\n",
      "Step 810:\n",
      "Loss: 0.4970291256904602\n",
      "Average steps: 5.8603997230529785\n",
      "Eikonal loss: 0.05895309895277023\n",
      "Step 815:\n",
      "Loss: 0.5104069709777832\n",
      "Average steps: 5.892499923706055\n",
      "Eikonal loss: 0.06163320690393448\n",
      "Step 820:\n",
      "Loss: 0.49447524547576904\n",
      "Average steps: 5.883599758148193\n",
      "Eikonal loss: 0.05863676592707634\n",
      "Step 825:\n",
      "Loss: 0.4879177510738373\n",
      "Average steps: 5.881799697875977\n",
      "Eikonal loss: 0.057595811784267426\n",
      "Step 830:\n",
      "Loss: 0.5039693117141724\n",
      "Average steps: 5.872200012207031\n",
      "Eikonal loss: 0.06101461127400398\n",
      "Step 835:\n",
      "Loss: 0.48511648178100586\n",
      "Average steps: 5.88730001449585\n",
      "Eikonal loss: 0.05756894499063492\n",
      "Step 840:\n",
      "Loss: 0.4913179278373718\n",
      "Average steps: 5.867099761962891\n",
      "Eikonal loss: 0.05882091075181961\n",
      "Step 845:\n",
      "Loss: 0.4811059832572937\n",
      "Average steps: 5.889800071716309\n",
      "Eikonal loss: 0.05708584934473038\n",
      "Step 850:\n",
      "Loss: 0.4842861294746399\n",
      "Average steps: 5.873199939727783\n",
      "Eikonal loss: 0.058105241507291794\n",
      "Step 855:\n",
      "Loss: 0.4833378791809082\n",
      "Average steps: 5.84559965133667\n",
      "Eikonal loss: 0.058272507041692734\n",
      "Step 860:\n",
      "Loss: 0.4768640398979187\n",
      "Average steps: 5.845900058746338\n",
      "Eikonal loss: 0.05679047852754593\n",
      "Step 865:\n",
      "Loss: 0.4735177159309387\n",
      "Average steps: 5.832399845123291\n",
      "Eikonal loss: 0.05611192435026169\n",
      "Step 870:\n",
      "Loss: 0.47048264741897583\n",
      "Average steps: 5.823999881744385\n",
      "Eikonal loss: 0.05550919100642204\n",
      "Step 875:\n",
      "Loss: 0.47017914056777954\n",
      "Average steps: 5.819900035858154\n",
      "Eikonal loss: 0.05560115724802017\n",
      "Step 880:\n",
      "Loss: 0.4684765040874481\n",
      "Average steps: 5.874399662017822\n",
      "Eikonal loss: 0.055432409048080444\n",
      "Step 885:\n",
      "Loss: 0.5910578370094299\n",
      "Average steps: 6.092299938201904\n",
      "Eikonal loss: 0.0802745372056961\n",
      "Step 890:\n",
      "Loss: 0.5954907536506653\n",
      "Average steps: 6.424299716949463\n",
      "Eikonal loss: 0.07907823473215103\n",
      "Step 895:\n",
      "Loss: 0.6274431943893433\n",
      "Average steps: 6.489500045776367\n",
      "Eikonal loss: 0.08420903980731964\n",
      "Step 900:\n",
      "Loss: 0.5799140334129333\n",
      "Average steps: 6.309700012207031\n",
      "Eikonal loss: 0.07512535899877548\n",
      "Step 905:\n",
      "Loss: 0.5765573978424072\n",
      "Average steps: 6.146299839019775\n",
      "Eikonal loss: 0.0739322081208229\n",
      "Step 910:\n",
      "Loss: 0.5450352430343628\n",
      "Average steps: 6.031199932098389\n",
      "Eikonal loss: 0.06786184757947922\n",
      "Step 915:\n",
      "Loss: 0.5213164687156677\n",
      "Average steps: 5.848499774932861\n",
      "Eikonal loss: 0.06300929933786392\n",
      "Step 920:\n",
      "Loss: 0.5229194164276123\n",
      "Average steps: 5.700699806213379\n",
      "Eikonal loss: 0.06304793804883957\n",
      "Step 925:\n",
      "Loss: 0.5277414321899414\n",
      "Average steps: 5.6230998039245605\n",
      "Eikonal loss: 0.06397323310375214\n",
      "Step 930:\n",
      "Loss: 0.5280492305755615\n",
      "Average steps: 5.587100028991699\n",
      "Eikonal loss: 0.06398018449544907\n",
      "Step 935:\n",
      "Loss: 0.5137619376182556\n",
      "Average steps: 5.605000019073486\n",
      "Eikonal loss: 0.06199036166071892\n",
      "Step 940:\n",
      "Loss: 0.5098347663879395\n",
      "Average steps: 5.630899906158447\n",
      "Eikonal loss: 0.061546690762043\n",
      "Step 945:\n",
      "Loss: 0.5045430064201355\n",
      "Average steps: 5.674299716949463\n",
      "Eikonal loss: 0.06064983829855919\n",
      "Step 950:\n",
      "Loss: 0.4840658903121948\n",
      "Average steps: 5.721799850463867\n",
      "Eikonal loss: 0.0571373775601387\n",
      "Step 955:\n",
      "Loss: 0.488678514957428\n",
      "Average steps: 5.744199752807617\n",
      "Eikonal loss: 0.05867154896259308\n",
      "Step 960:\n",
      "Loss: 0.481661856174469\n",
      "Average steps: 5.750199794769287\n",
      "Eikonal loss: 0.05787213146686554\n",
      "Step 965:\n",
      "Loss: 0.48191630840301514\n",
      "Average steps: 5.734600067138672\n",
      "Eikonal loss: 0.05836716666817665\n",
      "Step 970:\n",
      "Loss: 0.47268515825271606\n",
      "Average steps: 5.701299667358398\n",
      "Eikonal loss: 0.05697391927242279\n",
      "Step 975:\n",
      "Loss: 0.4924246668815613\n",
      "Average steps: 5.620199680328369\n",
      "Eikonal loss: 0.06087774410843849\n",
      "Step 980:\n",
      "Loss: 0.5141432881355286\n",
      "Average steps: 5.576999664306641\n",
      "Eikonal loss: 0.06517986208200455\n",
      "Step 985:\n",
      "Loss: 0.49789145588874817\n",
      "Average steps: 5.632299900054932\n",
      "Eikonal loss: 0.06302698701620102\n",
      "Step 990:\n",
      "Loss: 0.5100494027137756\n",
      "Average steps: 5.672800064086914\n",
      "Eikonal loss: 0.06586431711912155\n",
      "Step 995:\n",
      "Loss: 0.49347251653671265\n",
      "Average steps: 5.694799900054932\n",
      "Eikonal loss: 0.06317425519227982\n",
      "Step 1000:\n",
      "Loss: 0.4819321036338806\n",
      "Average steps: 5.720699787139893\n",
      "Eikonal loss: 0.0613735094666481\n",
      "Step 1005:\n",
      "Loss: 0.47265148162841797\n",
      "Average steps: 5.700999736785889\n",
      "Eikonal loss: 0.059528764337301254\n",
      "Step 1010:\n",
      "Loss: 0.4735366106033325\n",
      "Average steps: 5.70389986038208\n",
      "Eikonal loss: 0.05996030569076538\n",
      "Step 1015:\n",
      "Loss: 0.4610385596752167\n",
      "Average steps: 5.713900089263916\n",
      "Eikonal loss: 0.0574522502720356\n",
      "Step 1020:\n",
      "Loss: 0.45495617389678955\n",
      "Average steps: 5.750999927520752\n",
      "Eikonal loss: 0.05611811950802803\n",
      "Step 1025:\n",
      "Loss: 0.4508260488510132\n",
      "Average steps: 5.855099678039551\n",
      "Eikonal loss: 0.055295709520578384\n",
      "Step 1030:\n",
      "Loss: 0.4549281597137451\n",
      "Average steps: 5.864599704742432\n",
      "Eikonal loss: 0.05565398931503296\n",
      "Step 1035:\n",
      "Loss: 0.446297287940979\n",
      "Average steps: 5.897299766540527\n",
      "Eikonal loss: 0.05340300127863884\n",
      "Step 1040:\n",
      "Loss: 0.45323532819747925\n",
      "Average steps: 5.9029998779296875\n",
      "Eikonal loss: 0.054437827318906784\n",
      "Step 1045:\n",
      "Loss: 0.44339728355407715\n",
      "Average steps: 5.901199817657471\n",
      "Eikonal loss: 0.052255235612392426\n",
      "Step 1050:\n",
      "Loss: 0.4562916159629822\n",
      "Average steps: 5.924699783325195\n",
      "Eikonal loss: 0.05503244325518608\n",
      "Step 1055:\n",
      "Loss: 0.4512075185775757\n",
      "Average steps: 5.891499996185303\n",
      "Eikonal loss: 0.053909026086330414\n",
      "Step 1060:\n",
      "Loss: 0.44090092182159424\n",
      "Average steps: 5.9243998527526855\n",
      "Eikonal loss: 0.05241341516375542\n",
      "Step 1065:\n",
      "Loss: 0.45326554775238037\n",
      "Average steps: 5.949199676513672\n",
      "Eikonal loss: 0.055426452308893204\n",
      "Step 1070:\n",
      "Loss: 0.44848117232322693\n",
      "Average steps: 5.86299991607666\n",
      "Eikonal loss: 0.054280590265989304\n",
      "Step 1075:\n",
      "Loss: 0.44402480125427246\n",
      "Average steps: 5.798999786376953\n",
      "Eikonal loss: 0.05294700711965561\n",
      "Step 1080:\n",
      "Loss: 0.4275406002998352\n",
      "Average steps: 5.816400051116943\n",
      "Eikonal loss: 0.04992102086544037\n",
      "Step 1085:\n",
      "Loss: 0.42209288477897644\n",
      "Average steps: 5.82289981842041\n",
      "Eikonal loss: 0.048959147185087204\n",
      "Step 1090:\n",
      "Loss: 0.43575286865234375\n",
      "Average steps: 5.815999984741211\n",
      "Eikonal loss: 0.05121524631977081\n",
      "Step 1095:\n",
      "Loss: 0.4270193874835968\n",
      "Average steps: 5.814899921417236\n",
      "Eikonal loss: 0.049538787454366684\n",
      "Step 1100:\n",
      "Loss: 0.43179643154144287\n",
      "Average steps: 5.822299957275391\n",
      "Eikonal loss: 0.05095326155424118\n",
      "Step 1105:\n",
      "Loss: 0.4182497262954712\n",
      "Average steps: 5.770299911499023\n",
      "Eikonal loss: 0.04788544774055481\n",
      "Step 1110:\n",
      "Loss: 0.4278433620929718\n",
      "Average steps: 5.763499736785889\n",
      "Eikonal loss: 0.049927178770303726\n",
      "Step 1115:\n",
      "Loss: 0.42404812574386597\n",
      "Average steps: 5.7855000495910645\n",
      "Eikonal loss: 0.04950771480798721\n",
      "Step 1120:\n",
      "Loss: 0.4221154451370239\n",
      "Average steps: 5.792500019073486\n",
      "Eikonal loss: 0.04930500313639641\n",
      "Step 1125:\n",
      "Loss: 0.43242231011390686\n",
      "Average steps: 5.82450008392334\n",
      "Eikonal loss: 0.051278308033943176\n",
      "Step 1130:\n",
      "Loss: 0.4330873489379883\n",
      "Average steps: 5.826600074768066\n",
      "Eikonal loss: 0.051387567073106766\n",
      "Step 1135:\n",
      "Loss: 0.4489504098892212\n",
      "Average steps: 5.850399971008301\n",
      "Eikonal loss: 0.054533861577510834\n",
      "Step 1140:\n",
      "Loss: 0.43136560916900635\n",
      "Average steps: 5.850100040435791\n",
      "Eikonal loss: 0.05129330977797508\n",
      "Step 1145:\n",
      "Loss: 0.43198561668395996\n",
      "Average steps: 5.848700046539307\n",
      "Eikonal loss: 0.051381517201662064\n",
      "Step 1150:\n",
      "Loss: 0.4376426935195923\n",
      "Average steps: 5.854300022125244\n",
      "Eikonal loss: 0.05267999693751335\n",
      "Step 1155:\n",
      "Loss: 0.4387592673301697\n",
      "Average steps: 5.866199970245361\n",
      "Eikonal loss: 0.0529853031039238\n",
      "Step 1160:\n",
      "Loss: 0.44197767972946167\n",
      "Average steps: 5.867199897766113\n",
      "Eikonal loss: 0.05382087826728821\n",
      "Step 1165:\n",
      "Loss: 0.4314435124397278\n",
      "Average steps: 5.820399761199951\n",
      "Eikonal loss: 0.051642123609781265\n",
      "Step 1170:\n",
      "Loss: 0.4571690261363983\n",
      "Average steps: 5.899799823760986\n",
      "Eikonal loss: 0.05761518329381943\n",
      "Step 1175:\n",
      "Loss: 0.5267976522445679\n",
      "Average steps: 5.94789981842041\n",
      "Eikonal loss: 0.07164522260427475\n",
      "Step 1180:\n",
      "Loss: 0.49832481145858765\n",
      "Average steps: 6.061800003051758\n",
      "Eikonal loss: 0.06570401787757874\n",
      "Step 1185:\n",
      "Loss: 0.4549992084503174\n",
      "Average steps: 6.051699638366699\n",
      "Eikonal loss: 0.05654236301779747\n",
      "Step 1190:\n",
      "Loss: 0.45932871103286743\n",
      "Average steps: 5.985599994659424\n",
      "Eikonal loss: 0.05656531825661659\n",
      "Step 1195:\n",
      "Loss: 0.4619181156158447\n",
      "Average steps: 5.870999813079834\n",
      "Eikonal loss: 0.05641661211848259\n",
      "Step 1200:\n",
      "Loss: 0.4672071039676666\n",
      "Average steps: 5.795599937438965\n",
      "Eikonal loss: 0.05718063935637474\n",
      "Step 1205:\n",
      "Loss: 0.45367518067359924\n",
      "Average steps: 5.735099792480469\n",
      "Eikonal loss: 0.05439075827598572\n",
      "Step 1210:\n",
      "Loss: 0.4368745684623718\n",
      "Average steps: 5.731499671936035\n",
      "Eikonal loss: 0.05103832855820656\n",
      "Step 1215:\n",
      "Loss: 0.4468930959701538\n",
      "Average steps: 5.7565999031066895\n",
      "Eikonal loss: 0.053290776908397675\n",
      "Step 1220:\n",
      "Loss: 0.44378575682640076\n",
      "Average steps: 5.842700004577637\n",
      "Eikonal loss: 0.05352942273020744\n",
      "Step 1225:\n",
      "Loss: 0.47026973962783813\n",
      "Average steps: 5.950999736785889\n",
      "Eikonal loss: 0.059538546949625015\n",
      "Step 1230:\n",
      "Loss: 0.4393087327480316\n",
      "Average steps: 5.889599800109863\n",
      "Eikonal loss: 0.05333290994167328\n",
      "Step 1235:\n",
      "Loss: 0.4356880784034729\n",
      "Average steps: 5.8394999504089355\n",
      "Eikonal loss: 0.052751969546079636\n",
      "Step 1240:\n",
      "Loss: 0.44287365674972534\n",
      "Average steps: 5.855499744415283\n",
      "Eikonal loss: 0.05481841787695885\n",
      "Step 1245:\n",
      "Loss: 0.4553593397140503\n",
      "Average steps: 5.850499629974365\n",
      "Eikonal loss: 0.05744888633489609\n",
      "Step 1250:\n",
      "Loss: 0.4522690176963806\n",
      "Average steps: 5.925899982452393\n",
      "Eikonal loss: 0.05695771425962448\n",
      "Step 1255:\n",
      "Loss: 0.4465709924697876\n",
      "Average steps: 5.899999618530273\n",
      "Eikonal loss: 0.05523663014173508\n",
      "Step 1260:\n",
      "Loss: 0.4495221972465515\n",
      "Average steps: 5.815299987792969\n",
      "Eikonal loss: 0.055041782557964325\n",
      "Step 1265:\n",
      "Loss: 0.42998844385147095\n",
      "Average steps: 5.788300037384033\n",
      "Eikonal loss: 0.05085306242108345\n",
      "Step 1270:\n",
      "Loss: 0.4423472285270691\n",
      "Average steps: 5.812299728393555\n",
      "Eikonal loss: 0.05343371629714966\n",
      "Step 1275:\n",
      "Loss: 0.44665229320526123\n",
      "Average steps: 5.799599647521973\n",
      "Eikonal loss: 0.05416848510503769\n",
      "Step 1280:\n",
      "Loss: 0.4453679919242859\n",
      "Average steps: 5.809199810028076\n",
      "Eikonal loss: 0.05398348346352577\n",
      "Step 1285:\n",
      "Loss: 0.4386821389198303\n",
      "Average steps: 5.862599849700928\n",
      "Eikonal loss: 0.05339905992150307\n",
      "Step 1290:\n",
      "Loss: 0.43957051634788513\n",
      "Average steps: 5.8703999519348145\n",
      "Eikonal loss: 0.05381696671247482\n",
      "Step 1295:\n",
      "Loss: 0.44504407048225403\n",
      "Average steps: 5.855099678039551\n",
      "Eikonal loss: 0.05508676543831825\n",
      "Step 1300:\n",
      "Loss: 0.44224298000335693\n",
      "Average steps: 5.857199668884277\n",
      "Eikonal loss: 0.05492354556918144\n",
      "Step 1305:\n",
      "Loss: 0.43311822414398193\n",
      "Average steps: 5.836799621582031\n",
      "Eikonal loss: 0.05355748161673546\n",
      "Step 1310:\n",
      "Loss: 0.44273653626441956\n",
      "Average steps: 5.911799907684326\n",
      "Eikonal loss: 0.0562683567404747\n",
      "Step 1315:\n",
      "Loss: 0.4313089847564697\n",
      "Average steps: 5.869199752807617\n",
      "Eikonal loss: 0.053786251693964005\n",
      "Step 1320:\n",
      "Loss: 0.4467775225639343\n",
      "Average steps: 5.799900054931641\n",
      "Eikonal loss: 0.05568639934062958\n",
      "Step 1325:\n",
      "Loss: 0.4503851532936096\n",
      "Average steps: 5.708699703216553\n",
      "Eikonal loss: 0.05529408901929855\n",
      "Step 1330:\n",
      "Loss: 0.4371102750301361\n",
      "Average steps: 5.6956000328063965\n",
      "Eikonal loss: 0.05254659429192543\n",
      "Step 1335:\n",
      "Loss: 0.4487863779067993\n",
      "Average steps: 5.7403998374938965\n",
      "Eikonal loss: 0.05527458339929581\n",
      "Step 1340:\n",
      "Loss: 0.4513581395149231\n",
      "Average steps: 5.813600063323975\n",
      "Eikonal loss: 0.056449804455041885\n",
      "Step 1345:\n",
      "Loss: 0.448135644197464\n",
      "Average steps: 5.834199905395508\n",
      "Eikonal loss: 0.05611803010106087\n",
      "Step 1350:\n",
      "Loss: 0.43553656339645386\n",
      "Average steps: 5.876699924468994\n",
      "Eikonal loss: 0.05415479466319084\n",
      "Step 1355:\n",
      "Loss: 0.43000102043151855\n",
      "Average steps: 5.884500026702881\n",
      "Eikonal loss: 0.05312305688858032\n",
      "Step 1360:\n",
      "Loss: 0.45054641366004944\n",
      "Average steps: 5.8471999168396\n",
      "Eikonal loss: 0.05725644528865814\n",
      "Step 1365:\n",
      "Loss: 0.4402207136154175\n",
      "Average steps: 5.852999687194824\n",
      "Eikonal loss: 0.05530272051692009\n",
      "Step 1370:\n",
      "Loss: 0.44574791193008423\n",
      "Average steps: 5.8653998374938965\n",
      "Eikonal loss: 0.05654507875442505\n",
      "Step 1375:\n",
      "Loss: 0.4259290099143982\n",
      "Average steps: 5.856100082397461\n",
      "Eikonal loss: 0.052665695548057556\n",
      "Step 1380:\n",
      "Loss: 0.4418279528617859\n",
      "Average steps: 5.843499660491943\n",
      "Eikonal loss: 0.05598761513829231\n",
      "Step 1385:\n",
      "Loss: 0.44096505641937256\n",
      "Average steps: 5.823899745941162\n",
      "Eikonal loss: 0.05564171075820923\n",
      "Step 1390:\n",
      "Loss: 0.44346559047698975\n",
      "Average steps: 5.8013997077941895\n",
      "Eikonal loss: 0.05567733570933342\n",
      "Step 1395:\n",
      "Loss: 0.4385055899620056\n",
      "Average steps: 5.8094000816345215\n",
      "Eikonal loss: 0.05439496785402298\n",
      "Step 1400:\n",
      "Loss: 0.4264916479587555\n",
      "Average steps: 5.8225998878479\n",
      "Eikonal loss: 0.05207597091794014\n",
      "Step 1405:\n",
      "Loss: 0.4381420314311981\n",
      "Average steps: 5.876299858093262\n",
      "Eikonal loss: 0.05477625131607056\n",
      "Step 1410:\n",
      "Loss: 0.4300202429294586\n",
      "Average steps: 5.896699905395508\n",
      "Eikonal loss: 0.053283415734767914\n",
      "Step 1415:\n",
      "Loss: 0.4191209077835083\n",
      "Average steps: 5.897299766540527\n",
      "Eikonal loss: 0.051026053726673126\n",
      "Step 1420:\n",
      "Loss: 0.4347173273563385\n",
      "Average steps: 5.876999855041504\n",
      "Eikonal loss: 0.053681664168834686\n",
      "Step 1425:\n",
      "Loss: 0.4400692880153656\n",
      "Average steps: 5.845200061798096\n",
      "Eikonal loss: 0.054409146308898926\n",
      "Step 1430:\n",
      "Loss: 0.42611974477767944\n",
      "Average steps: 5.8294997215271\n",
      "Eikonal loss: 0.052243370562791824\n",
      "Step 1435:\n",
      "Loss: 0.4318844676017761\n",
      "Average steps: 5.810100078582764\n",
      "Eikonal loss: 0.05396171659231186\n",
      "Step 1440:\n",
      "Loss: 0.41172558069229126\n",
      "Average steps: 5.775099754333496\n",
      "Eikonal loss: 0.04982632398605347\n",
      "Step 1445:\n",
      "Loss: 0.4146915078163147\n",
      "Average steps: 5.774199962615967\n",
      "Eikonal loss: 0.050520580261945724\n",
      "Step 1450:\n",
      "Loss: 0.4130995571613312\n",
      "Average steps: 5.775399684906006\n",
      "Eikonal loss: 0.050320304930210114\n",
      "Step 1455:\n",
      "Loss: 0.4141290783882141\n",
      "Average steps: 5.745100021362305\n",
      "Eikonal loss: 0.050147153437137604\n",
      "Step 1460:\n",
      "Loss: 0.41044652462005615\n",
      "Average steps: 5.791999816894531\n",
      "Eikonal loss: 0.05015992373228073\n",
      "Step 1465:\n",
      "Loss: 0.4170725345611572\n",
      "Average steps: 5.79640007019043\n",
      "Eikonal loss: 0.051759716123342514\n",
      "Step 1470:\n",
      "Loss: 0.4192672669887543\n",
      "Average steps: 5.798399925231934\n",
      "Eikonal loss: 0.052357420325279236\n",
      "Step 1475:\n",
      "Loss: 0.4290672838687897\n",
      "Average steps: 5.791800022125244\n",
      "Eikonal loss: 0.05402692034840584\n",
      "Step 1480:\n",
      "Loss: 0.42467784881591797\n",
      "Average steps: 5.799999713897705\n",
      "Eikonal loss: 0.053214170038700104\n",
      "Step 1485:\n",
      "Loss: 0.4241064786911011\n",
      "Average steps: 5.796199798583984\n",
      "Eikonal loss: 0.05307377874851227\n",
      "Step 1490:\n",
      "Loss: 0.43051061034202576\n",
      "Average steps: 5.691499710083008\n",
      "Eikonal loss: 0.053652793169021606\n",
      "Step 1495:\n",
      "Loss: 0.4198762774467468\n",
      "Average steps: 5.628299713134766\n",
      "Eikonal loss: 0.05137792229652405\n",
      "Step 1500:\n",
      "Loss: 0.4258629381656647\n",
      "Average steps: 5.652899742126465\n",
      "Eikonal loss: 0.053351402282714844\n",
      "Step 1505:\n",
      "Loss: 0.420606791973114\n",
      "Average steps: 5.648499965667725\n",
      "Eikonal loss: 0.05254235863685608\n",
      "Step 1510:\n",
      "Loss: 0.41261062026023865\n",
      "Average steps: 5.6651997566223145\n",
      "Eikonal loss: 0.05104214325547218\n",
      "Step 1515:\n",
      "Loss: 0.4082566499710083\n",
      "Average steps: 5.738800048828125\n",
      "Eikonal loss: 0.050504229962825775\n",
      "Step 1520:\n",
      "Loss: 0.4294259250164032\n",
      "Average steps: 5.7769999504089355\n",
      "Eikonal loss: 0.054742224514484406\n",
      "Step 1525:\n",
      "Loss: 0.4092519283294678\n",
      "Average steps: 5.793699741363525\n",
      "Eikonal loss: 0.050227660685777664\n",
      "Step 1530:\n",
      "Loss: 0.4168265163898468\n",
      "Average steps: 5.792900085449219\n",
      "Eikonal loss: 0.05148880556225777\n",
      "Step 1535:\n",
      "Loss: 0.4099379777908325\n",
      "Average steps: 5.809000015258789\n",
      "Eikonal loss: 0.05014219507575035\n",
      "Step 1540:\n",
      "Loss: 0.41640669107437134\n",
      "Average steps: 5.832900047302246\n",
      "Eikonal loss: 0.051391929388046265\n",
      "Step 1545:\n",
      "Loss: 0.41711553931236267\n",
      "Average steps: 5.820799827575684\n",
      "Eikonal loss: 0.05143146961927414\n",
      "Step 1550:\n",
      "Loss: 0.4213973879814148\n",
      "Average steps: 5.86329984664917\n",
      "Eikonal loss: 0.052748143672943115\n",
      "Step 1555:\n",
      "Loss: 0.41084468364715576\n",
      "Average steps: 5.825999736785889\n",
      "Eikonal loss: 0.050286464393138885\n",
      "Step 1560:\n",
      "Loss: 0.4160612225532532\n",
      "Average steps: 5.850299835205078\n",
      "Eikonal loss: 0.051712993532419205\n",
      "Step 1565:\n",
      "Loss: 0.43277508020401\n",
      "Average steps: 5.863800048828125\n",
      "Eikonal loss: 0.055362217128276825\n",
      "Step 1570:\n",
      "Loss: 0.4068058431148529\n",
      "Average steps: 5.835299968719482\n",
      "Eikonal loss: 0.050094909965991974\n",
      "Step 1575:\n",
      "Loss: 0.414980947971344\n",
      "Average steps: 5.855899810791016\n",
      "Eikonal loss: 0.05187437683343887\n",
      "Step 1580:\n",
      "Loss: 0.4038429260253906\n",
      "Average steps: 5.799599647521973\n",
      "Eikonal loss: 0.04921935498714447\n",
      "Step 1585:\n",
      "Loss: 0.41264617443084717\n",
      "Average steps: 5.766900062561035\n",
      "Eikonal loss: 0.05080283805727959\n",
      "Step 1590:\n",
      "Loss: 0.4074198305606842\n",
      "Average steps: 5.8003997802734375\n",
      "Eikonal loss: 0.04997478052973747\n",
      "Step 1595:\n",
      "Loss: 0.4016216993331909\n",
      "Average steps: 5.787499904632568\n",
      "Eikonal loss: 0.04855755716562271\n",
      "Step 1600:\n",
      "Loss: 0.39479923248291016\n",
      "Average steps: 5.810499668121338\n",
      "Eikonal loss: 0.047206711024045944\n",
      "Step 1605:\n",
      "Loss: 0.3949519395828247\n",
      "Average steps: 5.846299648284912\n",
      "Eikonal loss: 0.04781268909573555\n",
      "Step 1610:\n",
      "Loss: 0.3722541332244873\n",
      "Average steps: 5.826200008392334\n",
      "Eikonal loss: 0.04288862645626068\n",
      "Step 1615:\n",
      "Loss: 0.3917672038078308\n",
      "Average steps: 5.763899803161621\n",
      "Eikonal loss: 0.04635671153664589\n",
      "Step 1620:\n",
      "Loss: 0.39827626943588257\n",
      "Average steps: 5.790099620819092\n",
      "Eikonal loss: 0.048185624182224274\n",
      "Step 1625:\n",
      "Loss: 0.40613484382629395\n",
      "Average steps: 5.799499988555908\n",
      "Eikonal loss: 0.050004299730062485\n",
      "Step 1630:\n",
      "Loss: 0.39688920974731445\n",
      "Average steps: 5.7779998779296875\n",
      "Eikonal loss: 0.048328399658203125\n",
      "Step 1635:\n",
      "Loss: 0.3998830318450928\n",
      "Average steps: 5.768699645996094\n",
      "Eikonal loss: 0.04894328489899635\n",
      "Step 1640:\n",
      "Loss: 0.40568000078201294\n",
      "Average steps: 5.740799903869629\n",
      "Eikonal loss: 0.04993212968111038\n",
      "Step 1645:\n",
      "Loss: 0.3928448557853699\n",
      "Average steps: 5.737400054931641\n",
      "Eikonal loss: 0.047573406249284744\n",
      "Step 1650:\n",
      "Loss: 0.40372174978256226\n",
      "Average steps: 5.775300025939941\n",
      "Eikonal loss: 0.05028897523880005\n",
      "Step 1655:\n",
      "Loss: 0.4134633243083954\n",
      "Average steps: 5.779999732971191\n",
      "Eikonal loss: 0.05238775536417961\n",
      "Step 1660:\n",
      "Loss: 0.3966740369796753\n",
      "Average steps: 5.802399635314941\n",
      "Eikonal loss: 0.049138884991407394\n",
      "Step 1665:\n",
      "Loss: 0.41281548142433167\n",
      "Average steps: 5.787499904632568\n",
      "Eikonal loss: 0.05250919610261917\n",
      "Step 1670:\n",
      "Loss: 0.40394312143325806\n",
      "Average steps: 5.776700019836426\n",
      "Eikonal loss: 0.050844594836235046\n",
      "Step 1675:\n",
      "Loss: 0.3943030834197998\n",
      "Average steps: 5.72599983215332\n",
      "Eikonal loss: 0.04832783713936806\n",
      "Step 1680:\n",
      "Loss: 0.37964823842048645\n",
      "Average steps: 5.718699932098389\n",
      "Eikonal loss: 0.04530273377895355\n",
      "Step 1685:\n",
      "Loss: 0.3903976082801819\n",
      "Average steps: 5.733999729156494\n",
      "Eikonal loss: 0.04769010469317436\n",
      "Step 1690:\n",
      "Loss: 0.38032805919647217\n",
      "Average steps: 5.751499652862549\n",
      "Eikonal loss: 0.046099357306957245\n",
      "Step 1695:\n",
      "Loss: 0.3916352689266205\n",
      "Average steps: 5.737400054931641\n",
      "Eikonal loss: 0.04825633764266968\n",
      "Step 1700:\n",
      "Loss: 0.38030242919921875\n",
      "Average steps: 5.7778000831604\n",
      "Eikonal loss: 0.046064481139183044\n",
      "Step 1705:\n",
      "Loss: 0.3816283345222473\n",
      "Average steps: 5.767699718475342\n",
      "Eikonal loss: 0.04610850289463997\n",
      "Step 1710:\n",
      "Loss: 0.38648903369903564\n",
      "Average steps: 5.762399673461914\n",
      "Eikonal loss: 0.04678650572896004\n",
      "Step 1715:\n",
      "Loss: 0.36859777569770813\n",
      "Average steps: 5.811500072479248\n",
      "Eikonal loss: 0.043788060545921326\n",
      "Step 1720:\n",
      "Loss: 0.3761367201805115\n",
      "Average steps: 5.8094000816345215\n",
      "Eikonal loss: 0.04503951221704483\n",
      "Step 1725:\n",
      "Loss: 0.37553054094314575\n",
      "Average steps: 5.833699703216553\n",
      "Eikonal loss: 0.044710736721754074\n",
      "Step 1730:\n",
      "Loss: 0.37424278259277344\n",
      "Average steps: 5.842199802398682\n",
      "Eikonal loss: 0.044779591262340546\n",
      "Step 1735:\n",
      "Loss: 0.36400115489959717\n",
      "Average steps: 5.815799713134766\n",
      "Eikonal loss: 0.04262302815914154\n",
      "Step 1740:\n",
      "Loss: 0.37044286727905273\n",
      "Average steps: 5.808000087738037\n",
      "Eikonal loss: 0.04415012151002884\n",
      "Step 1745:\n",
      "Loss: 0.36052000522613525\n",
      "Average steps: 5.782700061798096\n",
      "Eikonal loss: 0.04200582578778267\n",
      "Step 1750:\n",
      "Loss: 0.3642057776451111\n",
      "Average steps: 5.821099758148193\n",
      "Eikonal loss: 0.04299720749258995\n",
      "Step 1755:\n",
      "Loss: 0.3727465867996216\n",
      "Average steps: 5.843800067901611\n",
      "Eikonal loss: 0.044577352702617645\n",
      "Step 1760:\n",
      "Loss: 0.37229418754577637\n",
      "Average steps: 5.834699630737305\n",
      "Eikonal loss: 0.044166069477796555\n",
      "Step 1765:\n",
      "Loss: 0.36337095499038696\n",
      "Average steps: 5.79509973526001\n",
      "Eikonal loss: 0.042082857340574265\n",
      "Step 1770:\n",
      "Loss: 0.35825929045677185\n",
      "Average steps: 5.807399749755859\n",
      "Eikonal loss: 0.041194915771484375\n",
      "Step 1775:\n",
      "Loss: 0.3631353974342346\n",
      "Average steps: 5.843499660491943\n",
      "Eikonal loss: 0.04205834120512009\n",
      "Step 1780:\n",
      "Loss: 0.3678962290287018\n",
      "Average steps: 5.8317999839782715\n",
      "Eikonal loss: 0.04251280799508095\n",
      "Step 1785:\n",
      "Loss: 0.3618525266647339\n",
      "Average steps: 5.81719970703125\n",
      "Eikonal loss: 0.04147551953792572\n",
      "Step 1790:\n",
      "Loss: 0.36530137062072754\n",
      "Average steps: 5.757999897003174\n",
      "Eikonal loss: 0.04212893545627594\n",
      "Step 1795:\n",
      "Loss: 0.35639289021492004\n",
      "Average steps: 5.777499675750732\n",
      "Eikonal loss: 0.04107832908630371\n",
      "Step 1800:\n",
      "Loss: 0.3513471186161041\n",
      "Average steps: 5.749799728393555\n",
      "Eikonal loss: 0.03996531292796135\n",
      "Step 1805:\n",
      "Loss: 0.3615771234035492\n",
      "Average steps: 5.723199844360352\n",
      "Eikonal loss: 0.04182594642043114\n",
      "Step 1810:\n",
      "Loss: 0.36209917068481445\n",
      "Average steps: 5.741799831390381\n",
      "Eikonal loss: 0.04207427427172661\n",
      "Step 1815:\n",
      "Loss: 0.358747661113739\n",
      "Average steps: 5.766599655151367\n",
      "Eikonal loss: 0.04166711866855621\n",
      "Step 1820:\n",
      "Loss: 0.36323538422584534\n",
      "Average steps: 5.774199962615967\n",
      "Eikonal loss: 0.04266785457730293\n",
      "Step 1825:\n",
      "Loss: 0.36321377754211426\n",
      "Average steps: 5.779599666595459\n",
      "Eikonal loss: 0.04294208809733391\n",
      "Step 1830:\n",
      "Loss: 0.3604090213775635\n",
      "Average steps: 5.7754998207092285\n",
      "Eikonal loss: 0.04297420009970665\n",
      "Step 1835:\n",
      "Loss: 0.3571655750274658\n",
      "Average steps: 5.740999698638916\n",
      "Eikonal loss: 0.04233976826071739\n",
      "Step 1840:\n",
      "Loss: 0.3659120798110962\n",
      "Average steps: 5.743299961090088\n",
      "Eikonal loss: 0.04421960189938545\n",
      "Step 1845:\n",
      "Loss: 0.3712989389896393\n",
      "Average steps: 5.7683000564575195\n",
      "Eikonal loss: 0.045748673379421234\n",
      "Step 1850:\n",
      "Loss: 0.3693397045135498\n",
      "Average steps: 5.798599720001221\n",
      "Eikonal loss: 0.045465096831321716\n",
      "Step 1855:\n",
      "Loss: 0.3645521104335785\n",
      "Average steps: 5.757699966430664\n",
      "Eikonal loss: 0.04422381520271301\n",
      "Step 1860:\n",
      "Loss: 0.3525474965572357\n",
      "Average steps: 5.769399642944336\n",
      "Eikonal loss: 0.041512396186590195\n",
      "Step 1865:\n",
      "Loss: 0.35017451643943787\n",
      "Average steps: 5.807699680328369\n",
      "Eikonal loss: 0.04123548045754433\n",
      "Step 1870:\n",
      "Loss: 0.35787925124168396\n",
      "Average steps: 5.81119966506958\n",
      "Eikonal loss: 0.0426701121032238\n",
      "Step 1875:\n",
      "Loss: 0.3585701286792755\n",
      "Average steps: 5.825099945068359\n",
      "Eikonal loss: 0.042750097811222076\n",
      "Step 1880:\n",
      "Loss: 0.35753417015075684\n",
      "Average steps: 5.762099742889404\n",
      "Eikonal loss: 0.0420883409678936\n",
      "Step 1885:\n",
      "Loss: 0.35232222080230713\n",
      "Average steps: 5.809499740600586\n",
      "Eikonal loss: 0.04189584031701088\n",
      "Step 1890:\n",
      "Loss: 0.36139512062072754\n",
      "Average steps: 5.7403998374938965\n",
      "Eikonal loss: 0.043196048587560654\n",
      "Step 1895:\n",
      "Loss: 0.3616057336330414\n",
      "Average steps: 5.734499931335449\n",
      "Eikonal loss: 0.04307779669761658\n",
      "Step 1900:\n",
      "Loss: 0.3575773239135742\n",
      "Average steps: 5.724999904632568\n",
      "Eikonal loss: 0.04240547865629196\n",
      "Step 1905:\n",
      "Loss: 0.3422567546367645\n",
      "Average steps: 5.713799953460693\n",
      "Eikonal loss: 0.03903581202030182\n",
      "Step 1910:\n",
      "Loss: 0.3522711992263794\n",
      "Average steps: 5.7683000564575195\n",
      "Eikonal loss: 0.04195034131407738\n",
      "Step 1915:\n",
      "Loss: 0.34914258122444153\n",
      "Average steps: 5.776199817657471\n",
      "Eikonal loss: 0.04132992401719093\n",
      "Step 1920:\n",
      "Loss: 0.3631206452846527\n",
      "Average steps: 5.815399646759033\n",
      "Eikonal loss: 0.04425464943051338\n",
      "Step 1925:\n",
      "Loss: 0.34509968757629395\n",
      "Average steps: 5.779999732971191\n",
      "Eikonal loss: 0.04032771289348602\n",
      "Step 1930:\n",
      "Loss: 0.3384091556072235\n",
      "Average steps: 5.812299728393555\n",
      "Eikonal loss: 0.039409760385751724\n",
      "Step 1935:\n",
      "Loss: 0.3521425426006317\n",
      "Average steps: 5.823999881744385\n",
      "Eikonal loss: 0.04223935306072235\n",
      "Step 1940:\n",
      "Loss: 0.3498573303222656\n",
      "Average steps: 5.838299751281738\n",
      "Eikonal loss: 0.04167231172323227\n",
      "Step 1945:\n",
      "Loss: 0.3533439636230469\n",
      "Average steps: 5.817800045013428\n",
      "Eikonal loss: 0.042262762784957886\n",
      "Step 1950:\n",
      "Loss: 0.346452534198761\n",
      "Average steps: 5.8028998374938965\n",
      "Eikonal loss: 0.04069104045629501\n",
      "Step 1955:\n",
      "Loss: 0.3462073802947998\n",
      "Average steps: 5.8206000328063965\n",
      "Eikonal loss: 0.0409633070230484\n",
      "Step 1960:\n",
      "Loss: 0.3490084707736969\n",
      "Average steps: 5.769700050354004\n",
      "Eikonal loss: 0.04115229845046997\n",
      "Step 1965:\n",
      "Loss: 0.3577370047569275\n",
      "Average steps: 5.822999954223633\n",
      "Eikonal loss: 0.043288420885801315\n",
      "Step 1970:\n",
      "Loss: 0.34392276406288147\n",
      "Average steps: 5.808499813079834\n",
      "Eikonal loss: 0.040442608296871185\n",
      "Step 1975:\n",
      "Loss: 0.34548020362854004\n",
      "Average steps: 5.8231000900268555\n",
      "Eikonal loss: 0.04081111401319504\n",
      "Step 1980:\n",
      "Loss: 0.3438032865524292\n",
      "Average steps: 5.777100086212158\n",
      "Eikonal loss: 0.04019483923912048\n",
      "Step 1985:\n",
      "Loss: 0.34378665685653687\n",
      "Average steps: 5.753599643707275\n",
      "Eikonal loss: 0.0402819886803627\n",
      "Step 1990:\n",
      "Loss: 0.3430943489074707\n",
      "Average steps: 5.765899658203125\n",
      "Eikonal loss: 0.040206123143434525\n",
      "Step 1995:\n",
      "Loss: 0.34459108114242554\n",
      "Average steps: 5.786299705505371\n",
      "Eikonal loss: 0.040465500205755234\n",
      "Step 2000:\n",
      "Loss: 0.34419670701026917\n",
      "Average steps: 5.755300045013428\n",
      "Eikonal loss: 0.040419526398181915\n",
      "Step 2005:\n",
      "Loss: 0.32988983392715454\n",
      "Average steps: 5.753599643707275\n",
      "Eikonal loss: 0.03758850693702698\n",
      "Step 2010:\n",
      "Loss: 0.34902840852737427\n",
      "Average steps: 5.706399917602539\n",
      "Eikonal loss: 0.04098884388804436\n",
      "Step 2015:\n",
      "Loss: 0.3503583073616028\n",
      "Average steps: 5.787199974060059\n",
      "Eikonal loss: 0.042068809270858765\n",
      "Step 2020:\n",
      "Loss: 0.3605884909629822\n",
      "Average steps: 5.802299976348877\n",
      "Eikonal loss: 0.04428558051586151\n",
      "Step 2025:\n",
      "Loss: 0.3489522933959961\n",
      "Average steps: 5.757299900054932\n",
      "Eikonal loss: 0.0415923073887825\n",
      "Step 2030:\n",
      "Loss: 0.3457421660423279\n",
      "Average steps: 5.734099864959717\n",
      "Eikonal loss: 0.04102405533194542\n",
      "Step 2035:\n",
      "Loss: 0.35180628299713135\n",
      "Average steps: 5.7627997398376465\n",
      "Eikonal loss: 0.04255906119942665\n",
      "Step 2040:\n",
      "Loss: 0.3451825976371765\n",
      "Average steps: 5.783499717712402\n",
      "Eikonal loss: 0.04141624644398689\n",
      "Step 2045:\n",
      "Loss: 0.3283788561820984\n",
      "Average steps: 5.793900012969971\n",
      "Eikonal loss: 0.03787007927894592\n",
      "Step 2050:\n",
      "Loss: 0.34540635347366333\n",
      "Average steps: 5.7916998863220215\n",
      "Eikonal loss: 0.04111906513571739\n",
      "Step 2055:\n",
      "Loss: 0.3448635935783386\n",
      "Average steps: 5.81689977645874\n",
      "Eikonal loss: 0.04129394516348839\n",
      "Step 2060:\n",
      "Loss: 0.3492136597633362\n",
      "Average steps: 5.810800075531006\n",
      "Eikonal loss: 0.042066290974617004\n",
      "Step 2065:\n",
      "Loss: 0.3437567949295044\n",
      "Average steps: 5.783400058746338\n",
      "Eikonal loss: 0.04079326242208481\n",
      "Step 2070:\n",
      "Loss: 0.3428325057029724\n",
      "Average steps: 5.787899971008301\n",
      "Eikonal loss: 0.04094534367322922\n",
      "Step 2075:\n",
      "Loss: 0.3526766896247864\n",
      "Average steps: 5.812699794769287\n",
      "Eikonal loss: 0.043115872889757156\n",
      "Step 2080:\n",
      "Loss: 0.3466269373893738\n",
      "Average steps: 5.741399765014648\n",
      "Eikonal loss: 0.04164782539010048\n",
      "Step 2085:\n",
      "Loss: 0.34177273511886597\n",
      "Average steps: 5.7718000411987305\n",
      "Eikonal loss: 0.04064984247088432\n",
      "Step 2090:\n",
      "Loss: 0.35153865814208984\n",
      "Average steps: 5.776299953460693\n",
      "Eikonal loss: 0.04261254891753197\n",
      "Step 2095:\n",
      "Loss: 0.3425289988517761\n",
      "Average steps: 5.771100044250488\n",
      "Eikonal loss: 0.040938280522823334\n",
      "Step 2100:\n",
      "Loss: 0.3355012536048889\n",
      "Average steps: 5.719599723815918\n",
      "Eikonal loss: 0.03942623361945152\n",
      "Step 2105:\n",
      "Loss: 0.3519565463066101\n",
      "Average steps: 5.767499923706055\n",
      "Eikonal loss: 0.04314098879694939\n",
      "Step 2110:\n",
      "Loss: 0.3577077090740204\n",
      "Average steps: 5.792299747467041\n",
      "Eikonal loss: 0.04414121061563492\n",
      "Step 2115:\n",
      "Loss: 0.35708922147750854\n",
      "Average steps: 5.76609992980957\n",
      "Eikonal loss: 0.04397565498948097\n",
      "Step 2120:\n",
      "Loss: 0.3521919250488281\n",
      "Average steps: 5.733099937438965\n",
      "Eikonal loss: 0.042808014899492264\n",
      "Step 2125:\n",
      "Loss: 0.3568139672279358\n",
      "Average steps: 5.815299987792969\n",
      "Eikonal loss: 0.04415836185216904\n",
      "Step 2130:\n",
      "Loss: 0.3440307676792145\n",
      "Average steps: 5.753200054168701\n",
      "Eikonal loss: 0.04173171520233154\n",
      "Step 2135:\n",
      "Loss: 0.337319552898407\n",
      "Average steps: 5.664499759674072\n",
      "Eikonal loss: 0.04011787101626396\n",
      "Step 2140:\n",
      "Loss: 0.3545442819595337\n",
      "Average steps: 5.671299934387207\n",
      "Eikonal loss: 0.04386137053370476\n",
      "Step 2145:\n",
      "Loss: 0.3555561900138855\n",
      "Average steps: 5.694799900054932\n",
      "Eikonal loss: 0.04420012980699539\n",
      "Step 2150:\n",
      "Loss: 0.3449159860610962\n",
      "Average steps: 5.662499904632568\n",
      "Eikonal loss: 0.041746970266103745\n",
      "Step 2155:\n",
      "Loss: 0.3435187339782715\n",
      "Average steps: 5.696699619293213\n",
      "Eikonal loss: 0.04165210947394371\n",
      "Step 2160:\n",
      "Loss: 0.3406030535697937\n",
      "Average steps: 5.7403998374938965\n",
      "Eikonal loss: 0.0412968173623085\n",
      "Step 2165:\n",
      "Loss: 0.3416638970375061\n",
      "Average steps: 5.782599925994873\n",
      "Eikonal loss: 0.0414513424038887\n",
      "Step 2170:\n",
      "Loss: 0.34997427463531494\n",
      "Average steps: 5.811999797821045\n",
      "Eikonal loss: 0.04296663776040077\n",
      "Step 2175:\n",
      "Loss: 0.3478659987449646\n",
      "Average steps: 5.791500091552734\n",
      "Eikonal loss: 0.04270010441541672\n",
      "Step 2180:\n",
      "Loss: 0.33882492780685425\n",
      "Average steps: 5.77810001373291\n",
      "Eikonal loss: 0.04086107760667801\n",
      "Step 2185:\n",
      "Loss: 0.3434253931045532\n",
      "Average steps: 5.814599990844727\n",
      "Eikonal loss: 0.04166188836097717\n",
      "Step 2190:\n",
      "Loss: 0.3338688015937805\n",
      "Average steps: 5.763799667358398\n",
      "Eikonal loss: 0.03950067609548569\n",
      "Step 2195:\n",
      "Loss: 0.3323690891265869\n",
      "Average steps: 5.792399883270264\n",
      "Eikonal loss: 0.03930448740720749\n",
      "Step 2200:\n",
      "Loss: 0.34033694863319397\n",
      "Average steps: 5.830599784851074\n",
      "Eikonal loss: 0.04101976752281189\n",
      "Step 2205:\n",
      "Loss: 0.3340432047843933\n",
      "Average steps: 5.7891998291015625\n",
      "Eikonal loss: 0.039476849138736725\n",
      "Step 2210:\n",
      "Loss: 0.3490481972694397\n",
      "Average steps: 5.819999694824219\n",
      "Eikonal loss: 0.04270923137664795\n",
      "Step 2215:\n",
      "Loss: 0.3459289073944092\n",
      "Average steps: 5.838699817657471\n",
      "Eikonal loss: 0.04218079894781113\n",
      "Step 2220:\n",
      "Loss: 0.3488711416721344\n",
      "Average steps: 5.78249979019165\n",
      "Eikonal loss: 0.04278363659977913\n",
      "Step 2225:\n",
      "Loss: 0.33751949667930603\n",
      "Average steps: 5.816299915313721\n",
      "Eikonal loss: 0.04025676101446152\n",
      "Step 2230:\n",
      "Loss: 0.3391704261302948\n",
      "Average steps: 5.847099781036377\n",
      "Eikonal loss: 0.04042559117078781\n",
      "Step 2235:\n",
      "Loss: 0.3465282917022705\n",
      "Average steps: 5.830599784851074\n",
      "Eikonal loss: 0.04170126095414162\n",
      "Step 2240:\n",
      "Loss: 0.3427717089653015\n",
      "Average steps: 5.794899940490723\n",
      "Eikonal loss: 0.04092751815915108\n",
      "Step 2245:\n",
      "Loss: 0.338659405708313\n",
      "Average steps: 5.7993998527526855\n",
      "Eikonal loss: 0.040381014347076416\n",
      "Step 2250:\n",
      "Loss: 0.3539036512374878\n",
      "Average steps: 5.853399753570557\n",
      "Eikonal loss: 0.04355834051966667\n",
      "Step 2255:\n",
      "Loss: 0.3453989326953888\n",
      "Average steps: 5.789000034332275\n",
      "Eikonal loss: 0.041778821498155594\n",
      "Step 2260:\n",
      "Loss: 0.33318766951560974\n",
      "Average steps: 5.775700092315674\n",
      "Eikonal loss: 0.03973159193992615\n",
      "Step 2265:\n",
      "Loss: 0.3336274027824402\n",
      "Average steps: 5.796000003814697\n",
      "Eikonal loss: 0.04020773246884346\n",
      "Step 2270:\n",
      "Loss: 0.3422278165817261\n",
      "Average steps: 5.763799667358398\n",
      "Eikonal loss: 0.04146796837449074\n",
      "Step 2275:\n",
      "Loss: 0.325014591217041\n",
      "Average steps: 5.7846999168396\n",
      "Eikonal loss: 0.03789839148521423\n",
      "Step 2280:\n",
      "Loss: 0.3357500433921814\n",
      "Average steps: 5.849699974060059\n",
      "Eikonal loss: 0.0400913804769516\n",
      "Step 2285:\n",
      "Loss: 0.343816339969635\n",
      "Average steps: 5.822700023651123\n",
      "Eikonal loss: 0.04187296703457832\n",
      "Step 2290:\n",
      "Loss: 0.33411863446235657\n",
      "Average steps: 5.813299655914307\n",
      "Eikonal loss: 0.03990739583969116\n",
      "Step 2295:\n",
      "Loss: 0.3388857841491699\n",
      "Average steps: 5.820699691772461\n",
      "Eikonal loss: 0.040857944637537\n",
      "Step 2300:\n",
      "Loss: 0.3436267375946045\n",
      "Average steps: 5.836999893188477\n",
      "Eikonal loss: 0.04184005782008171\n",
      "Step 2305:\n",
      "Loss: 0.332943856716156\n",
      "Average steps: 5.828099727630615\n",
      "Eikonal loss: 0.0398404486477375\n",
      "Step 2310:\n",
      "Loss: 0.3280049264431\n",
      "Average steps: 5.773399829864502\n",
      "Eikonal loss: 0.03907002881169319\n",
      "Step 2315:\n",
      "Loss: 0.32617440819740295\n",
      "Average steps: 5.781399726867676\n",
      "Eikonal loss: 0.03870084136724472\n",
      "Step 2320:\n",
      "Loss: 0.3337690830230713\n",
      "Average steps: 5.827600002288818\n",
      "Eikonal loss: 0.03995858505368233\n",
      "Step 2325:\n",
      "Loss: 0.32841479778289795\n",
      "Average steps: 5.788399696350098\n",
      "Eikonal loss: 0.03888446465134621\n",
      "Step 2330:\n",
      "Loss: 0.32634198665618896\n",
      "Average steps: 5.800299644470215\n",
      "Eikonal loss: 0.0384172722697258\n",
      "Step 2335:\n",
      "Loss: 0.3343586325645447\n",
      "Average steps: 5.800299644470215\n",
      "Eikonal loss: 0.039758700877428055\n",
      "Step 2340:\n",
      "Loss: 0.348937451839447\n",
      "Average steps: 5.856400012969971\n",
      "Eikonal loss: 0.0426458865404129\n",
      "Step 2345:\n",
      "Loss: 0.34799516201019287\n",
      "Average steps: 5.856899738311768\n",
      "Eikonal loss: 0.042511191219091415\n",
      "Step 2350:\n",
      "Loss: 0.3558031916618347\n",
      "Average steps: 5.794600009918213\n",
      "Eikonal loss: 0.04437900334596634\n",
      "Step 2355:\n",
      "Loss: 0.3331543803215027\n",
      "Average steps: 5.797399997711182\n",
      "Eikonal loss: 0.04018310829997063\n",
      "Step 2360:\n",
      "Loss: 0.3328930735588074\n",
      "Average steps: 5.794600009918213\n",
      "Eikonal loss: 0.040185507386922836\n",
      "Step 2365:\n",
      "Loss: 0.32164597511291504\n",
      "Average steps: 5.744999885559082\n",
      "Eikonal loss: 0.03803063929080963\n",
      "Step 2370:\n",
      "Loss: 0.3276151418685913\n",
      "Average steps: 5.75439977645874\n",
      "Eikonal loss: 0.03924274072051048\n",
      "Step 2375:\n",
      "Loss: 0.321830153465271\n",
      "Average steps: 5.742499828338623\n",
      "Eikonal loss: 0.03799000009894371\n",
      "Step 2380:\n",
      "Loss: 0.3287864923477173\n",
      "Average steps: 5.7714996337890625\n",
      "Eikonal loss: 0.0395226813852787\n",
      "Step 2385:\n",
      "Loss: 0.33317312598228455\n",
      "Average steps: 5.807199954986572\n",
      "Eikonal loss: 0.04033873602747917\n",
      "Step 2390:\n",
      "Loss: 0.35325732827186584\n",
      "Average steps: 5.864999771118164\n",
      "Eikonal loss: 0.0438101589679718\n",
      "Step 2395:\n",
      "Loss: 0.336284875869751\n",
      "Average steps: 5.82450008392334\n",
      "Eikonal loss: 0.04045889154076576\n",
      "Step 2400:\n",
      "Loss: 0.3395695686340332\n",
      "Average steps: 5.8078999519348145\n",
      "Eikonal loss: 0.04116811975836754\n",
      "Step 2405:\n",
      "Loss: 0.34133148193359375\n",
      "Average steps: 5.815199851989746\n",
      "Eikonal loss: 0.04130008444190025\n",
      "Step 2410:\n",
      "Loss: 0.3396267294883728\n",
      "Average steps: 5.784499645233154\n",
      "Eikonal loss: 0.04088447615504265\n",
      "Step 2415:\n",
      "Loss: 0.33993303775787354\n",
      "Average steps: 5.769199848175049\n",
      "Eikonal loss: 0.04079049453139305\n",
      "Step 2420:\n",
      "Loss: 0.33722859621047974\n",
      "Average steps: 5.773799896240234\n",
      "Eikonal loss: 0.040416497737169266\n",
      "Step 2425:\n",
      "Loss: 0.35108813643455505\n",
      "Average steps: 5.840099811553955\n",
      "Eikonal loss: 0.04320754110813141\n",
      "Step 2430:\n",
      "Loss: 0.34822165966033936\n",
      "Average steps: 5.841599941253662\n",
      "Eikonal loss: 0.042559899389743805\n",
      "Step 2435:\n",
      "Loss: 0.33821797370910645\n",
      "Average steps: 5.75469970703125\n",
      "Eikonal loss: 0.04091595858335495\n",
      "Step 2440:\n",
      "Loss: 0.3357347249984741\n",
      "Average steps: 5.713699817657471\n",
      "Eikonal loss: 0.04041716828942299\n",
      "Step 2445:\n",
      "Loss: 0.32490915060043335\n",
      "Average steps: 5.7465996742248535\n",
      "Eikonal loss: 0.03831137716770172\n",
      "Step 2450:\n",
      "Loss: 0.34001195430755615\n",
      "Average steps: 5.815700054168701\n",
      "Eikonal loss: 0.0410870723426342\n",
      "Step 2455:\n",
      "Loss: 0.344326376914978\n",
      "Average steps: 5.89109992980957\n",
      "Eikonal loss: 0.04123333469033241\n",
      "Step 2460:\n",
      "Loss: 0.33442258834838867\n",
      "Average steps: 5.877500057220459\n",
      "Eikonal loss: 0.039125487208366394\n",
      "Step 2465:\n",
      "Loss: 0.3374921679496765\n",
      "Average steps: 5.844299793243408\n",
      "Eikonal loss: 0.040160514414310455\n",
      "Step 2470:\n",
      "Loss: 0.34008121490478516\n",
      "Average steps: 5.845900058746338\n",
      "Eikonal loss: 0.04074997454881668\n",
      "Step 2475:\n",
      "Loss: 0.3316817283630371\n",
      "Average steps: 5.874300003051758\n",
      "Eikonal loss: 0.03905686363577843\n",
      "Step 2480:\n",
      "Loss: 0.339881032705307\n",
      "Average steps: 5.856800079345703\n",
      "Eikonal loss: 0.04097520187497139\n",
      "Step 2485:\n",
      "Loss: 0.33582013845443726\n",
      "Average steps: 5.867099761962891\n",
      "Eikonal loss: 0.04020854830741882\n",
      "Step 2490:\n",
      "Loss: 0.33893775939941406\n",
      "Average steps: 5.872399806976318\n",
      "Eikonal loss: 0.040585197508335114\n",
      "Step 2495:\n",
      "Loss: 0.3256297707557678\n",
      "Average steps: 5.822099685668945\n",
      "Eikonal loss: 0.03852549195289612\n",
      "Training done !\n",
      "Video saved successfully as depth_map_training.mp4\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "L = 5\n",
    "lr = 1e-4\n",
    "num_iters = 2500\n",
    "\n",
    "# Misc parameters\n",
    "display_every = 5\n",
    "\n",
    "# Model\n",
    "model = VeryTinyNeDFModel(L=L)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Seed RNG\n",
    "seed = 9458\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Setup training\n",
    "target_depth = testimg.float().to(device)\n",
    "target_tform_cam2world = testpose\n",
    "ray_origins, ray_directions = get_ray_bundle(height, width, focal_length, target_tform_cam2world)\n",
    "\n",
    "# Save depth maps for video\n",
    "depth_maps = []\n",
    "\n",
    "eikonal_penalty_weight = 5  # Weight for eikonal loss\n",
    "\n",
    "step_penalty_weight = 0  # Adjust weight to balance depth loss and step penalty\n",
    "step_penalty_alpha = 0.05  # Exponential growth factor for step penalty\n",
    "\n",
    "for i in range(num_iters):\n",
    "    # Use NeDF sphere tracing to compute the depth map and step count\n",
    "    depth_predicted, steps, query_points, query_results = render_depth_sphere_tracing(model, ray_origins, ray_directions, target_depth, near_thresh)\n",
    "\n",
    "    gradients = compute_gradients(model, query_points)\n",
    "    eikonal_loss = compute_eikonal_loss(gradients)\n",
    "\n",
    "    # Compute the mean-squared error loss between predicted and true depth map\n",
    "    depth_loss = torch.nn.functional.mse_loss(depth_predicted, target_depth)\n",
    "\n",
    "    # Exponential penalty for step count to prioritize reducing steps (not working)\n",
    "    # step_penalty = torch.mean(torch.exp(step_penalty_alpha * steps))\n",
    "    \n",
    "    # Use eikonal loss instead, very effective\n",
    "    eikonal_penalty = eikonal_penalty_weight * eikonal_loss\n",
    "\n",
    "    # Combine the depth loss and step penalty\n",
    "    loss = depth_loss + eikonal_penalty\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Display progress\n",
    "    if i % display_every == 0:\n",
    "        print(f\"Step {i}:\")\n",
    "        print(f\"Loss: {loss}\")\n",
    "        print(f\"Average steps: {steps.mean().item()}\")\n",
    "        print(f\"Eikonal loss: {eikonal_loss}\")\n",
    "        \n",
    "        # Convert depth map to image (normalize for visualization)\n",
    "        depth_img = depth_predicted.detach().cpu().numpy()\n",
    "        depth_img_normalized = (depth_img - depth_img.min()) / (depth_img.max() - depth_img.min()) * 255.0\n",
    "        depth_maps.append(depth_img_normalized.astype(np.uint8))\n",
    "\n",
    "        # Display the current depth map\n",
    "        # plt.imshow(depth_img_normalized, cmap='inferno')\n",
    "        # plt.title(f\"Iteration {i} - Depth Map\")\n",
    "        # plt.colorbar()\n",
    "        # plt.show()\n",
    "\n",
    "print('Training done !')\n",
    "\n",
    "# Save the video using OpenCV\n",
    "output_file = \"depth_map_training.mp4\"\n",
    "height, width = depth_maps[0].shape\n",
    "video_writer = cv2.VideoWriter(\n",
    "    output_file, cv2.VideoWriter_fourcc(*'mp4v'), 5, (width, height), isColor=True\n",
    ")\n",
    "\n",
    "# Write each depth map to the video\n",
    "for depth_map in depth_maps:\n",
    "    # Ensure grayscale map is expanded to (H, W, 3) to represent RGB channels\n",
    "    frame = np.stack([depth_map]*3, axis=-1)  # Duplicate grayscale into R, G, B\n",
    "    video_writer.write(frame)\n",
    "\n",
    "video_writer.release()\n",
    "print(f\"Video saved successfully as {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
